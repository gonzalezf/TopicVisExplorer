{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim, pickle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "LdaModel = gensim.models.ldamodel.LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ruta_word_embedding = '../data/wiki.multi.en.vec'\n",
    "#ruta_word_embedding = '../data/embedding_english__europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.bin'\n",
    "ruta_word_embedding = '../data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "wordembedding = gensim.models.Word2Vec.load(ruta_word_embedding)\n",
    "#wordembedding = KeyedVectors.load_word2vec_format(ruta_word_embedding, binary=False)\n",
    "#wordembedding = KeyedVectors.load_word2vec_format(ruta_word_embedding, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load topic modelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: \n",
    "LDAvis Scenario:\n",
    "If you wish to calculate topic similarity metric over one topic modeling output, therefore lda_model_collection_1\n",
    "and lda_model_collection_2 should be the same. \n",
    "\n",
    "Sankey diagram scenario:\n",
    "If you wish to calculate topic similarity metric over two different topic modeling outputs, lda_model_collection_1\n",
    "and lda_model_collection_2 should be different\n",
    "\n",
    "\n",
    "The same shouuld be done with the most relevant documents.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "lda_model_collecion_1 = LdaModel.load(\"../data/cambridge_analytica/regional_datasets/files_europe/english_europe_tweets_20190411.csv_gensim.model\")\n",
    "lda_model_collecion_2 = LdaModel.load(\"../data/cambridge_analytica/regional_datasets/files_northamerica/english_northamerica_tweets_20190411.csv_gensim.model\")\n",
    "#lda_model_collecion_2 = LdaModel.load(\"../data/cambridge_analytica/regional_datasets/files_northamerica/english_northamerica_tweets_20190411.csv_gensim.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cambridge_analytica/regional_datasets/files_europe/english_europe_tweets_20190411.csvsent_topics_sorteddf_mallet_ldamodel', 'rb') as f:\n",
    "    most_relevant_documents_collection_1 = pickle.load(f)\n",
    "most_relevant_documents_collection_1 = most_relevant_documents_collection_1[['Topic_Num','Topic_Perc_Contrib','text']]\n",
    "\n",
    "# ../data/cambridge_analytica/regional_datasets/files_northamerica/english_northamerica_tweets_20190411.csvsent_topics_sorteddf_mallet_ldamodel\n",
    "with open('../data/cambridge_analytica/regional_datasets/files_northamerica/english_northamerica_tweets_20190411.csvsent_topics_sorteddf_mallet_ldamodel', 'rb') as f:\n",
    "    most_relevant_documents_collection_2 = pickle.load(f)\n",
    "most_relevant_documents_collection_2 = most_relevant_documents_collection_2[['Topic_Num','Topic_Perc_Contrib','text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prepared data from each collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/cambridge_analytica/regional_datasets/files_europe/english_europe_tweets_20190411_prepared_data_dict_with_more_info', 'rb') as f:\n",
    "    PreparedData_dict_with_more_info_collection_1 = pickle.load(f)\n",
    "topic_order_1 = PreparedData_dict_with_more_info_collection_1['topic.order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/cambridge_analytica/regional_datasets/files_northamerica/english_northamerica_tweets_20190411_prepared_data_dict_with_more_info', 'rb') as f:\n",
    "    PreparedData_dict_with_more_info_collection_2 = pickle.load(f)\n",
    "topic_order_2 = PreparedData_dict_with_more_info_collection_1['topic.order']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add relevance column to each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_lambda = 0.6 #according to a user study, this is the best value. We can change this!!!\n",
    "\n",
    "\n",
    "tinfo_collection_1 = pd.DataFrame.from_dict(PreparedData_dict_with_more_info_collection_1['tinfo'])\n",
    "tinfo_collection_1['relevance'] = relevance_lambda * tinfo_collection_1['logprob']+ (1.00-relevance_lambda)*tinfo_collection_1['loglift']\n",
    "\n",
    "tinfo_collection_2 = pd.DataFrame.from_dict(PreparedData_dict_with_more_info_collection_2['tinfo'])\n",
    "tinfo_collection_2['relevance'] = relevance_lambda * tinfo_collection_2['logprob']+ (1.00-relevance_lambda)*tinfo_collection_2['loglift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how get terms order by relevance\n",
    "'''\n",
    "topic_id = 0\n",
    "tinfo_collection_1.loc[tinfo_collection_1['Category'] == 'Topic'+str(topic_id+1)].sort_values(by='relevance', ascending=False)[['Term','relevance']][:n_terms]\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import spacy\n",
    "\n",
    "from string import punctuation\n",
    "from gensim.utils import simple_preprocess\n",
    "from string import digits\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''My own tokenizer '''\n",
    "\n",
    "punctuation+=\"¡¿<>'`\"\n",
    "punctuation+='\"'\n",
    "\n",
    "#Remove digits and puntuaction\n",
    "remove_digits = str.maketrans(digits, ' '*len(digits))#remove_digits = str.maketrans('', '', digits)\n",
    "remove_punctuation = str.maketrans(punctuation, ' '*len(punctuation))#remove_punctuation = str.maketrans('', '', punctuation)\n",
    "remove_hashtags_caracter = str.maketrans('#', ' '*len('#'))\n",
    "#las palabras de los hashtag se mantiene, pero no el simbolo. \n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "def sent_to_words(sentence):\n",
    "    return tknzr.tokenize(sentence)\n",
    "    \n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    doc = nlp(\" \".join(texts)) \n",
    "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "        \n",
    "def text_cleaner(tweet):\n",
    "    tweet = tweet.translate(remove_digits)\n",
    "    #tweet = tweet.lower() it wasn't a good idea,, we lost a lot of\n",
    "    tweet = tweet.translate(remove_punctuation)\n",
    "    tweet = tweet.translate(remove_hashtags_caracter)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    #tweet = tweet.strip().split()\n",
    "    #filtered_words = [word for word in tweet if word not in stopWords]\n",
    "    #corpus[id_tweet]= filtered_words\n",
    "    #id_tweet+=1\n",
    "    #tweet = ' '.join(sent_to_words(tweet))\n",
    "    tweet = sent_to_words(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    new_tweet  = []\n",
    "    for elem in tweet:\n",
    "        if len(elem)>0:\n",
    "            new_tweet.append(elem[0])\n",
    "    tweet = lemmatization(new_tweet, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    return tweet[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arrow', 'arrow']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner('This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <-- arrows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, that vectors are going to be calculated according to topic order of PreparedData\n",
    "\n",
    "def get_dicts_relevant_keywords_documents(lda_model,df_relevant_documents, n_terms, topic_order, PreparedData_dict_with_more_info):\n",
    "    num_topics = lda_model.num_topics\n",
    "    #create dictionary of top keywords \n",
    "    topKeywordsDict = {}\n",
    "    for topic_id in range(num_topics):\n",
    "        \n",
    "        topKeywordsDict[topic_id] = []\n",
    "        \n",
    "        def save_relevant_keywords_in_dict(row):\n",
    "            topKeywordsDict[topic_id].append({  #el topic_id, debe ser segun el orden de lda_model\n",
    "                \"term\":row['Term'],\n",
    "                \"relevance\":row['relevance']\n",
    "            })\n",
    "            \n",
    "        topic_on_tinfo = topic_order.index(topic_id+1)+1    \n",
    "        PreparedData_dict_with_more_info.loc[PreparedData_dict_with_more_info['Category'] == 'Topic'+str(topic_on_tinfo)].sort_values(by='relevance', ascending=False)[['Term','relevance']][:n_terms].apply(save_relevant_keywords_in_dict, axis=1)    \n",
    "        \n",
    "        \n",
    "            \n",
    "    #create dictionary of relevant documents\n",
    "    relevantDocumentsDict = {}\n",
    "    \n",
    "    def save_relevant_documents_in_dict(row):\n",
    "        topic_id = int(row['Topic_Num'])\n",
    "        if topic_id not in relevantDocumentsDict:\n",
    "            relevantDocumentsDict[topic_id]=[]\n",
    "        relevantDocumentsDict[topic_id].append({\n",
    "            'topic_perc_contrib':row['Topic_Perc_Contrib'],\n",
    "            'text':row['text']\n",
    "        })\n",
    "        return None\n",
    "    \n",
    "    df_relevant_documents.apply(save_relevant_documents_in_dict, axis=1)\n",
    "    \n",
    "        \n",
    "    return (topKeywordsDict, relevantDocumentsDict)\n",
    "\n",
    "\n",
    "def getDocumentVector(text, wordembedding,  topic_id , topic_order, PreparedData_dict_with_more_info):\n",
    "    #preprocesar    \n",
    "    #encontrar palabras en word embedding\n",
    "    #ponderas palabras with relevance metric\n",
    "    \n",
    "    topic_on_tinfo = topic_order.index(topic_id+1)+1    \n",
    "    list_terms_relevance = PreparedData_dict_with_more_info.loc[PreparedData_dict_with_more_info['Category'] == 'Topic'+str(topic_on_tinfo)].sort_values(by='relevance', ascending=False)['Term'].tolist()\n",
    "    document_vector = 0.0\n",
    "    words_found = set()\n",
    "    for word in text_cleaner(text):    \n",
    "        if word in list_terms_relevance:\n",
    "            raking_word = float(list_terms_relevance.index(word)+1)\n",
    "            if word in wordembedding.wv:\n",
    "                #print(\"WORD FOUND\", word)\n",
    "                document_vector+=wordembedding.wv[word]/raking_word #aqui hay que ponderar\n",
    "                words_found.add(word.upper())\n",
    "            else:\n",
    "                print(\"WARNING, Word not found:\", word)\n",
    "        #else:\n",
    "        #    print(\"not found word\", word.upper(), \"for document:\",text_cleaner(text))\n",
    "    #print(\"words found\", words_found)\n",
    "    return document_vector\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def get_topkeywords_relevantdocuments_vectors(wordembedding, lda_model,most_relevant_documents,  n_terms, topic_order, PreparedData_dict_with_more_info, topkdocuments): #n_terms : numero de top keywords a considerar\n",
    "    topKeywordsDict, relevantDocumentsDict = get_dicts_relevant_keywords_documents(lda_model, most_relevant_documents, n_terms, topic_order,  PreparedData_dict_with_more_info)\n",
    "\n",
    "    ##Create top keyword vector per topic\n",
    "    topkeywords_vectors_dict = {}\n",
    "    num_topics = lda_model.num_topics\n",
    "    for topic_id in range(num_topics):\n",
    "        topkeywords_vector = 0\n",
    "        ranking = 1.0\n",
    "        for item in topKeywordsDict[topic_id]:\n",
    "            if item['term'] in wordembedding.wv: \n",
    "                topkeywords_vector += wordembedding.wv[item['term']]/ranking\n",
    "            else:\n",
    "                print(\"WARNING NOT FOUND: \", item['term'],\" position:\",ranking)\n",
    "            ranking+=1\n",
    "        topkeywords_vectors_dict[topic_id] = topkeywords_vector\n",
    "        \n",
    "    #Create a top relevant document vector    \n",
    "    relevantdocuments_vectors_dict = {}\n",
    "    for topic_id in range(num_topics):\n",
    "        relevantDocumentsvector = 0.0\n",
    "        j = 0\n",
    "        for item in relevantDocumentsDict[topic_id][0:topkdocuments]: #we consider only the most k documents.             \n",
    "            j+=1                                            \n",
    "            relevantDocumentsvector+= float(item['topic_perc_contrib'])*getDocumentVector(item['text'], wordembedding, topic_id, topic_order, PreparedData_dict_with_more_info) #PODRIA HACER UNA ESPECIE DE RANKING, SIMILAR A LO QUE HICE CON LAS TOP KEYWORDS.\n",
    "        relevantdocuments_vectors_dict[topic_id] = relevantDocumentsvector\n",
    "        \n",
    "    return (topkeywords_vectors_dict, relevantdocuments_vectors_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we calculate once the topkeywords_vector and the relevant documents_vector for each topic\n",
    "#We are going to calculate several times:      #final topic vector = (lambda)topic_keyword_vector + (lambda-1)topic_document_vector\n",
    "#because we are going to try different lambda (between 0 and 1)\n",
    "def get_topic_vectors(wordembedding, lda_model,most_relevant_documents,  n_terms, lambda_, topic_order, PreparedData_dict_with_more_info, topkdocuments):\n",
    "    num_topics = lda_model.num_topics\n",
    "    topkeywords_vectors_dict, relevantdocuments_vectors_dict = get_topkeywords_relevantdocuments_vectors(wordembedding, lda_model,most_relevant_documents,  n_terms, topic_order, PreparedData_dict_with_more_info, topkdocuments)\n",
    "    final_topic_vectors_dict = dict()\n",
    "    for topic_id in range(num_topics):\n",
    "        final_topic_vector = lambda_*topkeywords_vectors_dict[topic_id]+(1-lambda_)*relevantdocuments_vectors_dict[topic_id]\n",
    "        final_topic_vectors_dict[topic_id] = final_topic_vector\n",
    "    return final_topic_vectors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This matrix is calculated by a specific lambda. \n",
    "def get_matrix(wordembedding, lda_model_1,most_relevant_documents_1,lda_model_2,most_relevant_documents_2, n_terms, lambda_, topic_order_1, topic_order_2, PreparedData_dict_with_more_info_1, PreparedData_dict_with_more_info_2, topkdocuments):\n",
    "    \n",
    "    #final topic vector = (lambda)topic_keyword_vector + (lambda-1)topic_document_vector\n",
    "    final_topic_vectors_dict_1 =  get_topic_vectors(wordembedding, lda_model_1,most_relevant_documents_1,  n_terms, lambda_, topic_order_1, PreparedData_dict_with_more_info_1, topkdocuments)\n",
    "    final_topic_vectors_dict_2 =  get_topic_vectors(wordembedding, lda_model_2,most_relevant_documents_2,  n_terms, lambda_, topic_order_2,  PreparedData_dict_with_more_info_2, topkdocuments)\n",
    "    \n",
    "    topic_similarity_matrix = []\n",
    "    for i in range(lda_model_1.num_topics):\n",
    "        row = []\n",
    "        for j in range(lda_model_2.num_topics):\n",
    "            topic_i = final_topic_vectors_dict_1[i].reshape(1,-1)\n",
    "            topic_j = final_topic_vectors_dict_2[j].reshape(1,-1)\n",
    "            row.append(float(cosine_similarity(topic_i,topic_j)))\n",
    "            #print(i,j,float(cosine_similarity(topic_i,topic_j)))\n",
    "        topic_similarity_matrix.append(row)\n",
    "    topic_similarity_matrix= np.asarray(topic_similarity_matrix)\n",
    "    return topic_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.57\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.69\n",
      "0.7\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.82\n",
      "0.83\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "topn = 20\n",
    "topkdocuments = 20\n",
    "#lambda_ = 0.8\n",
    "\n",
    "\n",
    "#i = 0.0\n",
    "i = 0.0\n",
    "matrices_dict = dict()\n",
    "while i <=1.01:\n",
    "    lambda_ = round(i*100/100,2)\n",
    "    print(lambda_)\n",
    "    matrix = get_matrix(wordembedding, lda_model_collecion_1, most_relevant_documents_collection_1, lda_model_collecion_2, most_relevant_documents_collection_2,topn, lambda_, topic_order_1, topic_order_2, tinfo_collection_1, tinfo_collection_1, topkdocuments)\n",
    "    matrices_dict[lambda_] = matrix\n",
    "    i+=0.01\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cambridge_analytica/regional_datasets/matrix_europe_vs_northamerica_own_wordembedding_final', 'wb') as f:\n",
    "            pickle.dump(matrices_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99633479,  0.14988817,  0.29329914, -0.08134293,  0.53773975,\n",
       "         0.13499707, -0.35923755,  0.20835081,  0.2383368 ,  0.33324265,\n",
       "         0.16214065],\n",
       "       [ 0.1387414 ,  0.98451376, -0.22514254, -0.3184033 , -0.11611461,\n",
       "        -0.0812121 , -0.51116979, -0.15153964,  0.22815076,  0.00279417,\n",
       "        -0.1648926 ],\n",
       "       [ 0.28215104, -0.16495165,  0.99649435,  0.51128352,  0.26514667,\n",
       "         0.0299394 ,  0.08406849,  0.64607501,  0.22171441,  0.3973726 ,\n",
       "         0.08754478],\n",
       "       [-0.08046842, -0.24579939,  0.52549839,  0.99675691,  0.0503291 ,\n",
       "        -0.06085549,  0.54527229,  0.4518255 ,  0.2310625 ,  0.30215418,\n",
       "         0.24053933],\n",
       "       [ 0.56561613, -0.11152926,  0.32607791,  0.09095387,  0.98771983,\n",
       "         0.14907433, -0.08216837,  0.37363175,  0.28352067,  0.40270835,\n",
       "         0.25520119],\n",
       "       [ 0.13555886, -0.08942778,  0.02091907, -0.06558391,  0.17932616,\n",
       "         0.99837989, -0.09866506,  0.1315632 ,  0.09602447,  0.04816668,\n",
       "         0.26221499],\n",
       "       [-0.34839869, -0.48121801,  0.07551415,  0.550385  , -0.10439688,\n",
       "        -0.11536566,  0.99756253,  0.06433065, -0.14176771, -0.00198817,\n",
       "         0.14863822],\n",
       "       [ 0.20702241, -0.08248364,  0.6465503 ,  0.44809926,  0.3225258 ,\n",
       "         0.12418424,  0.08344383,  0.99987018,  0.25301817,  0.42272311,\n",
       "         0.11842604],\n",
       "       [ 0.20423231,  0.27543083,  0.19062731,  0.21256983,  0.22459181,\n",
       "         0.14155132, -0.11655805,  0.22189936,  0.98628926,  0.29211938,\n",
       "         0.1719785 ],\n",
       "       [ 0.30541143,  0.02246474,  0.3898344 ,  0.30222315,  0.33323896,\n",
       "         0.04459997,  0.0141574 ,  0.41533917,  0.31274605,  0.99879336,\n",
       "         0.21345825],\n",
       "       [ 0.16184841, -0.17754334,  0.09207306,  0.23853514,  0.26777995,\n",
       "         0.27398551,  0.15899077,  0.13297644,  0.15520582,  0.22248636,\n",
       "         0.99820948]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrices_dict[0.89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 0.03532088454815349),\n",
       " ('leave', 0.03348963680253045),\n",
       " ('vote', 0.025332260481118727),\n",
       " ('campaign', 0.023487139646513694),\n",
       " ('election', 0.022821231375378042),\n",
       " ('amp', 0.019033878083294025),\n",
       " ('lie', 0.01650897588857135),\n",
       " ('government', 0.014511251075164397),\n",
       " ('democracy', 0.013637246469298854),\n",
       " ('tory', 0.012874226575289255)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_collecion_1.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 0.031173801054826443),\n",
       " ('back', 0.023785109775542743),\n",
       " ('put', 0.018098859315589355),\n",
       " ('man', 0.01462038513430639),\n",
       " ('show', 0.014389795167423034),\n",
       " ('start', 0.014065987979884705),\n",
       " ('turn', 0.012790383907764014),\n",
       " ('guy', 0.011701214276953269),\n",
       " ('bring', 0.011421562614988348),\n",
       " ('play', 0.01139212559793941)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_collecion_2.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cambridge_analytica/regional_datasets/files_europe/english_europe_tweets_20190411.csvsent_topics_sorteddf_mallet_ldamodel', 'rb') as f:\n",
    "    most_relevant_documents_collection_1 = pickle.load(f)\n",
    "#most_relevant_documents_collection_1 = most_relevant_documents_collection_1[['Topic_Num','Topic_Perc_Contrib','text']]\n",
    "most_relevant_documents_collection_1.to_csv('borrar_review_delete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
