{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import topicvisexplorer\n",
    "import importlib\n",
    "import pickle\n",
    "import random\n",
    "from _topic_similarity_matrix import *\n",
    "#from _topic_splitting_helpers import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['usernameremoved', 'link', 'remove', 'usernameremove', 'amp', 'linkremoved',' <link removed>','usernameremoved','<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "\n",
    "name_column_text = 'texto_completo'\n",
    "name_tokenizacion = 'text_cleaner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document relevant seed words\n",
    "f = open('json_file_topic_splitting_test.json',) \n",
    "data = json.load(f) \n",
    "new_document_seeds_TopicA = data['new_document_seeds']['TopicA']\n",
    "new_document_seeds_TopicB = data['new_document_seeds']['TopicB']\n",
    "old_circle_positions = data['old_circle_positions']\n",
    "current_number_of_topics = data['current_number_of_topics']\n",
    "topic_id = data['topic_id'] # parte del 1\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(topicvisexplorer)\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"name\")\n",
    "vis.load_single_corpus_data(\"models_output/single_corpus_europe_cambridge_analytica_lda_mallet_with_word_embedding_final_dataset_user_study_topics_labeled.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_corpus_data = topicvisexplorer.single_corpus_data\n",
    "optimization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import word embedding\n",
    "word_embedding_model = single_corpus_data['word_embedding_model']\n",
    "wordembedding = single_corpus_data['word_embedding_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for optimization\n",
    "\n",
    "with open('data_for_optimization/list_relevant_documents_with_vectors_for_topic_splitting.pickle', 'rb') as handle:\n",
    "    data_for_optimization = pickle.load(handle)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([               0,                1,                2,                3,\n",
       "                      4,                5,                6,                7,\n",
       "                      8,                9,               10, 'texto_completo',\n",
       "         'text_cleaner',       '0_vector',       '1_vector',       '2_vector',\n",
       "             '3_vector',       '4_vector',       '5_vector',       '6_vector',\n",
       "             '7_vector',       '8_vector',       '9_vector',      '10_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_optimization.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.; Idea mandar parte del arreglo del lambdata!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of relevant terms. Terms more relevant are going to appear first\n",
    "PreparedData_dict_with_more_info = single_corpus_data['tinfo_collection']\n",
    "list_terms_relevance = PreparedData_dict_with_more_info.loc[PreparedData_dict_with_more_info['Category'] == 'Topic'+str(topic_id)].sort_values(by='relevance', ascending=False)['Term'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [meet, new, product, transparent, camhatch, we...\n",
       "1         [week, trigger, see, lefty, trump, sadly, mayb...\n",
       "2                                          [finest, remove]\n",
       "3         [s, confident, privacy, protection, personal, ...\n",
       "4                   [obscure, canadian, tech, firm, remove]\n",
       "                                ...                        \n",
       "111740    [call, startup, bring, value, industrial, sect...\n",
       "111741    [mimic, disabled, people, sexually, abuse, wom...\n",
       "111742    [make, upset, let, delete, awful, status, pop,...\n",
       "111743                 [use, hashtag, ad, facebook, remove]\n",
       "111744    [digital, age, datum, make, well, informed, de...\n",
       "Name: text_cleaner, Length: 111745, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_optimization['text_cleaner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1_vector</th>\n",
       "      <th>2_vector</th>\n",
       "      <th>3_vector</th>\n",
       "      <th>4_vector</th>\n",
       "      <th>5_vector</th>\n",
       "      <th>6_vector</th>\n",
       "      <th>7_vector</th>\n",
       "      <th>8_vector</th>\n",
       "      <th>9_vector</th>\n",
       "      <th>10_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067683</td>\n",
       "      <td>0.075224</td>\n",
       "      <td>0.111145</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.087522</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.133339</td>\n",
       "      <td>0.066889</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.005832044226394828, 0.004824629630888921, ...</td>\n",
       "      <td>[0.17334513068948354, 0.4751854061034919, -1.0...</td>\n",
       "      <td>[0.007541435136715791, -0.005965379836311513, ...</td>\n",
       "      <td>[-0.11085861998157043, -0.053958353831603745, ...</td>\n",
       "      <td>[-0.16557050217070923, 0.04593195003303663, 0....</td>\n",
       "      <td>[-8.961415625208247e-05, 0.0001998736659061251...</td>\n",
       "      <td>[-0.4008968984995249, 0.33226708981874253, -0....</td>\n",
       "      <td>[0.008453479881438852, -0.061913447329061455, ...</td>\n",
       "      <td>[-3.1126375603207634e-05, 0.000324685262853563...</td>\n",
       "      <td>[0.006710508750586541, 0.005146126403815288, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095597</td>\n",
       "      <td>0.098896</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.078630</td>\n",
       "      <td>0.078384</td>\n",
       "      <td>0.125669</td>\n",
       "      <td>0.079250</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.47801444855747377, -1.406873718365432, 0.90...</td>\n",
       "      <td>[-6.75392171274325e-05, -2.130824873347592e-05...</td>\n",
       "      <td>[-6.702237811850864e-05, -2.114604035341472e-0...</td>\n",
       "      <td>[-6.693600602147853e-05, -2.1092522047183593e-...</td>\n",
       "      <td>[-0.17950507357764423, -0.05090236161026951, 0...</td>\n",
       "      <td>[0.00011658700321959259, 2.6941313763018115e-0...</td>\n",
       "      <td>[-0.0003428991963687622, -0.000321200400094312...</td>\n",
       "      <td>[0.0010872332971416654, 0.008660244556040197, ...</td>\n",
       "      <td>[-0.00021789716439002405, 0.000529974763367135...</td>\n",
       "      <td>[-0.004681282343824478, 0.007711118507131687, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.106643</td>\n",
       "      <td>0.106413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>...</td>\n",
       "      <td>[-7.068185425396223e-05, 0.0001059592759702354...</td>\n",
       "      <td>[-6.940999679727611e-05, 0.0001058115776686463...</td>\n",
       "      <td>[-5.7999792318241816e-05, 9.666439291322604e-0...</td>\n",
       "      <td>[-5.586606670249239e-05, 9.499314182903618e-05...</td>\n",
       "      <td>[-5.5158763558438295e-05, 9.45056090131402e-05...</td>\n",
       "      <td>[-8.213255051714441e-05, 0.0001150737152784131...</td>\n",
       "      <td>[-5.368500802660492e-05, 9.3279559223447e-05, ...</td>\n",
       "      <td>[-5.1279670230997e-05, 9.123190102400258e-05, ...</td>\n",
       "      <td>[-5.0732120058682995e-05, 9.091558968066238e-0...</td>\n",
       "      <td>[-4.9866681308685656e-05, 9.022851008921862e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083831</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.136948</td>\n",
       "      <td>0.097352</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>0.077989</td>\n",
       "      <td>0.089814</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.001956692503767954, -0.0014925585828677868,...</td>\n",
       "      <td>[0.7237029657366634, 0.7550907500003632, -0.63...</td>\n",
       "      <td>[0.48076796395451993, 0.25955325932636697, 0.3...</td>\n",
       "      <td>[0.0001122247154512479, 0.00018976365345224622...</td>\n",
       "      <td>[-0.011298434824226433, -0.2411481819126493, -...</td>\n",
       "      <td>[7.353772724627561e-05, -0.0004141479512327350...</td>\n",
       "      <td>[0.007881602803934129, 0.0034839859345083823, ...</td>\n",
       "      <td>[0.00011209843165715938, 0.0001891622232506051...</td>\n",
       "      <td>[-0.00028662316950089917, 0.000659989788346138...</td>\n",
       "      <td>[-0.008899104187946705, 0.010824778607911867, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.148244</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.093189</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.093545</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.07708563475614483, -0.08111963419287349, -0...</td>\n",
       "      <td>[0.027712605423914738, -0.015586473551593372, ...</td>\n",
       "      <td>[0.0025146853035380445, -0.0037733842618763447...</td>\n",
       "      <td>[9.155319560250064e-06, -0.0001351459086436079...</td>\n",
       "      <td>[9.159173913531049e-06, -0.0001351810369669692...</td>\n",
       "      <td>[0.01230773606908997, -0.01799366630439181, 0....</td>\n",
       "      <td>[9.164873006284324e-06, -0.0001356177135676262...</td>\n",
       "      <td>[0.000405963665514264, -0.00035627104989544023...</td>\n",
       "      <td>[9.143537766931331e-06, -0.0001351759810859221...</td>\n",
       "      <td>[9.136198116266314e-06, -0.0001350651427856064...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.067683  0.075224  0.111145  0.083138  0.087522  0.095556  0.066845   \n",
       "1  0.095597  0.098896  0.078370  0.078370  0.078370  0.112853  0.078630   \n",
       "2  0.087413  0.087413  0.087413  0.087643  0.087413  0.087413  0.087413   \n",
       "3  0.083831  0.092424  0.136948  0.097352  0.075758  0.109086  0.077989   \n",
       "4  0.081169  0.148244  0.096213  0.093189  0.081169  0.081169  0.093545   \n",
       "\n",
       "          7         8         9  ...  \\\n",
       "0  0.120091  0.133339  0.066889  ...   \n",
       "1  0.078384  0.125669  0.079250  ...   \n",
       "2  0.106643  0.106413  0.087413  ...   \n",
       "3  0.089814  0.075758  0.076336  ...   \n",
       "4  0.081761  0.081204  0.081169  ...   \n",
       "\n",
       "                                            1_vector  \\\n",
       "0  [-0.005832044226394828, 0.004824629630888921, ...   \n",
       "1  [0.47801444855747377, -1.406873718365432, 0.90...   \n",
       "2  [-7.068185425396223e-05, 0.0001059592759702354...   \n",
       "3  [0.001956692503767954, -0.0014925585828677868,...   \n",
       "4  [0.07708563475614483, -0.08111963419287349, -0...   \n",
       "\n",
       "                                            2_vector  \\\n",
       "0  [0.17334513068948354, 0.4751854061034919, -1.0...   \n",
       "1  [-6.75392171274325e-05, -2.130824873347592e-05...   \n",
       "2  [-6.940999679727611e-05, 0.0001058115776686463...   \n",
       "3  [0.7237029657366634, 0.7550907500003632, -0.63...   \n",
       "4  [0.027712605423914738, -0.015586473551593372, ...   \n",
       "\n",
       "                                            3_vector  \\\n",
       "0  [0.007541435136715791, -0.005965379836311513, ...   \n",
       "1  [-6.702237811850864e-05, -2.114604035341472e-0...   \n",
       "2  [-5.7999792318241816e-05, 9.666439291322604e-0...   \n",
       "3  [0.48076796395451993, 0.25955325932636697, 0.3...   \n",
       "4  [0.0025146853035380445, -0.0037733842618763447...   \n",
       "\n",
       "                                            4_vector  \\\n",
       "0  [-0.11085861998157043, -0.053958353831603745, ...   \n",
       "1  [-6.693600602147853e-05, -2.1092522047183593e-...   \n",
       "2  [-5.586606670249239e-05, 9.499314182903618e-05...   \n",
       "3  [0.0001122247154512479, 0.00018976365345224622...   \n",
       "4  [9.155319560250064e-06, -0.0001351459086436079...   \n",
       "\n",
       "                                            5_vector  \\\n",
       "0  [-0.16557050217070923, 0.04593195003303663, 0....   \n",
       "1  [-0.17950507357764423, -0.05090236161026951, 0...   \n",
       "2  [-5.5158763558438295e-05, 9.45056090131402e-05...   \n",
       "3  [-0.011298434824226433, -0.2411481819126493, -...   \n",
       "4  [9.159173913531049e-06, -0.0001351810369669692...   \n",
       "\n",
       "                                            6_vector  \\\n",
       "0  [-8.961415625208247e-05, 0.0001998736659061251...   \n",
       "1  [0.00011658700321959259, 2.6941313763018115e-0...   \n",
       "2  [-8.213255051714441e-05, 0.0001150737152784131...   \n",
       "3  [7.353772724627561e-05, -0.0004141479512327350...   \n",
       "4  [0.01230773606908997, -0.01799366630439181, 0....   \n",
       "\n",
       "                                            7_vector  \\\n",
       "0  [-0.4008968984995249, 0.33226708981874253, -0....   \n",
       "1  [-0.0003428991963687622, -0.000321200400094312...   \n",
       "2  [-5.368500802660492e-05, 9.3279559223447e-05, ...   \n",
       "3  [0.007881602803934129, 0.0034839859345083823, ...   \n",
       "4  [9.164873006284324e-06, -0.0001356177135676262...   \n",
       "\n",
       "                                            8_vector  \\\n",
       "0  [0.008453479881438852, -0.061913447329061455, ...   \n",
       "1  [0.0010872332971416654, 0.008660244556040197, ...   \n",
       "2  [-5.1279670230997e-05, 9.123190102400258e-05, ...   \n",
       "3  [0.00011209843165715938, 0.0001891622232506051...   \n",
       "4  [0.000405963665514264, -0.00035627104989544023...   \n",
       "\n",
       "                                            9_vector  \\\n",
       "0  [-3.1126375603207634e-05, 0.000324685262853563...   \n",
       "1  [-0.00021789716439002405, 0.000529974763367135...   \n",
       "2  [-5.0732120058682995e-05, 9.091558968066238e-0...   \n",
       "3  [-0.00028662316950089917, 0.000659989788346138...   \n",
       "4  [9.143537766931331e-06, -0.0001351759810859221...   \n",
       "\n",
       "                                           10_vector  \n",
       "0  [0.006710508750586541, 0.005146126403815288, -...  \n",
       "1  [-0.004681282343824478, 0.007711118507131687, ...  \n",
       "2  [-4.9866681308685656e-05, 9.022851008921862e-0...  \n",
       "3  [-0.008899104187946705, 0.010824778607911867, ...  \n",
       "4  [9.136198116266314e-06, -0.0001350651427856064...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_optimization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if optimization == True and int(topic_id)-1 in data_for_optimization.columns:\n",
    "    list_relevant_documents = data_for_optimization.to_dict('records')\n",
    "else:\n",
    "    list_relevant_documents = random.sample(single_corpus_data['relevantDocumentsDict'],10000)\n",
    "    list_relevant_documents = pd.DataFrame(list_relevant_documents).sort_values(int(topic_id)-1, ascending=False).reset_index()\n",
    "    #the idea is do this only ONCE! and tenerlo precalculado para el user study\n",
    "    print('cleaning sample fo text')\n",
    "    list_relevant_documents[name_tokenizacion] = list_relevant_documents[name_column_text].apply(lambda x: text_cleaner(x))\n",
    "    list_relevant_documents = list_relevant_documents.to_dict('records')\n",
    "    print('cleaning documents seeds topic a')\n",
    "\n",
    "    new_document_seeds_TopicA = pd.DataFrame(new_document_seeds_TopicA).reset_index()\n",
    "    new_document_seeds_TopicA[name_tokenizacion] = new_document_seeds_TopicA[name_column_text].apply(lambda x: text_cleaner(x))\n",
    "    new_document_seeds_TopicA = new_document_seeds_TopicA.to_dict('records')\n",
    "    print('cleaning documents seeds topic B')\n",
    "\n",
    "    new_document_seeds_TopicB = pd.DataFrame(new_document_seeds_TopicB).reset_index()\n",
    "    new_document_seeds_TopicB[name_tokenizacion] = new_document_seeds_TopicB[name_column_text].apply(lambda x: text_cleaner(x))\n",
    "    new_document_seeds_TopicB = new_document_seeds_TopicB.to_dict('records')\n",
    "    print('getting new subtopics')\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/////////////////////////////Parameters////////////////////////////\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['usernameremoved', 'link', 'remove', 'usernameremove', 'amp', 'linkremoved',' <link removed>','usernameremoved','<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "\n",
    "name_column_text = 'texto_completo'\n",
    "name_tokenizacion = 'text_cleaner'\n",
    "#/////////////////////////////Parameters////////////////////////////\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import importlib\n",
    "import topicvisexplorer\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy import spatial\n",
    "\n",
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "import unidecode\n",
    "import _prepare\n",
    "import _prepare_single_topic\n",
    "import gensim_helpers\n",
    "import spacy\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "#libraries to tokenize text\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this should be easy to change for users\n",
    "punctuation+=\"¡¿<>'`\"\n",
    "punctuation+='\"'\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    \n",
    "\n",
    "#Remove digits and puntuaction\n",
    "remove_digits = str.maketrans(digits, ' '*len(digits))#remove_digits = str.maketrans('', '', digits)\n",
    "remove_punctuation = str.maketrans(punctuation, ' '*len(punctuation))#remove_punctuation = str.maketrans('', '', punctuation)\n",
    "remove_hashtags_caracter = str.maketrans('#', ' '*len('#'))\n",
    "#las palabras de los hashtag se mantiene, pero no el simbolo. \n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "#this should be easy to change for users\n",
    "punctuation+=\"¡¿<>'`\"\n",
    "punctuation+='\"'\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    \n",
    "\n",
    "#Remove digits and puntuaction\n",
    "remove_digits = str.maketrans(digits, ' '*len(digits))#remove_digits = str.maketrans('', '', digits)\n",
    "remove_punctuation = str.maketrans(punctuation, ' '*len(punctuation))#remove_punctuation = str.maketrans('', '', punctuation)\n",
    "remove_hashtags_caracter = str.maketrans('#', ' '*len('#'))\n",
    "#las palabras de los hashtag se mantiene, pero no el simbolo. \n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "def sent_to_words(sentence):\n",
    "    return tknzr.tokenize(sentence)\n",
    "    \n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    doc = nlp(\" \".join(texts)) \n",
    "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "        \n",
    "def text_cleaner(tweet):\n",
    "\n",
    "    tweet = tweet.translate(remove_digits)\n",
    "    #tweet = tweet.lower() it wasn't a good idea,, we lost a lot of\n",
    "    tweet = tweet.translate(remove_punctuation)\n",
    "    tweet = tweet.translate(remove_hashtags_caracter)\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    tweet = sent_to_words(tweet)\n",
    "\n",
    "    tweet = remove_stopwords(tweet)\n",
    "\n",
    "    new_tweet  = []\n",
    "    for elem in tweet:\n",
    "        if len(elem)>0:\n",
    "            new_tweet.append(elem[0])\n",
    "    tweet = lemmatization(new_tweet, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    return tweet[0]\n",
    "\n",
    "\n",
    "def getDocumentVector(text, wordembedding ,  list_terms_relevance):   \n",
    "\n",
    "    document_vector = np.array([0.0]*300)# 300 dimensions\n",
    "    \n",
    "    for word in text:  \n",
    "        #print('esta es una word', word)\n",
    "        if word in list_terms_relevance:\n",
    "            raking_word = float(list_terms_relevance.index(word)+1)\n",
    "            if word in wordembedding: #if word in wordembedding.wv:\n",
    "                #print(\"WORD FOUND\", word, raking_word)\n",
    "                document_vector+=wordembedding[word]/raking_word #aqui hay que ponderar\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"WARNING, Word not found:\", word)    \n",
    "\n",
    "    return document_vector\n",
    "\n",
    "\n",
    "def get_initial_document_vector_by_class(list_terms_relevance, topic_id, name_tokenizacion,documents_class_A, documents_class_B, wordembedding):\n",
    "    relevantDocumentsvector_class_A = 0.0\n",
    "    list_documents_A = []\n",
    "    for row in documents_class_A:\n",
    "        current_text = row[name_tokenizacion]\n",
    "        current_contribution = row[str(int(topic_id) -1)]\n",
    "        current_document_vector = getDocumentVector(current_text, wordembedding,  list_terms_relevance)    \n",
    "        relevantDocumentsvector_class_A+= current_contribution*current_document_vector\n",
    "        list_documents_A.append((current_contribution, current_text))\n",
    "    \n",
    "    relevantDocumentsvector_class_B= 0.0\n",
    "    list_documents_B = []\n",
    "    for row in documents_class_B:\n",
    "        current_text = row[name_tokenizacion]\n",
    "        current_contribution = row[str(int(topic_id) -1)]\n",
    "        current_document_vector = getDocumentVector(current_text, wordembedding,  list_terms_relevance)    \n",
    "        relevantDocumentsvector_class_B+= current_contribution*current_document_vector\n",
    "        list_documents_B.append((current_contribution, current_text))\n",
    "\n",
    "    return(relevantDocumentsvector_class_A, relevantDocumentsvector_class_B, list_documents_A, list_documents_B)\n",
    "\n",
    "\n",
    "\n",
    "def fill_lists_documents_a_b(row, list_terms_relevance, vector_A, vector_B, documents_A, documents_B, most_relevant_documents_topic):\n",
    "\n",
    "    current_contribution = row[int(topic_id)-1]\n",
    "    current_text = row[name_tokenizacion]\n",
    "    current_document_vector = getDocumentVector(current_text, wordembedding, list_terms_relevance).reshape(-1, 1)\n",
    "    similarity_vectorA_currentvector = np.arccos(spatial.distance.cosine(vector_A, current_document_vector)-1) / np.pi\n",
    "    similarity_vectorB_currentvector = np.arccos(spatial.distance.cosine(vector_B, current_document_vector)-1) / np.pi\n",
    "\n",
    "    #I need this information to get the matrix of most relevant documents according to the similarity score\n",
    "\n",
    "    #most_relevant_documents_topic.add((similarity_vectorA_currentvector,similarity_vectorB_currentvector,  current_contribution, row[name_column_text]))\n",
    "\n",
    "    if similarity_vectorA_currentvector>= similarity_vectorB_currentvector:\n",
    "        #append element to documentsA\n",
    "        documents_A.append((current_contribution, row[name_tokenizacion]))\n",
    "        most_relevant_documents_topic.add((similarity_vectorA_currentvector,0,  current_contribution, row[name_column_text]))\n",
    "\n",
    "    else:\n",
    "        documents_B.append((current_contribution, row[name_tokenizacion]))\n",
    "        most_relevant_documents_topic.add((0,similarity_vectorB_currentvector,  current_contribution, row[name_column_text])) #QUIZAS LO CORRECTO Es que en vez de 0, sea 1-similarity_vectorB_currentvector\n",
    "\n",
    "\n",
    "\n",
    "def create_two_list_of_documents(list_terms_relevance, list_relevant_documents, topic_id, name_tokenizacion,name_column_text, new_document_seeds_TopicA, new_document_seeds_TopicB, wordembedding):\n",
    "    documents_A = []\n",
    "    documents_B = []\n",
    "    most_relevant_documents_topic = set()\n",
    "    new_document_seeds_TopicA_df = pd.DataFrame(new_document_seeds_TopicA)\n",
    "    new_document_seeds_TopicB_df = pd.DataFrame(new_document_seeds_TopicB)\n",
    "    \n",
    "    vector_A, vector_B, seeds_documents_A, seeds_documents_B = get_initial_document_vector_by_class(list_terms_relevance, topic_id, name_tokenizacion,new_document_seeds_TopicA, new_document_seeds_TopicB, wordembedding)\n",
    "    vector_A = vector_A.reshape(-1, 1)\n",
    "    vector_B = vector_B.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    list_relevant_documents = pd.DataFrame(list_relevant_documents).sort_values(int(topic_id)-1, ascending=False).reset_index()\n",
    "    \n",
    "    print(' que es esta wea',list_relevant_documents.head())    \n",
    "    list_relevant_documents.apply(lambda row:  fill_lists_documents_a_b(row, list_terms_relevance,vector_A, vector_B, documents_A, documents_B, most_relevant_documents_topic), axis=1)\n",
    "    new_document_seeds_TopicA_df.apply(lambda row:  fill_lists_documents_a_b(row, list_terms_relevance,vector_A, vector_B, documents_A, documents_B, most_relevant_documents_topic), axis=1)\n",
    "    new_document_seeds_TopicB_df.apply(lambda row:  fill_lists_documents_a_b(row, list_terms_relevance,vector_A, vector_B, documents_A, documents_B, most_relevant_documents_topic), axis=1)\n",
    "    \n",
    "    \n",
    "    print('Documentos en A temp ', len(documents_A))\n",
    "    print('Documentos en B temp ', len(documents_B))\n",
    "    \n",
    "        \n",
    "    return (seeds_documents_A, seeds_documents_B, documents_A, documents_B, most_relevant_documents_topic)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def getCorpusDictionaryfromSentences(sentences):\n",
    "    print('lda started')\n",
    "    #data_words = list(sent_to_words(sentences))\n",
    "    #data_ready = process_words(sentences)  # processed Text Data!\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(sentences)\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in sentences]\n",
    "    # Build LDA model\n",
    "    print('sendind corpus and dictionary')\n",
    "    return (corpus, id2word)\n",
    "\n",
    "\n",
    "def get_new_subtopics(list_terms_relevance, list_relevant_documents, topic_id, name_tokenizacion,name_column_text, new_document_seeds_TopicA, new_document_seeds_TopicB, wordembedding):\n",
    "    final_list_A = []\n",
    "    final_list_B = []\n",
    "\n",
    "    seeds_documents_A, seeds_documents_B, documents_A, documents_B, most_relevant_documents_topic = create_two_list_of_documents(list_terms_relevance, list_relevant_documents, topic_id, name_tokenizacion,name_column_text, new_document_seeds_TopicA, new_document_seeds_TopicB, wordembedding)\n",
    "    #print('seeds documents a', len(seeds_documents_A))\n",
    "    #print('seeds documents b', len(seeds_documents_B))\n",
    "    #print('seeds documents a', len(documents_A))\n",
    "    #print('seeds documents a', len(documents_B))\n",
    "    \n",
    "    print('tipo seeds', type(seeds_documents_A))\n",
    "    print(seeds_documents_A)\n",
    "    for contribution, text in seeds_documents_A:\n",
    "        final_list_A.extend([text]*int(contribution*100))\n",
    "    for contribution, text in seeds_documents_B:\n",
    "        final_list_B.extend([text]*int(contribution*100))\n",
    "\n",
    "    for contribution, text in documents_A:\n",
    "        final_list_A.extend([text]*int(contribution*100))\n",
    "    for contribution, text in documents_B:\n",
    "        final_list_B.extend([text]*int(contribution*100))\n",
    "    \n",
    "    freq_topic_A = len(final_list_A)/(len(final_list_A)+len(final_list_B))\n",
    "    freq_topic_B = len(final_list_B)/(len(final_list_A)+len(final_list_B))\n",
    "    \n",
    "    return(getCorpusDictionaryfromSentences(final_list_A), getCorpusDictionaryfromSentences(final_list_B), most_relevant_documents_topic, freq_topic_A, freq_topic_B)\n",
    "\n",
    "def update_topic_term_dists(row, total_frequency):\n",
    "    row['topic_term_dists'] = row['term_frequency']/total_frequency\n",
    "    return row\n",
    "\n",
    "    \n",
    "def extract_data_without_topic_model(corpus, dictionary):\n",
    "\n",
    "\n",
    "    topic_model = None\n",
    "    if not gensim.matutils.ismatrix(corpus):\n",
    "        corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))\n",
    "    else:\n",
    "        corpus_csc = corpus\n",
    "        # Need corpus to be a streaming gensim list corpus for len and inference functions below:\n",
    "        corpus = gensim.matutils.Sparse2Corpus(corpus_csc)\n",
    "\n",
    "    vocab = list(dictionary.token2id.keys())\n",
    "    \n",
    "    beta = 0.01\n",
    "    fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)\n",
    "    term_freqs = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]\n",
    "    term_freqs[term_freqs == 0] = beta\n",
    "\n",
    "    assert term_freqs.shape[0] == len(dictionary),\\\n",
    "        'Term frequencies and dictionary have different shape {} != {}'.format(\n",
    "        term_freqs.shape[0], len(dictionary))\n",
    "\n",
    "\n",
    "\n",
    "    topic_term_dists = term_freqs/term_freqs.sum(axis=0) # esta bien esto! \n",
    "    return {'topic_term_dists': topic_term_dists, 'vocab': vocab, 'term_frequency': term_freqs}\n",
    "\n",
    "\n",
    "def change_frequency_on_prepared_data(row, new_subtopic_df, total_sum_frequency_corpus):\n",
    "    current_term = row['Term']\n",
    "    current_total = row['Total']\n",
    "    new_subtopic_df = pd.DataFrame(new_subtopic_df)\n",
    "    current_total_new_subtopic_df = new_subtopic_df['term_frequency'].sum()\n",
    "    if current_term in list(new_subtopic_df['vocab']) and current_total>0:\n",
    "        new_subtopic_df = pd.DataFrame(new_subtopic_df)\n",
    "        old_freq = row['Freq']\n",
    "        new_prob = float(new_subtopic_df.loc[new_subtopic_df['vocab'] == current_term]['topic_term_dists'])\n",
    "        current_frequency = float(new_subtopic_df.loc[new_subtopic_df['vocab'] == current_term]['term_frequency'])\n",
    "\n",
    "        row['Freq'] = new_prob*row['Total']\n",
    "        row['logprob'] = np.log(new_prob)\n",
    "        #row['loglift'] = np.log(new_prob/(current_total/total_sum_frequency_corpus))     \n",
    "        row['loglift'] = np.log(new_prob/(current_frequency/current_total_new_subtopic_df))                 \n",
    "\n",
    "    else:\n",
    "        row['Freq'] = 0    \n",
    "        row['logprob'] = 0\n",
    "        row['loglift'] = 0  \n",
    "    return row\n",
    "    \n",
    "\n",
    "\n",
    "#Ojo, la frecuencia a actualizar sera del primer parametro q se le pase a la funcion, el q uno llava data model a\n",
    "#en algun momento ahbra que intercambiar, data model a debe ser data model b. \n",
    "def update_current_freq_and_total_freq_on_prepared_data(row, data_model_A_df, data_model_B_df, list_terms_A, list_terms_B, total_sum_frequency_corpus):\n",
    "    current_term = row['Term']\n",
    "    if(current_term in list_terms_A):\n",
    "        term_frequency_A = float(data_model_A_df.loc[data_model_A_df['vocab']==current_term]['term_frequency'])\n",
    "    else:\n",
    "        term_frequency_A = 0\n",
    "    if(current_term in list_terms_B):\n",
    "        term_frequency_B = float(data_model_B_df.loc[data_model_B_df['vocab']==current_term]['term_frequency'])\n",
    "    else:\n",
    "        term_frequency_B = 0\n",
    "    row['Total'] = term_frequency_A+term_frequency_B\n",
    "    row['Freq'] = term_frequency_A\n",
    "    if(current_term in list_terms_A):\n",
    "        new_prob = float(data_model_A_df.loc[data_model_A_df['vocab'] == current_term]['topic_term_dists'])\n",
    "        row['logprob'] = np.log(new_prob)\n",
    "        row['loglift'] = np.log(new_prob/((term_frequency_A+term_frequency_B)/total_sum_frequency_corpus))\n",
    "    else:\n",
    "        row['logprob'] = 0\n",
    "        row['loglift'] = 0 #auqnue la verdad en evz de cero, creo que el valor debiese ser - inf\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " que es esta wea    level_0  index         0         1         2         3         4         5  \\\n",
      "0        0      2  0.114846  0.088000  0.072150  0.072150  0.103749  0.072150   \n",
      "1        1      1  0.102694  0.084175  0.121197  0.084175  0.084175  0.084175   \n",
      "2        2      0  0.090909  0.074516  0.074516  0.074519  0.074876  0.148599   \n",
      "3        3      4  0.090879  0.079745  0.138139  0.091633  0.085401  0.079745   \n",
      "4        4      7  0.090731  0.074592  0.074516  0.085725  0.074538  0.075877   \n",
      "\n",
      "          6         7         8         9        10  \\\n",
      "0  0.072150  0.104043  0.124479  0.092942  0.083340   \n",
      "1  0.084175  0.102703  0.084175  0.084175  0.084180   \n",
      "2  0.091246  0.098453  0.106942  0.074591  0.090834   \n",
      "3  0.079745  0.079757  0.079745  0.097289  0.097923   \n",
      "4  0.090909  0.074516  0.123719  0.106098  0.128781   \n",
      "\n",
      "                                      texto_completo  \\\n",
      "0  The trouble is they get away with it. If comme...   \n",
      "1  <usernameremoved>   #Google Takes Aim at USA B...   \n",
      "2  Two thirds into into the meeting, #Zuckerberg ...   \n",
      "3  Facebook offers to sell back to us what is rig...   \n",
      "4  Interesting article from <usernameremoved> abo...   \n",
      "\n",
      "                                        text_cleaner  \n",
      "0  [trouble, comment, pounce, press, bring, atten...  \n",
      "1  [usernameremoved, take, biometric, privacy, la...  \n",
      "2  [third, meeting, still, reply, single, questio...  \n",
      "3  [offer, sell, back, rightly, first, place, che...  \n",
      "4  [interesting, article, usernameremoved, data, ...  \n",
      "Documentos en A temp  2\n",
      "Documentos en B temp  10\n",
      "tipo seeds <class 'list'>\n",
      "[(0.2755929161793122, ['dogsoftwitter', 'capone', 'vid', 'attention', 's', 'interested', 'fostering', 'cost', 'adopt', 'dog', 'mail', 'usernameremove', 'mustlovedogsnyc', 'usernameremoved', 'contact', 'rescue', 'amp', 'guide', 'link', 'remove'])]\n",
      "lda started\n",
      "sendind corpus and dictionary\n",
      "lda started\n",
      "sendind corpus and dictionary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results  = get_new_subtopics(list_terms_relevance, list_relevant_documents, topic_id, name_tokenizacion,name_column_text, new_document_seeds_TopicA, new_document_seeds_TopicB, word_embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for topic splitting has been saved\n",
      "Geeting new term-topic distributions\n",
      "actualizando el modelo A and B\n",
      "Getting most relevant documents\n",
      "Getting new prepared data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a52dd74fe41c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Topic'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Topic'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mupdate_current_freq_and_total_freq_on_prepared_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_model_A_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_model_B_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_terms_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_terms_B\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_sum_frequency_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7551\u001b[0m         )\n\u001b[1;32m-> 7552\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-a52dd74fe41c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Topic'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_tinfo_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Topic'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mupdate_current_freq_and_total_freq_on_prepared_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_model_A_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_model_B_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_terms_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_terms_B\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_sum_frequency_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-b71a673ffbe4>\u001b[0m in \u001b[0;36mupdate_current_freq_and_total_freq_on_prepared_data\u001b[1;34m(row, data_model_A_df, data_model_B_df, list_terms_A, list_terms_B, total_sum_frequency_corpus)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mterm_frequency_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Total'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterm_frequency_A\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mterm_frequency_B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m     \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Freq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterm_frequency_A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_term\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_terms_A\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mnew_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_model_A_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_model_A_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vocab'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcurrent_term\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_term_dists'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model_topic_A, model_topic_B, most_relevant_documents_topic, freq_topic_A, freq_topic_B = results\n",
    "#  CREAR PICKLEEE!!! CON ESTA DATAAA!!!\n",
    "with open('models_output/testing_spliting_models_topic_A_B.pkl', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=4) #protocol 4 is compatible with python 3.6+\n",
    "    print(\"Results for topic splitting has been saved\")\n",
    "print('Geeting new term-topic distributions')\n",
    "# Get new term-topic distributions in the new subtopics\n",
    "#get new distribution of terms, topic A\n",
    "corpus_topic_A, dictionary_topic_A = model_topic_A\n",
    "data_model_A = extract_data_without_topic_model(corpus_topic_A, dictionary_topic_A)\n",
    "\n",
    "\n",
    "corpus_topic_B, dictionary_topic_B = model_topic_B\n",
    "data_model_B = extract_data_without_topic_model(corpus_topic_B, dictionary_topic_B)\n",
    "print('actualizando el modelo A and B')\n",
    "#filtrar por terminos que si aparezcan en lists terms relevance\n",
    "df_temp = pd.DataFrame(data_model_A)\n",
    "df_temp = df_temp[df_temp['vocab'].isin(list_terms_relevance)]\n",
    "data_model_A = df_temp.to_dict()\n",
    "df_temp = pd.DataFrame(data_model_B)\n",
    "df_temp = df_temp[df_temp['vocab'].isin(list_terms_relevance)]\n",
    "data_model_B = df_temp.to_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Get most relevant documents\n",
    "print('Getting most relevant documents')\n",
    "new_dict = dict()\n",
    "#set columns of the new subtopics to NaN values\n",
    "df = pd.DataFrame(single_corpus_data['relevantDocumentsDict'])\n",
    "\n",
    "df[int(topic_id-1)]= 0.0\n",
    "df[current_number_of_topics]= 0.0\n",
    "\n",
    "for row in most_relevant_documents_topic:\n",
    "\n",
    "    contribution_to_topic_a = row[0]\n",
    "    contribution_to_topic_b = row[1]\n",
    "    indexs = df.index[df['texto_completo'] == row[-1]].tolist()\n",
    "    if len(indexs)<1:\n",
    "        print('Error, text not found')\n",
    "    #set final contribution to topic a, is contribution to topic_a multiply by the previous contribuiton\n",
    "    df.loc[indexs,int(topic_id-1)] = contribution_to_topic_a\n",
    "    df.loc[indexs,current_number_of_topics] = contribution_to_topic_b\n",
    "\n",
    "\n",
    "#order columns\n",
    "intList=sorted([i for i in df.columns.values if type(i) is int])\n",
    "strList=sorted([i for i in df.columns.values if type(i) is str])\n",
    "new_order = intList+strList\n",
    "df = df[new_order]\n",
    "single_corpus_data['relevantDocumentsDict'] = df.to_dict('records')\n",
    "\n",
    "new_dict['relevantDocumentsDict_fromPython'] =json.dumps( single_corpus_data['relevantDocumentsDict'])\n",
    "\n",
    "#Get prepared data\n",
    "print('Getting new prepared data')\n",
    "temp = single_corpus_data['PreparedDataObtained']\n",
    "\n",
    "#update MdsDat\n",
    "#add temporal coordinates. We are going to change these later with the new topic similarity metric.\n",
    "temp['mdsDat']['x'].append(temp['mdsDat']['x'][topic_id-1])\n",
    "temp['mdsDat']['y'].append(temp['mdsDat']['y'][topic_id-1])\n",
    "temp['mdsDat']['topics'].append(len(temp['mdsDat']['topics'])+1)\n",
    "temp['mdsDat']['cluster'].append(temp['mdsDat']['cluster'][topic_id-1])\n",
    "#update the frequency of the topic\n",
    "old_frequency = temp['mdsDat']['Freq'][topic_id-1]\n",
    "temp['mdsDat']['Freq'][topic_id-1] = old_frequency*freq_topic_A\n",
    "temp['mdsDat']['Freq'].append(old_frequency*freq_topic_B)\n",
    "#Update topic.order\n",
    "temp['topic.order'].append(len(temp['topic.order'])+1)\n",
    "\n",
    "temp_tinfo_df = pd.DataFrame(temp[ 'tinfo'])\n",
    "temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(topic_id)].sort_values(by=['Freq'], ascending=False)\n",
    "temp_tinfo_df = pd.DataFrame(temp[ 'tinfo'])\n",
    "\n",
    "data_model_A_df = pd.DataFrame(data_model_A)\n",
    "data_model_B_df = pd.DataFrame(data_model_B)\n",
    "total_sum_frequency_corpus = data_model_A_df['term_frequency'].sum()+data_model_B_df['term_frequency'].sum()\n",
    "list_terms_A = list(data_model_A_df['vocab'])\n",
    "list_terms_B = list(data_model_B_df['vocab'])\n",
    "\n",
    "\n",
    "temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(topic_id)] = temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(topic_id)].apply(lambda row:  update_current_freq_and_total_freq_on_prepared_data(row, data_model_A_df,data_model_B_df, list_terms_A, list_terms_B,total_sum_frequency_corpus), axis=1)\n",
    "\n",
    "\n",
    "#copy values for the new subtopic b\n",
    "temp2 = temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(topic_id)]\n",
    "temp2.Category = 'Topic'+str(current_number_of_topics+1) \n",
    "temp_tinfo_df = temp_tinfo_df.append(temp2, ignore_index=True)\n",
    "\n",
    "#update those values with the current terms probability\\\n",
    "temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(current_number_of_topics+1)] = temp_tinfo_df[temp_tinfo_df.Category == 'Topic'+str(current_number_of_topics+1)].apply(lambda row:  update_current_freq_and_total_freq_on_prepared_data(row, data_model_B_df,data_model_A_df, list_terms_B, list_terms_A,total_sum_frequency_corpus), axis=1)\n",
    "\n",
    "\n",
    "#save the new tinfo\n",
    "temp_tinfo_df.reset_index(drop=True, inplace=True)\n",
    "temp[ 'tinfo']  = temp_tinfo_df.to_dict(orient='list')\n",
    "\n",
    "\n",
    "\n",
    "print('New number of topics')\n",
    "\n",
    "#Get new topic similarity matrix\n",
    "print('Getting new topic similarity matrix')\n",
    "newClass = TopicVisExplorer(\"name\") #dejar esta en el codigo final\n",
    "word_embedding_model = single_corpus_data['word_embedding_model']\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6\n",
    "print('Calculando topic similarity metrix')\n",
    "\n",
    "\n",
    "lda_model = single_corpus_data['lda_model']\n",
    "corpus = single_corpus_data['corpus']\n",
    "id2word = single_corpus_data['id2word']\n",
    "matrix_documents_topic_contribution = pd.DataFrame(single_corpus_data['relevantDocumentsDict'])\n",
    "\n",
    "\n",
    "new_topic_similarity_matrix = newClass.calculate_topic_similarity_on_single_corpus_for_topic_splitting(current_number_of_topics, word_embedding_model, lda_model, corpus, id2word, matrix_documents_topic_contribution,topn_terms, topk_documents, relevance_lambda)\n",
    "single_corpus_data['topic_similarity_matrix'] = new_topic_similarity_matrix\n",
    "print('Topic similarity matrix has been calculated')\n",
    "\n",
    "\n",
    "\n",
    "old_circle_positions = json_file['old_circle_positions']\n",
    "\n",
    "for omega in old_circle_positions.keys():\n",
    "    old_circle_positions[omega].append(old_circle_positions[omega][topic_id-1])\n",
    "\n",
    "\n",
    "print('Calculating new circle positions with procrustes')\n",
    "\n",
    "new_circle_positions = get_circle_positions_from_old_matrix(old_circle_positions, new_topic_similarity_matrix )\n",
    "print('json new circle,. estas son las keys', json.loads(new_circle_positions).keys())\n",
    "print('primer arreglo', json.loads(new_circle_positions)['0.0'])\n",
    "\n",
    "single_corpus_data['new_circle_positions'] = new_circle_positions\n",
    "\n",
    "print('------- falta calcular el nuevo topic orderingX')                 \n",
    "topic_order =  single_corpus_data['topic.order']\n",
    "\n",
    "\n",
    "#visualizar neuvos resultados\n",
    "\n",
    "PreparedDataObtained = single_corpus_data['PreparedDataObtained'] \n",
    "\n",
    "#Return results in a dictionary\n",
    "\n",
    "\n",
    "new_dict['new_circle_positions'] = single_corpus_data['new_circle_positions'] \n",
    "\n",
    "data = [single_corpus_data['PreparedDataObtained']]\n",
    "data_json_format = []\n",
    "for elem in data:\n",
    "    elem = js.dumps(elem, cls=NumPyEncoder)\n",
    "    data_json_format.append(elem)\n",
    "\n",
    "new_dict['PreparedDataObtained_fromPython'] = js.loads(data_json_format[0])\n",
    "\n",
    "\n",
    "#The following line is necessary to delete inf and nan values that javascript JSON.parse cant process\n",
    "new_dict['PreparedDataObtained_fromPython']['tinfo'] = pd.DataFrame(new_dict['PreparedDataObtained_fromPython']['tinfo']).replace([np.inf, -np.inf, np.nan], 0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
