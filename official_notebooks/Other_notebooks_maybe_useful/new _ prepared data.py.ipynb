{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            console.log(\"estos son los terminos! quizas los idncies estan malos\", terms_topic_1)\n",
    "\n",
    "               // grab the bar-chart data for this topic only:\n",
    "               var dat2_test = lamData.filter(function(d) {\n",
    "                return d.Category == \"Topic\" + index_topic_name_1+1;\n",
    "                });\n",
    "            \n",
    "            // define relevance:\n",
    "                for (var i = 0; i < dat2_test.length; i++) {\n",
    "                    dat2_test[i].relevance = lambda.current * dat2_test[i].logprob +(1 - lambda.current) * dat2_test[i].loglift;\n",
    "                }\n",
    "\n",
    "            // sort by relevance:\n",
    "                    dat2_test.sort(fancysort(\"relevance\"));\n",
    "            \n",
    "            // truncate to the top R tokens:\n",
    "                    var dat3_test = dat2_test.slice(0, R);\n",
    "                    \n",
    "            console.log(\"esoty en topic on con estos datos\", lamData, dat3_test, dat2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#antiguo\n",
    "\n",
    "  for(var i = 0; i < terms_topic_1.length; i += 1) {            //we have a 'matrix'. There is the same information for all the terms.                                                                                 \n",
    "                var row_topic_1 = terms_topic_1[i]\n",
    "                var row_topic_2 = terms_topic_2.find( row => row.Term ===  terms_topic_1[i].Term)\n",
    "                \n",
    "                \n",
    "                \n",
    "                //we need the new probability of the term in the merged topic, and the probability term in all the corpus (check this with prepared data file)\n",
    "                console.log(row_topic_1)\n",
    "                console.log(row_topic_2)\n",
    "                console.log(\"anti wea topic 1\", row_topic_1.logprob, row_topic_1.loglift, row_topic_1.Freq, row_topic_1.Term, Math.exp(row_topic_1.logprob))\n",
    "\n",
    "                var new_probability_term = Math.exp(row_topic_1.logprob)+Math.exp(row_topic_2.logprob) \n",
    "                \n",
    "                var probability_term_in_corpus_1 = Math.exp(row_topic_1.logprob)/Math.exp(row_topic_1.loglift) \n",
    "                var probability_term_in_corpus_2 = Math.exp(row_topic_2.logprob)/Math.exp(row_topic_2.loglift) \n",
    "                var probability_term_in_corpus;\n",
    "                //chose the higher probability. I can't understand why the probability could be different. Please fix prepared data.\n",
    "                /*\n",
    "                if(probability_term_in_corpus_1>probability_term_in_corpus_2){\n",
    "                    probability_term_in_corpus = probability_term_in_corpus_1\n",
    "                }\n",
    "                else{\n",
    "                    probability_term_in_corpus = probability_term_in_corpus_2\n",
    "                }                \n",
    "\n",
    "                if(isNaN(probability_term_in_corpus)){\n",
    "                    probability_term_in_corpus= 0\n",
    "                }\n",
    "                */\n",
    "\n",
    "                probability_term_in_corpus = probability_term_in_corpus_1\n",
    "                var new_logprob = Math.log(new_probability_term)\n",
    "                var new_loglift = Math.log(new_probability_term/probability_term_in_corpus)\n",
    "                var new_frequency = row_topic_1.Freq+row_topic_2.Freq     \n",
    "                if(isNaN(new_loglift)){\n",
    "                    new_loglift= -Infinity\n",
    "                }\n",
    "                \n",
    "                //update values in first topic\n",
    "                row_topic_1.logprob = new_logprob //the log prob es la suma de ambas probabilidades. Luego se le aplica logaritmo natural a esa suma\n",
    "                row_topic_1.loglift = new_loglift                          \n",
    "                row_topic_1.Freq = new_frequency\n",
    "                \n",
    "                console.log(\"post wea topic 1\", row_topic_1.logprob, row_topic_1.loglift, row_topic_1.Freq, row_topic_1.Term, new_probability_term)\n",
    "                //console.log(\"anti wea topic 2\", row_topic_2.logprob, row_topic_2.loglift, row_topic_2.Freq, row_topic_2.Term)\n",
    "                //update in the second topic\n",
    "                row_topic_2.logprob = new_logprob //the log prob es la suma de ambas probabilidades. Luego se le aplica logaritmo natural a esa suma\n",
    "                row_topic_2.loglift = new_loglift                          \n",
    "                row_topic_2.Freq = new_frequency\n",
    "                //console.log(\"post wea topic 2\", row_topic_2.logprob, row_topic_2.loglift, row_topic_2.Freq, row_topic_2.Term)\n",
    "                                            \n",
    "            }      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topicvisexplorer.single_corpus_data\n",
    "import pickle\n",
    "import topicvisexplorer\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pyLDAvis Prepare\n",
    "===============\n",
    "Main transformation functions for preparing LDAdata to the visualization's data structures\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "#from past.builtins import basestring\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import logging\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from utils import NumPyEncoder\n",
    "try:\n",
    "    from sklearn.manifold import MDS, TSNE\n",
    "    sklearn_present = True\n",
    "except ImportError:\n",
    "    sklearn_present = False\n",
    "\n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "\n",
    "    def err(msg):\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err_msg = ('Number of rows of topic_term_dists does not match number of columns of '\n",
    "                   'doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "        err(err_msg)\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err_msg = ('Length of doc_lengths not equal to the number of rows in doc_topic_dists;'\n",
    "                   'both should be equal to the number of documents in the data.')\n",
    "        err(err_msg)\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err_msg = ('Number of terms in vocabulary does not match the number of columns of '\n",
    "                   'topic_term_dists (where each row of topic_term_dists is a probability '\n",
    "                   'distribution of terms for a given topic)')\n",
    "        err(err_msg)\n",
    "    if len(term_frequency) != W:\n",
    "        err_msg = ('Length of term_frequency not equal to the number of terms in the '\n",
    "                   'number of terms in the vocabulary (len of vocab)')\n",
    "        err(err_msg)\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "\n",
    "\n",
    "def _jensen_shannon(_P, _Q):\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "\n",
    "def _pcoa(pair_dists, n_components=2):\n",
    "    \"\"\"Principal Coordinate Analysis,\n",
    "    aka Classical Multidimensional Scaling\n",
    "    \"\"\"\n",
    "    # code referenced from skbio.stats.ordination.pcoa\n",
    "    # https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py\n",
    "\n",
    "    # pairwise distance matrix is assumed symmetric\n",
    "    pair_dists = np.asarray(pair_dists, np.float64)\n",
    "\n",
    "    # perform SVD on double centred distance matrix\n",
    "    n = pair_dists.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = - H.dot(pair_dists ** 2).dot(H) / 2\n",
    "    eigvals, eigvecs = np.linalg.eig(B)\n",
    "\n",
    "    # Take first n_components of eigenvalues and eigenvectors\n",
    "    # sorted in decreasing order\n",
    "    ix = eigvals.argsort()[::-1][:n_components]\n",
    "    eigvals = eigvals[ix]\n",
    "    eigvecs = eigvecs[:, ix]\n",
    "\n",
    "    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes\n",
    "    # at least 1 eigenvalue must be zero\n",
    "    eigvals[np.isclose(eigvals, 0)] = 0\n",
    "    if np.any(eigvals < 0):\n",
    "        ix_neg = eigvals < 0\n",
    "        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)\n",
    "        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)\n",
    "\n",
    "    return np.sqrt(eigvals) * eigvecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def js_PCoA(distributions):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Principal Coordinate Analysis\n",
    "    (aka Classical Multidimensional Scaling)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcoa : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    return _pcoa(dist_matrix)\n",
    "\n",
    "\n",
    "def js_MMDS(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Metric Multidimensional Scaling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mmds : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def js_TSNE(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tsne : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "\n",
    "\n",
    "def _topic_coordinates(mds, topic_term_dists, topic_proportion):\n",
    "    K = topic_term_dists.shape[0]\n",
    "    mds_res = mds(topic_term_dists)\n",
    "    assert mds_res.shape == (K, 2)\n",
    "    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1),\n",
    "                          'cluster': 1, 'Freq': topic_proportion * 100})\n",
    "    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26\n",
    "    return mds_df\n",
    "\n",
    "\n",
    "def _chunks(l, n):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def _job_chunks(l, n_jobs):\n",
    "    n_chunks = n_jobs\n",
    "    if n_jobs < 0:\n",
    "        # so, have n chunks if we are using all n cores/cpus\n",
    "        n_chunks = cpu_count() + 1 - n_jobs\n",
    "\n",
    "    return _chunks(l, n_chunks)\n",
    "\n",
    "\n",
    "def _find_relevance(log_ttd, log_lift, R, lambda_):\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index)\n",
    "    #return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)\n",
    "\n",
    "\n",
    "def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):\n",
    "    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])\n",
    "\n",
    "\n",
    "def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n",
    "                vocab, lambda_step, R, n_jobs):\n",
    "    # marginal distribution over terms (width of blue bars)\n",
    "    term_proportion = term_frequency / term_frequency.sum()\n",
    "\n",
    "    # compute the distinctiveness and saliency of the terms:\n",
    "    # this determines the R terms that are displayed when no topic is selected\n",
    "    topic_given_term = topic_term_dists / topic_term_dists.sum()\n",
    "    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
    "    distinctiveness = kernel.sum()\n",
    "    saliency = term_proportion * distinctiveness\n",
    "    # Order the terms for the \"default\" view by decreasing saliency:\n",
    "    default_term_info = pd.DataFrame({\n",
    "        'saliency': saliency,\n",
    "        'Term': vocab,\n",
    "        'Freq': term_frequency,\n",
    "        'Total': term_frequency,\n",
    "        'Category': 'Default'})\n",
    "    default_term_info = default_term_info.sort_values(\n",
    "        by='saliency', ascending=False).drop('saliency', 1) #by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
    "    # Rounding Freq and Total to integer values to match LDAvis code:\n",
    "    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n",
    "    default_term_info['Total'] = np.floor(default_term_info['Total'])\n",
    "    ranks = np.arange(R, 0, -1)\n",
    "    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n",
    "\n",
    "    # compute relevance and top terms for each topic\n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n",
    "\n",
    "    def topic_top_term_df(tup):\n",
    "        new_topic_id, (original_topic_id, topic_terms) = tup\n",
    "        term_ix = topic_terms.unique()\n",
    "        return pd.DataFrame({'Term': vocab[term_ix],\n",
    "                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n",
    "                             'Total': term_frequency[term_ix],\n",
    "                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n",
    "                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n",
    "                             'Category': 'Topic%d' % new_topic_id})\n",
    "\n",
    "    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n",
    "                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n",
    "                          for ls in _job_chunks(lambda_seq, n_jobs)))\n",
    "    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n",
    "    return pd.concat([default_term_info] + list(topic_dfs), sort=True)\n",
    "\n",
    "\n",
    "def _token_table(topic_info, term_topic_freq, vocab, term_frequency):\n",
    "    # last, to compute the areas of the circles when a term is highlighted\n",
    "    # we must gather all unique terms that could show up (for every combination\n",
    "    # of topic and value of lambda) and compute its distribution over topics.\n",
    "\n",
    "    # term-topic frequency table of unique terms across all topics and all values of lambda\n",
    "    term_ix = topic_info.index.unique()\n",
    "    term_ix = np.sort(term_ix)\n",
    "\n",
    "    top_topic_terms_freq = term_topic_freq[term_ix]\n",
    "    # use the new ordering for the topics\n",
    "    K = len(term_topic_freq)\n",
    "    top_topic_terms_freq.index = range(1, K + 1)\n",
    "    top_topic_terms_freq.index.name = 'Topic'\n",
    "\n",
    "    # we filter to Freq >= 0.5 to avoid sending too much data to the browser\n",
    "    token_table = pd.DataFrame({'Freq': top_topic_terms_freq.unstack()})\\\n",
    "        .reset_index().set_index('term').query('Freq >= 0.5')\n",
    "\n",
    "    token_table['Freq'] = token_table['Freq'].round()\n",
    "    token_table['Term'] = vocab[token_table.index.values].values\n",
    "    # Normalize token frequencies:\n",
    "    token_table['Freq'] = token_table.Freq / term_frequency[token_table.index]\n",
    "    return token_table.sort_values(by=['Term', 'Topic'])\n",
    "\n",
    "#Editar esto para hacer el merge\n",
    "def prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,\n",
    "            R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1,\n",
    "            plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=False):\n",
    "    print(\"Using Prepared Data script - sort_topics\", sort_topics)\n",
    "   \n",
    "    \n",
    "\n",
    "    # parse mds\n",
    "    if type(mds) == str:\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "    \n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    print(term_frequency)\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "    R = len(vocab)\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)\n",
    "    if (sort_topics):\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "    \n",
    "    topic_order = topic_proportion.index\n",
    "    # reorder all data based on new ordering of topics\n",
    "    topic_freq = topic_freq[topic_order]\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "    doc_topic_dists = doc_topic_dists[topic_order]\n",
    "    \n",
    "    # token counts for each term-topic combination (widths of red bars)\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    \n",
    "    # Quick fix for red bar width bug.  We calculate the\n",
    "    # term frequencies internally, using the topic term distributions and the\n",
    "    # topic frequencies, rather than using the user-supplied term frequencies.\n",
    "    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "    \n",
    "    topic_info = _topic_info(topic_term_dists, topic_proportion,\n",
    "                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "    \n",
    "    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)\n",
    "    \n",
    "    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "    \n",
    "    client_topic_order = [x + 1 for x in topic_order]\n",
    "    \n",
    "    return PreparedData(topic_coordinates, topic_info,\n",
    "                        token_table, R, lambda_step, plot_opts, client_topic_order)\n",
    "\n",
    "\n",
    "class PreparedData(namedtuple('PreparedData', ['topic_coordinates', 'topic_info', 'token_table',\n",
    "                                               'R', 'lambda_step', 'plot_opts', 'topic_order'])):\n",
    "\n",
    "    \n",
    "    def sorted_terms(self, topic=1, _lambda=1):\n",
    "        \"\"\"Retuns a dataframe using _lambda to calculate term relevance of a given topic.\"\"\"\n",
    "        tdf = pd.DataFrame(self.topic_info[self.topic_info.Category == 'Topic' + str(topic)])\n",
    "        if _lambda < 0 or _lambda > 1:\n",
    "            _lambda = 1\n",
    "        stdf = tdf.assign(relevance=_lambda * tdf['logprob'] + (1 - _lambda) * tdf['loglift'])\n",
    "        return stdf.sort_values('relevance', ascending=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {'mdsDat': self.topic_coordinates.to_dict(orient='list'),\n",
    "                'tinfo': self.topic_info.to_dict(orient='list'),\n",
    "                'token.table': self.token_table.to_dict(orient='list'),\n",
    "                'R': self.R,\n",
    "                'lambda.step': self.lambda_step,\n",
    "                'plot.opts': self.plot_opts,\n",
    "                'topic.order': self.topic_order}\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), cls=NumPyEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(topicvisexplorer)\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"name\")\n",
    "vis.load_single_corpus_data(\"models_output/single_corpus_europe_cambridge_analytica_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_dict = topicvisexplorer.single_corpus_data['data_dict']\n",
    "\n",
    "del(data_dict['mds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "0         117.0\n",
      "1         218.0\n",
      "2           1.0\n",
      "3        2194.0\n",
      "4         109.0\n",
      "          ...  \n",
      "31972       1.0\n",
      "31973       1.0\n",
      "31974       1.0\n",
      "31975       1.0\n",
      "31976       1.0\n",
      "Name: term_frequency, Length: 31977, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-3039ead0525e>:237: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "<ipython-input-17-3039ead0525e>:256: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "<ipython-input-17-3039ead0525e>:257: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n"
     ]
    }
   ],
   "source": [
    "temp = prepare(**data_dict)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
