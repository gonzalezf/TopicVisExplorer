{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Idea, do not depend of tinfo. \n",
    "Depender de los elementos claves de eso, para que el logprob y loglift coincidan!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tasks, \n",
    "1) why the frecuency is a float\n",
    "2) check probability and relevance function\n",
    "3) data dict, alguna forma de pasarle el doc_topic distribution? the idea is it should be faster\n",
    "4#cambiar beta a un numero muy chico\n",
    "5) check that prepare data funca, sin el 'mds' (el gensim helpers se lo manda al data_dict y ese se va a prepare data)\n",
    "6) para que se ocupa la saliency\n",
    "''';\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#topicvisexplorer.single_corpus_data\n",
    "import pickle\n",
    "import topicvisexplorer\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(topicvisexplorer)\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"name\")\n",
    "vis.load_single_corpus_data(\"models_output/single_corpus_europe_cambridge_analytica_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get my own tinfo (logprob, loglift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,\n",
    "            R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1,\n",
    "            plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=False):\n",
    "    print(\"Using Prepared Data script - sort_topics\", sort_topics)\n",
    "    \"\"\"Transforms the topic model distributions and related corpus data into\n",
    "    the data structures needed for the visualization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "        Matrix of topic-term probabilities. Where `n_terms` is `len(vocab)`.\n",
    "    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape `n_docs`\n",
    "        The length of each document, i.e. the number of words in each document.\n",
    "        The order of the numbers should be consistent with the ordering of the\n",
    "        docs in `doc_topic_dists`.\n",
    "    vocab : array-like, shape `n_terms`\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape `n_terms`\n",
    "        The count of each particular term over the entire corpus. The ordering\n",
    "        of these counts should correspond with `vocab` and `topic_term_dists`.\n",
    "    R : int\n",
    "        The number of terms to display in the barcharts of the visualization.\n",
    "        Default is 30. Recommended to be roughly between 10 and 50.\n",
    "    lambda_step : float, between 0 and 1\n",
    "        Determines the interstep distance in the grid of lambda values over\n",
    "        which to iterate when computing relevance.\n",
    "        Default is 0.01. Recommended to be between 0.01 and 0.1.\n",
    "    mds : function or a string representation of function\n",
    "        A function that takes `topic_term_dists` as an input and outputs a\n",
    "        `n_topics` by `2`  distance matrix. The output approximates the distance\n",
    "        between topics. See :func:`js_PCoA` for details on the default function.\n",
    "        A string representation currently accepts `pcoa` (or upper case variant),\n",
    "        `mmds` (or upper case variant) and `tsne` (or upper case variant),\n",
    "        if `sklearn` package is installed for the latter two.\n",
    "    n_jobs : int\n",
    "        The number of cores to be used to do the computations. The regular\n",
    "        joblib conventions are followed so `-1`, which is the default, will\n",
    "        use all cores.\n",
    "    plot_opts : dict, with keys 'xlab' and `ylab`\n",
    "        Dictionary of plotting options, right now only used for the axis labels.\n",
    "    sort_topics : sort topics by topic proportion (percentage of tokens covered). Set to false to\n",
    "        to keep original topic order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "        A named tuple containing all the data structures required to create\n",
    "        the visualization. To be passed on to functions like :func:`display`.\n",
    "        This named tuple can be represented as json or a python dictionary.\n",
    "        There is a helper function 'sorted_terms' that can be used to get\n",
    "        the terms of a topic using lambda to rank their relevance.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implements the method of `Sievert, C. and Shirley, K. (2014):\n",
    "    LDAvis: A Method for Visualizing and Interpreting Topics, ACL Workshop on\n",
    "    Interactive Language Learning, Visualization, and Interfaces.`\n",
    "\n",
    "    http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`save_json`: save json representation of a figure to file\n",
    "    :func:`save_html` : save html representation of a figure to file\n",
    "    :func:`show` : launch a local server and show a figure in a browser\n",
    "    :func:`display` : embed figure within the IPython notebook\n",
    "    :func:`enable_notebook` : automatically embed visualizations in IPython notebook\n",
    "   \"\"\"\n",
    "    \n",
    "\n",
    "    # parse mds\n",
    "    if type(mds) == str:\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "    \n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "    R = len(vocab)\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)\n",
    "    if (sort_topics):\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "    \n",
    "    topic_order = topic_proportion.index\n",
    "    # reorder all data based on new ordering of topics\n",
    "    topic_freq = topic_freq[topic_order]\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "    doc_topic_dists = doc_topic_dists[topic_order]\n",
    "    \n",
    "    # token counts for each term-topic combination (widths of red bars)\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    \n",
    "    # Quick fix for red bar width bug.  We calculate the\n",
    "    # term frequencies internally, using the topic term distributions and the\n",
    "    # topic frequencies, rather than using the user-supplied term frequencies.\n",
    "    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "    \n",
    "    topic_info = _topic_info(topic_term_dists, topic_proportion,\n",
    "                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "    \n",
    "    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)\n",
    "    \n",
    "    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "    \n",
    "    client_topic_order = [x + 1 for x in topic_order]\n",
    "    \n",
    "    return PreparedData(topic_coordinates, topic_info,\n",
    "                        token_table, R, lambda_step, plot_opts, client_topic_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#son numeros enteros\n",
    "data_dict['term_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_seq_items = 4000\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#def prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,        R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=False):\n",
    "#topic_info = _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "#def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,vocab, lambda_step, R, n_jobs):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new gensim helpers file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "### Getting data\n",
    "'''\n",
    "lda_model = topicvisexplorer.single_corpus_data['lda_model'][0]\n",
    "corpus = topicvisexplorer.single_corpus_data['corpus']\n",
    "id2word = topicvisexplorer.single_corpus_data['id2word']\n",
    "''';\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_dict = topicvisexplorer.single_corpus_data['data_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic_term_dists', 'doc_topic_dists', 'doc_lengths', 'vocab', 'term_frequency', 'mds'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([117., 218.,   1., ...,   1.,   1.,   1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['term_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.28584852e-04, 2.39585451e-04, 1.09901583e-06, ...,\n",
       "       1.09901583e-06, 1.09901583e-06, 1.09901583e-06])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_proportion = data_dict['term_frequency']/data_dict['term_frequency'].sum()\n",
    "term_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "909905.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['term_frequency'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_proportion = term_frequency / term_frequency.sum()\n",
    "log_lift = np.log(topic_term_dists / term_proportion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pyLDAvis Prepare\n",
    "===============\n",
    "Main transformation functions for preparing LDAdata to the visualization's data structures\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "#from past.builtins import basestring\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import logging\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from utils import NumPyEncoder\n",
    "try:\n",
    "    from sklearn.manifold import MDS, TSNE\n",
    "    sklearn_present = True\n",
    "except ImportError:\n",
    "    sklearn_present = False\n",
    "\n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "\n",
    "    def err(msg):\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err_msg = ('Number of rows of topic_term_dists does not match number of columns of '\n",
    "                   'doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "        err(err_msg)\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err_msg = ('Length of doc_lengths not equal to the number of rows in doc_topic_dists;'\n",
    "                   'both should be equal to the number of documents in the data.')\n",
    "        err(err_msg)\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err_msg = ('Number of terms in vocabulary does not match the number of columns of '\n",
    "                   'topic_term_dists (where each row of topic_term_dists is a probability '\n",
    "                   'distribution of terms for a given topic)')\n",
    "        err(err_msg)\n",
    "    if len(term_frequency) != W:\n",
    "        err_msg = ('Length of term_frequency not equal to the number of terms in the '\n",
    "                   'number of terms in the vocabulary (len of vocab)')\n",
    "        err(err_msg)\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "\n",
    "\n",
    "def _jensen_shannon(_P, _Q):\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "\n",
    "def _pcoa(pair_dists, n_components=2):\n",
    "    \"\"\"Principal Coordinate Analysis,\n",
    "    aka Classical Multidimensional Scaling\n",
    "    \"\"\"\n",
    "    # code referenced from skbio.stats.ordination.pcoa\n",
    "    # https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py\n",
    "\n",
    "    # pairwise distance matrix is assumed symmetric\n",
    "    pair_dists = np.asarray(pair_dists, np.float64)\n",
    "\n",
    "    # perform SVD on double centred distance matrix\n",
    "    n = pair_dists.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = - H.dot(pair_dists ** 2).dot(H) / 2\n",
    "    eigvals, eigvecs = np.linalg.eig(B)\n",
    "\n",
    "    # Take first n_components of eigenvalues and eigenvectors\n",
    "    # sorted in decreasing order\n",
    "    ix = eigvals.argsort()[::-1][:n_components]\n",
    "    eigvals = eigvals[ix]\n",
    "    eigvecs = eigvecs[:, ix]\n",
    "\n",
    "    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes\n",
    "    # at least 1 eigenvalue must be zero\n",
    "    eigvals[np.isclose(eigvals, 0)] = 0\n",
    "    if np.any(eigvals < 0):\n",
    "        ix_neg = eigvals < 0\n",
    "        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)\n",
    "        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)\n",
    "\n",
    "    return np.sqrt(eigvals) * eigvecs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def js_PCoA(distributions):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Principal Coordinate Analysis\n",
    "    (aka Classical Multidimensional Scaling)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcoa : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    return _pcoa(dist_matrix)\n",
    "\n",
    "\n",
    "def js_MMDS(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Metric Multidimensional Scaling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mmds : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def js_TSNE(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tsne : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "\n",
    "\n",
    "def _topic_coordinates(mds, topic_term_dists, topic_proportion):\n",
    "    K = topic_term_dists.shape[0]\n",
    "    mds_res = mds(topic_term_dists)\n",
    "    assert mds_res.shape == (K, 2)\n",
    "    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1),\n",
    "                          'cluster': 1, 'Freq': topic_proportion * 100})\n",
    "    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26\n",
    "    return mds_df\n",
    "\n",
    "\n",
    "def _chunks(l, n):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def _job_chunks(l, n_jobs):\n",
    "    n_chunks = n_jobs\n",
    "    if n_jobs < 0:\n",
    "        # so, have n chunks if we are using all n cores/cpus\n",
    "        n_chunks = cpu_count() + 1 - n_jobs\n",
    "\n",
    "    return _chunks(l, n_chunks)\n",
    "\n",
    "\n",
    "def _find_relevance(log_ttd, log_lift, R, lambda_):\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index)\n",
    "    #return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)\n",
    "\n",
    "\n",
    "def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):\n",
    "    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])\n",
    "\n",
    "\n",
    "def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n",
    "                vocab, lambda_step, R, n_jobs):\n",
    "    # marginal distribution over terms (width of blue bars)\n",
    "    term_proportion = term_frequency / term_frequency.sum()\n",
    "\n",
    "    # compute the distinctiveness and saliency of the terms:\n",
    "    # this determines the R terms that are displayed when no topic is selected\n",
    "    topic_given_term = topic_term_dists / topic_term_dists.sum()\n",
    "    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
    "    distinctiveness = kernel.sum()\n",
    "    saliency = term_proportion * distinctiveness\n",
    "    # Order the terms for the \"default\" view by decreasing saliency:\n",
    "    default_term_info = pd.DataFrame({\n",
    "        'saliency': saliency,\n",
    "        'Term': vocab,\n",
    "        'Freq': term_frequency,\n",
    "        'Total': term_frequency,\n",
    "        'Category': 'Default'})\n",
    "    default_term_info = default_term_info.sort_values(\n",
    "        by='saliency', ascending=False).drop('saliency', 1) #by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
    "    # Rounding Freq and Total to integer values to match LDAvis code:\n",
    "    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n",
    "    default_term_info['Total'] = np.floor(default_term_info['Total'])\n",
    "    ranks = np.arange(R, 0, -1)\n",
    "    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n",
    "\n",
    "    # compute relevance and top terms for each topic\n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n",
    "\n",
    "    def topic_top_term_df(tup):\n",
    "        new_topic_id, (original_topic_id, topic_terms) = tup\n",
    "        term_ix = topic_terms.unique()\n",
    "        return pd.DataFrame({'Term': vocab[term_ix],\n",
    "                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n",
    "                             'Total': term_frequency[term_ix],\n",
    "                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n",
    "                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n",
    "                             'Category': 'Topic%d' % new_topic_id})\n",
    "\n",
    "    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n",
    "                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n",
    "                          for ls in _job_chunks(lambda_seq, n_jobs)))\n",
    "    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n",
    "    return pd.concat([default_term_info] + list(topic_dfs), sort=True)\n",
    "\n",
    "\n",
    "def _token_table(topic_info, term_topic_freq, vocab, term_frequency):\n",
    "    # last, to compute the areas of the circles when a term is highlighted\n",
    "    # we must gather all unique terms that could show up (for every combination\n",
    "    # of topic and value of lambda) and compute its distribution over topics.\n",
    "\n",
    "    # term-topic frequency table of unique terms across all topics and all values of lambda\n",
    "    term_ix = topic_info.index.unique()\n",
    "    term_ix = np.sort(term_ix)\n",
    "\n",
    "    top_topic_terms_freq = term_topic_freq[term_ix]\n",
    "    # use the new ordering for the topics\n",
    "    K = len(term_topic_freq)\n",
    "    top_topic_terms_freq.index = range(1, K + 1)\n",
    "    top_topic_terms_freq.index.name = 'Topic'\n",
    "\n",
    "    # we filter to Freq >= 0.5 to avoid sending too much data to the browser\n",
    "    token_table = pd.DataFrame({'Freq': top_topic_terms_freq.unstack()})\\\n",
    "        .reset_index().set_index('term').query('Freq >= 0.5')\n",
    "\n",
    "    token_table['Freq'] = token_table['Freq'].round()\n",
    "    token_table['Term'] = vocab[token_table.index.values].values\n",
    "    # Normalize token frequencies:\n",
    "    token_table['Freq'] = token_table.Freq / term_frequency[token_table.index]\n",
    "    return token_table.sort_values(by=['Term', 'Topic'])\n",
    "\n",
    "#Editar esto para hacer el merge\n",
    "def prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,\n",
    "            R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1,\n",
    "            plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=False):\n",
    "    print(\"Using Prepared Data script - sort_topics\", sort_topics)\n",
    "    \"\"\"Transforms the topic model distributions and related corpus data into\n",
    "    the data structures needed for the visualization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "        Matrix of topic-term probabilities. Where `n_terms` is `len(vocab)`.\n",
    "    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape `n_docs`\n",
    "        The length of each document, i.e. the number of words in each document.\n",
    "        The order of the numbers should be consistent with the ordering of the\n",
    "        docs in `doc_topic_dists`.\n",
    "    vocab : array-like, shape `n_terms`\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape `n_terms`\n",
    "        The count of each particular term over the entire corpus. The ordering\n",
    "        of these counts should correspond with `vocab` and `topic_term_dists`.\n",
    "    R : int\n",
    "        The number of terms to display in the barcharts of the visualization.\n",
    "        Default is 30. Recommended to be roughly between 10 and 50.\n",
    "    lambda_step : float, between 0 and 1\n",
    "        Determines the interstep distance in the grid of lambda values over\n",
    "        which to iterate when computing relevance.\n",
    "        Default is 0.01. Recommended to be between 0.01 and 0.1.\n",
    "    mds : function or a string representation of function\n",
    "        A function that takes `topic_term_dists` as an input and outputs a\n",
    "        `n_topics` by `2`  distance matrix. The output approximates the distance\n",
    "        between topics. See :func:`js_PCoA` for details on the default function.\n",
    "        A string representation currently accepts `pcoa` (or upper case variant),\n",
    "        `mmds` (or upper case variant) and `tsne` (or upper case variant),\n",
    "        if `sklearn` package is installed for the latter two.\n",
    "    n_jobs : int\n",
    "        The number of cores to be used to do the computations. The regular\n",
    "        joblib conventions are followed so `-1`, which is the default, will\n",
    "        use all cores.\n",
    "    plot_opts : dict, with keys 'xlab' and `ylab`\n",
    "        Dictionary of plotting options, right now only used for the axis labels.\n",
    "    sort_topics : sort topics by topic proportion (percentage of tokens covered). Set to false to\n",
    "        to keep original topic order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "        A named tuple containing all the data structures required to create\n",
    "        the visualization. To be passed on to functions like :func:`display`.\n",
    "        This named tuple can be represented as json or a python dictionary.\n",
    "        There is a helper function 'sorted_terms' that can be used to get\n",
    "        the terms of a topic using lambda to rank their relevance.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implements the method of `Sievert, C. and Shirley, K. (2014):\n",
    "    LDAvis: A Method for Visualizing and Interpreting Topics, ACL Workshop on\n",
    "    Interactive Language Learning, Visualization, and Interfaces.`\n",
    "\n",
    "    http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`save_json`: save json representation of a figure to file\n",
    "    :func:`save_html` : save html representation of a figure to file\n",
    "    :func:`show` : launch a local server and show a figure in a browser\n",
    "    :func:`display` : embed figure within the IPython notebook\n",
    "    :func:`enable_notebook` : automatically embed visualizations in IPython notebook\n",
    "   \"\"\"\n",
    "    \n",
    "\n",
    "    # parse mds\n",
    "    if type(mds) == str:\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "    \n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "    R = len(vocab)\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)\n",
    "    if (sort_topics):\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "    \n",
    "    topic_order = topic_proportion.index\n",
    "    # reorder all data based on new ordering of topics\n",
    "    topic_freq = topic_freq[topic_order]\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "    doc_topic_dists = doc_topic_dists[topic_order]\n",
    "    \n",
    "    # token counts for each term-topic combination (widths of red bars)\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    \n",
    "    # Quick fix for red bar width bug.  We calculate the\n",
    "    # term frequencies internally, using the topic term distributions and the\n",
    "    # topic frequencies, rather than using the user-supplied term frequencies.\n",
    "    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "    \n",
    "    topic_info = _topic_info(topic_term_dists, topic_proportion,\n",
    "                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "    \n",
    "    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)\n",
    "    \n",
    "    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "    \n",
    "    client_topic_order = [x + 1 for x in topic_order]\n",
    "    \n",
    "    return PreparedData(topic_coordinates, topic_info,\n",
    "                        token_table, R, lambda_step, plot_opts, client_topic_order)\n",
    "\n",
    "\n",
    "class PreparedData(namedtuple('PreparedData', ['topic_coordinates', 'topic_info', 'token_table',\n",
    "                                               'R', 'lambda_step', 'plot_opts', 'topic_order'])):\n",
    "\n",
    "    \n",
    "    def sorted_terms(self, topic=1, _lambda=1):\n",
    "        \"\"\"Retuns a dataframe using _lambda to calculate term relevance of a given topic.\"\"\"\n",
    "        tdf = pd.DataFrame(self.topic_info[self.topic_info.Category == 'Topic' + str(topic)])\n",
    "        if _lambda < 0 or _lambda > 1:\n",
    "            _lambda = 1\n",
    "        stdf = tdf.assign(relevance=_lambda * tdf['logprob'] + (1 - _lambda) * tdf['loglift'])\n",
    "        return stdf.sort_values('relevance', ascending=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {'mdsDat': self.topic_coordinates.to_dict(orient='list'),\n",
    "                'tinfo': self.topic_info.to_dict(orient='list'),\n",
    "                'token.table': self.token_table.to_dict(orient='list'),\n",
    "                'R': self.R,\n",
    "                'lambda.step': self.lambda_step,\n",
    "                'plot.opts': self.plot_opts,\n",
    "                'topic.order': self.topic_order}\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), cls=NumPyEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#topic_info.to_dict(orient='list'),\n",
    "#token_table.to_dict(orient='list'),\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to review this function\n",
    "def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n",
    "                vocab, lambda_step, R, n_jobs):\n",
    "    # marginal distribution over terms (width of blue bars)\n",
    "    term_proportion = term_frequency / term_frequency.sum()\n",
    "\n",
    "    # compute the distinctiveness and saliency of the terms:\n",
    "    # this determines the R terms that are displayed when no topic is selected\n",
    "    topic_given_term = topic_term_dists / topic_term_dists.sum()\n",
    "    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
    "    distinctiveness = kernel.sum()\n",
    "    saliency = term_proportion * distinctiveness\n",
    "    # Order the terms for the \"default\" view by decreasing saliency:\n",
    "    default_term_info = pd.DataFrame({\n",
    "        'saliency': saliency,\n",
    "        'Term': vocab,\n",
    "        'Freq': term_frequency,\n",
    "        'Total': term_frequency,\n",
    "        'Category': 'Default'})\n",
    "    default_term_info = default_term_info.sort_values(\n",
    "        by='saliency', ascending=False).drop('saliency', 1) #by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
    "    # Rounding Freq and Total to integer values to match LDAvis code:\n",
    "    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n",
    "    default_term_info['Total'] = np.floor(default_term_info['Total'])\n",
    "    ranks = np.arange(R, 0, -1)\n",
    "    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n",
    "\n",
    "    # compute relevance and top terms for each topic\n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n",
    "\n",
    "    def topic_top_term_df(tup):\n",
    "        new_topic_id, (original_topic_id, topic_terms) = tup\n",
    "        term_ix = topic_terms.unique()\n",
    "        return pd.DataFrame({'Term': vocab[term_ix],\n",
    "                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n",
    "                             'Total': term_frequency[term_ix],\n",
    "                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n",
    "                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n",
    "                             'Category': 'Topic%d' % new_topic_id})\n",
    "\n",
    "    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n",
    "                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n",
    "                          for ls in _job_chunks(lambda_seq, n_jobs)))\n",
    "    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n",
    "    return pd.concat([default_term_info] + list(topic_dfs), sort=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
