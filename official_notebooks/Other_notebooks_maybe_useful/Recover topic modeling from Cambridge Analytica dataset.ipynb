{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(_prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original idea from: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "\n",
    "\n",
    "In this post, we discuss techniques to visualize the output and results from topic model (LDA) based on the gensim package. I will be using a portion of the 20 Newsgroups dataset since the focus is more on approaches to visualizing the results.\n",
    "\n",
    "Let’s begin by importing the packages and the 20 News Groups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='c:\\\\users\\\\gonza\\\\tesisenv\\\\lib\\\\site-packages\\\\thinc\\\\neural\\\\_custom_kernels.cu' mode='r' encoding='utf8'>\n",
      "  SRC = (PWD / \"_custom_kernels.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='c:\\\\users\\\\gonza\\\\tesisenv\\\\lib\\\\site-packages\\\\thinc\\\\neural\\\\_murmur3.cu' mode='r' encoding='utf8'>\n",
      "  MMH_SRC = (PWD / \"_murmur3.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['linkremoved',' <link removed>','usernameremoved','<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions about how to install mallet are available here: http://mallet.cs.umass.edu/download.php\n",
    "\n",
    "'''\n",
    "Windows installation: After unzipping MALLET, set the environment variable %MALLET_HOME% to point to the MALLET directory.\n",
    "In all command line examples, substitute bin\\mallet for bin/mallet.\n",
    "'''\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "\n",
    "path_to_mallet_binary = \"C:\\\\mallet-2.0.8\\\\bin\\\\mallet\"\n",
    "os.environ.update({'MALLET_HOME':r'C:\\mallet-2.0.8'}) #OJO!, por alguna razon mallet solo puede estar disponible en esa carpeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Cambridge Analytica datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this libraries are for this dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_1 = 'english_europe_tweets_20190411.csv'\n",
    "file_name_2 = 'english_northamerica_tweets_20190411.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1  = pd.read_csv('data/cambridge_analytica/regional_datasets/'+file_name_1)\n",
    "df_2  = pd.read_csv('data/cambridge_analytica/regional_datasets/'+file_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1 = df_1.sample(1000)  #remove this line\n",
    "#df_2 = df_2.sample(1000)  #remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at2</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_created_at</th>\n",
       "      <th>retweet_full_text</th>\n",
       "      <th>texto_completo</th>\n",
       "      <th>ubicacion_encontrada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480940567924736</td>\n",
       "      <td>Sun Apr 01 16:24:07 +0000 2018</td>\n",
       "      <td>CamHatch</td>\n",
       "      <td>Rotterdam, The Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meet our newest product: the transparent CamHa...</td>\n",
       "      <td>Meet our newest product: the transparent CamHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meet our newest product: the transparent CamHa...</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980481203110281216</td>\n",
       "      <td>Sun Apr 01 16:25:09 +0000 2018</td>\n",
       "      <td>iky86</td>\n",
       "      <td>stoke on trent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SupportOurLefty This week has been the most #...</td>\n",
       "      <td>@SupportOurLefty This week has been the most #...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;usernameremoved&gt; This week has been the most ...</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480454846566401</td>\n",
       "      <td>Sun Apr 01 16:22:11 +0000 2018</td>\n",
       "      <td>MichaelJF80</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pulitzer prize for this lady. Investigative jo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pulitzer prize for this lady. Investigative jo...</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480424429400071</td>\n",
       "      <td>Sun Apr 01 16:22:04 +0000 2018</td>\n",
       "      <td>ruechids1</td>\n",
       "      <td>Enfield, London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theres no more secrecy, confident and privacy ...</td>\n",
       "      <td>Theres no more secrecy, confident and privacy ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theres no more secrecy, confident and privacy ...</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480325083164673</td>\n",
       "      <td>Sun Apr 01 16:21:40 +0000 2018</td>\n",
       "      <td>Hugh_Macfarlane</td>\n",
       "      <td>Glenrothes, Fife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AggregateIQ: the obscure Canadian tech firm an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AggregateIQ: the obscure Canadian tech firm an...</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 created_at2                     id  \\\n",
       "0           2  2018-04-01  id-980480940567924736   \n",
       "1          17  2018-04-01  id-980481203110281216   \n",
       "2          43  2018-04-01  id-980480454846566401   \n",
       "3          45  2018-04-01  id-980480424429400071   \n",
       "4          51  2018-04-01  id-980480325083164673   \n",
       "\n",
       "                       created_at user.screen_name  \\\n",
       "0  Sun Apr 01 16:24:07 +0000 2018         CamHatch   \n",
       "1  Sun Apr 01 16:25:09 +0000 2018            iky86   \n",
       "2  Sun Apr 01 16:22:11 +0000 2018      MichaelJF80   \n",
       "3  Sun Apr 01 16:22:04 +0000 2018        ruechids1   \n",
       "4  Sun Apr 01 16:21:40 +0000 2018  Hugh_Macfarlane   \n",
       "\n",
       "                user_location coordinates  \\\n",
       "0  Rotterdam, The Netherlands         NaN   \n",
       "1              stoke on trent         NaN   \n",
       "2          Liverpool, England         NaN   \n",
       "3             Enfield, London         NaN   \n",
       "4            Glenrothes, Fife         NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Meet our newest product: the transparent CamHa...   \n",
       "1  @SupportOurLefty This week has been the most #...   \n",
       "2  Pulitzer prize for this lady. Investigative jo...   \n",
       "3  Theres no more secrecy, confident and privacy ...   \n",
       "4  AggregateIQ: the obscure Canadian tech firm an...   \n",
       "\n",
       "                                           full_text  retweet_created_at  \\\n",
       "0  Meet our newest product: the transparent CamHa...                 NaN   \n",
       "1  @SupportOurLefty This week has been the most #...                 NaN   \n",
       "2                                                NaN                 NaN   \n",
       "3  Theres no more secrecy, confident and privacy ...                 NaN   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "   retweet_full_text                                     texto_completo  \\\n",
       "0                NaN  Meet our newest product: the transparent CamHa...   \n",
       "1                NaN  <usernameremoved> This week has been the most ...   \n",
       "2                NaN  Pulitzer prize for this lady. Investigative jo...   \n",
       "3                NaN  Theres no more secrecy, confident and privacy ...   \n",
       "4                NaN  AggregateIQ: the obscure Canadian tech firm an...   \n",
       "\n",
       "  ubicacion_encontrada  \n",
       "0          Netherlands  \n",
       "1       United Kingdom  \n",
       "2       United Kingdom  \n",
       "3       United Kingdom  \n",
       "4       United Kingdom  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at2</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_created_at</th>\n",
       "      <th>retweet_full_text</th>\n",
       "      <th>texto_completo</th>\n",
       "      <th>ubicacion_encontrada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480936205783047</td>\n",
       "      <td>Sun Apr 01 16:24:06 +0000 2018</td>\n",
       "      <td>stanscott53</td>\n",
       "      <td>Coldspring, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SenTedCruz Hold Facebook accountable for abus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;usernameremoved&gt; Hold Facebook accountable fo...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980481012550524928</td>\n",
       "      <td>Sun Apr 01 16:24:24 +0000 2018</td>\n",
       "      <td>RShaffer1</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both sides should just drop it. Trump won...el...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both sides should just drop it. Trump won...el...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480980191404032</td>\n",
       "      <td>Sun Apr 01 16:24:16 +0000 2018</td>\n",
       "      <td>pcampisi14</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$AAPL lobbing threes while they can. Wasnt it ...</td>\n",
       "      <td>$AAPL lobbing threes while they can. Wasnt it ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$AAPL lobbing threes while they can. Wasnt it ...</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980481205492682753</td>\n",
       "      <td>Sun Apr 01 16:25:10 +0000 2018</td>\n",
       "      <td>CoinSignalBot</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...</td>\n",
       "      <td>15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980481164334063616</td>\n",
       "      <td>Sun Apr 01 16:25:00 +0000 2018</td>\n",
       "      <td>kimbee802009</td>\n",
       "      <td>South Dakota, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SenJohnThune Hold Facebook accountable for ab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;usernameremoved&gt; Hold Facebook accountable fo...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 created_at2                     id  \\\n",
       "0           1  2018-04-01  id-980480936205783047   \n",
       "1           6  2018-04-01  id-980481012550524928   \n",
       "2           9  2018-04-01  id-980480980191404032   \n",
       "3          16  2018-04-01  id-980481205492682753   \n",
       "4          20  2018-04-01  id-980481164334063616   \n",
       "\n",
       "                       created_at user.screen_name      user_location  \\\n",
       "0  Sun Apr 01 16:24:06 +0000 2018      stanscott53     Coldspring, TX   \n",
       "1  Sun Apr 01 16:24:24 +0000 2018        RShaffer1         Boston, MA   \n",
       "2  Sun Apr 01 16:24:16 +0000 2018       pcampisi14   Toronto, Ontario   \n",
       "3  Sun Apr 01 16:25:10 +0000 2018    CoinSignalBot        Seattle, WA   \n",
       "4  Sun Apr 01 16:25:00 +0000 2018     kimbee802009  South Dakota, USA   \n",
       "\n",
       "  coordinates                                               text  \\\n",
       "0         NaN  @SenTedCruz Hold Facebook accountable for abus...   \n",
       "1         NaN  Both sides should just drop it. Trump won...el...   \n",
       "2         NaN  $AAPL lobbing threes while they can. Wasnt it ...   \n",
       "3         NaN  15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...   \n",
       "4         NaN  @SenJohnThune Hold Facebook accountable for ab...   \n",
       "\n",
       "                                           full_text  retweet_created_at  \\\n",
       "0                                                NaN                 NaN   \n",
       "1                                                NaN                 NaN   \n",
       "2  $AAPL lobbing threes while they can. Wasnt it ...                 NaN   \n",
       "3  15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...                 NaN   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "   retweet_full_text                                     texto_completo  \\\n",
       "0                NaN  <usernameremoved> Hold Facebook accountable fo...   \n",
       "1                NaN  Both sides should just drop it. Trump won...el...   \n",
       "2                NaN  $AAPL lobbing threes while they can. Wasnt it ...   \n",
       "3                NaN  15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...   \n",
       "4                NaN  <usernameremoved> Hold Facebook accountable fo...   \n",
       "\n",
       "  ubicacion_encontrada  \n",
       "0        United States  \n",
       "1        United States  \n",
       "2               Canada  \n",
       "3        United States  \n",
       "4        United States  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the emails, new line characters, single quotes and finally split the sentence into a list of words using gensim’s simple_preprocess(). Setting the deacc=True option removes punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['meet', 'our', 'newest', 'product', 'the', 'transparent', 'camhatch', 'webcam', 'cover', 'weve', 'combined', 'everything', 'you', 'loved', 'about', 'the', 'matte', 'black', 'version', 'with', 'the', 'transparency', 'of', 'mark', 'zuckerberg', 'protect', 'your', 'privacy', 'without', 'having', 'to', 'change', 'the', 'appearance', 'of', 'your', 'laptop', 'tablet', 'or', 'smartphone', 'link', 'removed']]\n",
      "[['usernameremoved', 'hold', 'facebook', 'accountable', 'for', 'abusing', 'our', 'data', 'and', 'censoring', 'our', 'voice', 'vote', 'no', 'on', 'the', 'cra', 'link', 'removed'], ['both', 'sides', 'should', 'just', 'drop', 'it', 'trump', 'won', 'elections', 'over', 'move', 'on', 'link', 'removed']]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data_1 = df_1.texto_completo.values.tolist()\n",
    "data_words_1 = list(sent_to_words(data_1))\n",
    "print(data_words_1[:1])\n",
    "\n",
    "data_2 = df_2.texto_completo.values.tolist()\n",
    "data_words_2 = list(sent_to_words(data_2))\n",
    "print(data_words_2[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Bigram, Trigram Models and Lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n",
    "\n",
    "Next, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
    "\n",
    "We keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, I use spacy for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you haven't installed yet\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram_1 = gensim.models.Phrases(data_words_1, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram_1 = gensim.models.Phrases(bigram_1[data_words_1], threshold=100)  \n",
    "bigram_mod_1 = gensim.models.phrases.Phraser(bigram_1)\n",
    "trigram_mod_1 = gensim.models.phrases.Phraser(trigram_1)\n",
    "\n",
    "bigram_2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram_2 = gensim.models.Phrases(bigram_2[data_words_2], threshold=100)  \n",
    "bigram_mod_2 = gensim.models.phrases.Phraser(bigram_2)\n",
    "trigram_mod_2 = gensim.models.phrases.Phraser(trigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "\"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "def process_words(texts, bigram_mod, trigram_mod, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):    \n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []    \n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-71e7e008fc90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_ready_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_mod_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_mod_1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# processed Text Data!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_ready_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_mod_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_mod_2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# processed Text Data!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-da56a3f16a4f>\u001b[0m in \u001b[0;36mprocess_words\u001b[1;34m(texts, bigram_mod, trigram_mod, stop_words, allowed_postags)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;34m\"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-da56a3f16a4f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;34m\"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-da56a3f16a4f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;34m\"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_ready_1 = process_words(data_words_1, bigram_mod_1, trigram_mod_1)  # processed Text Data!\n",
    "data_ready_2 = process_words(data_words_2, bigram_mod_2, trigram_mod_2)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_europe = \"C:\\\\Users\\\\gonza\\\\Desktop\\\\Material tesis\\\\data\\\\cambridge_analytica\\\\regional_datasets\\\\files_europe\\\\\"\n",
    "route_northamerica = \"C:\\\\Users\\\\gonza\\\\Desktop\\\\Material tesis\\\\data\\\\cambridge_analytica\\\\regional_datasets\\\\files_northamerica\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_1 = LdaModel.load(route_europe+\"english_europe_tweets_20190411.csv_gensim.model\")\n",
    "lda_model_2 = LdaModel.load(route_northamerica+\"english_northamerica_tweets_20190411.csv_gensim.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recover matrix for topic splitting operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.055*\"people\" + 0.035*\"delete\" + 0.032*\"year\" + 0.026*\"account\" + 0.022*\"friend\" + 0.021*\"time\" + 0.017*\"back\" + 0.014*\"message\" + 0.014*\"deletefacebook\" + 0.014*\"call\"'),\n",
       " (1,\n",
       "  '0.035*\"trump\" + 0.033*\"leave\" + 0.025*\"vote\" + 0.023*\"campaign\" + 0.023*\"election\" + 0.019*\"amp\" + 0.017*\"lie\" + 0.015*\"government\" + 0.014*\"democracy\" + 0.013*\"tory\"'),\n",
       " (2,\n",
       "  '0.135*\"datum\" + 0.104*\"privacy\" + 0.074*\"user\" + 0.024*\"personal\" + 0.022*\"information\" + 0.021*\"law\" + 0.019*\"access\" + 0.018*\"app\" + 0.017*\"give\" + 0.015*\"sell\"'),\n",
       " (3,\n",
       "  '0.085*\"datum\" + 0.047*\"big\" + 0.045*\"business\" + 0.017*\"service\" + 0.016*\"security\" + 0.015*\"problem\" + 0.011*\"model\" + 0.011*\"build\" + 0.011*\"customer\" + 0.010*\"online\"'),\n",
       " (4,\n",
       "  '0.064*\"facebook\" + 0.034*\"page\" + 0.033*\"follow\" + 0.032*\"find\" + 0.030*\"twitter\" + 0.027*\"post\" + 0.026*\"share\" + 0.024*\"live\" + 0.023*\"check\" + 0.019*\"video\"'),\n",
       " (5,\n",
       "  '0.045*\"question\" + 0.034*\"today\" + 0.026*\"watch\" + 0.024*\"time\" + 0.022*\"talk\" + 0.021*\"week\" + 0.020*\"answer\" + 0.018*\"open\" + 0.017*\"hear\" + 0.016*\"day\"'),\n",
       " (6,\n",
       "  '0.131*\"bigdata\" + 0.039*\"late\" + 0.034*\"ai\" + 0.033*\"analytic\" + 0.030*\"machinelearne\" + 0.030*\"technology\" + 0.022*\"marketing\" + 0.020*\"tech\" + 0.020*\"datascience\" + 0.017*\"learn\"'),\n",
       " (7,\n",
       "  '0.049*\"facebook\" + 0.047*\"scandal\" + 0.033*\"company\" + 0.032*\"change\" + 0.029*\"make\" + 0.027*\"data\" + 0.016*\"tool\" + 0.016*\"public\" + 0.012*\"world\" + 0.012*\"profile\"'),\n",
       " (8,\n",
       "  '0.045*\"work\" + 0.034*\"ad\" + 0.029*\"story\" + 0.019*\"pay\" + 0.017*\"report\" + 0.016*\"target\" + 0.016*\"show\" + 0.014*\"run\" + 0.014*\"party\" + 0.014*\"political\"'),\n",
       " (9,\n",
       "  '0.043*\"social\" + 0.041*\"medium\" + 0.035*\"news\" + 0.031*\"thing\" + 0.023*\"people\" + 0.020*\"platform\" + 0.019*\"stop\" + 0.019*\"give\" + 0.019*\"internet\" + 0.019*\"bad\"'),\n",
       " (10,\n",
       "  '0.051*\"good\" + 0.037*\"read\" + 0.036*\"make\" + 0.026*\"great\" + 0.021*\"interesting\" + 0.016*\"real\" + 0.016*\"article\" + 0.015*\"happen\" + 0.014*\"lot\" + 0.014*\"point\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_1.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.070*\"facebook\" + 0.041*\"delete\" + 0.036*\"year\" + 0.032*\"post\" + 0.031*\"twitter\" + 0.031*\"account\" + 0.030*\"friend\" + 0.025*\"page\" + 0.023*\"follow\" + 0.023*\"check\"'),\n",
       " (1,\n",
       "  '0.031*\"time\" + 0.024*\"back\" + 0.018*\"put\" + 0.015*\"man\" + 0.014*\"show\" + 0.014*\"start\" + 0.013*\"turn\" + 0.012*\"guy\" + 0.011*\"bring\" + 0.011*\"play\"'),\n",
       " (2,\n",
       "  '0.069*\"bigdata\" + 0.030*\"business\" + 0.028*\"big\" + 0.022*\"analytic\" + 0.019*\"learn\" + 0.019*\"tech\" + 0.018*\"ai\" + 0.016*\"machinelearne\" + 0.016*\"technology\" + 0.014*\"marketing\"'),\n",
       " (3,\n",
       "  '0.035*\"amp\" + 0.021*\"leave\" + 0.020*\"love\" + 0.019*\"free\" + 0.019*\"conservative\" + 0.017*\"call\" + 0.015*\"group\" + 0.015*\"hate\" + 0.015*\"stop\" + 0.011*\"woman\"'),\n",
       " (4,\n",
       "  '0.181*\"datum\" + 0.073*\"user\" + 0.044*\"give\" + 0.039*\"company\" + 0.034*\"data\" + 0.028*\"information\" + 0.027*\"sell\" + 0.027*\"share\" + 0.025*\"personal\" + 0.019*\"access\"'),\n",
       " (5,\n",
       "  '0.082*\"make\" + 0.047*\"good\" + 0.038*\"thing\" + 0.030*\"read\" + 0.025*\"bad\" + 0.024*\"great\" + 0.024*\"time\" + 0.017*\"feel\" + 0.017*\"problem\" + 0.017*\"answer\"'),\n",
       " (6,\n",
       "  '0.106*\"privacy\" + 0.031*\"facebook\" + 0.026*\"change\" + 0.018*\"issue\" + 0.016*\"protect\" + 0.015*\"law\" + 0.015*\"security\" + 0.014*\"service\" + 0.013*\"control\" + 0.013*\"scandal\"'),\n",
       " (7,\n",
       "  '0.093*\"people\" + 0.032*\"news\" + 0.024*\"lie\" + 0.018*\"story\" + 0.017*\"fake\" + 0.015*\"care\" + 0.015*\"report\" + 0.014*\"fact\" + 0.013*\"real\" + 0.013*\"world\"'),\n",
       " (8,\n",
       "  '0.044*\"work\" + 0.038*\"ad\" + 0.029*\"election\" + 0.028*\"pay\" + 0.027*\"campaign\" + 0.022*\"money\" + 0.020*\"vote\" + 0.019*\"run\" + 0.016*\"political\" + 0.014*\"russian\"'),\n",
       " (9,\n",
       "  '0.137*\"trump\" + 0.018*\"retweete\" + 0.016*\"family\" + 0.015*\"call\" + 0.013*\"child\" + 0.012*\"break\" + 0.012*\"president\" + 0.011*\"supporter\" + 0.011*\"policy\" + 0.011*\"end\"'),\n",
       " (10,\n",
       "  '0.047*\"question\" + 0.039*\"today\" + 0.038*\"medium\" + 0.038*\"social\" + 0.037*\"watch\" + 0.027*\"live\" + 0.022*\"talk\" + 0.021*\"hear\" + 0.020*\"testimony\" + 0.020*\"hearing\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2word_1 = Dictionary.load(route_europe+\"english_europe_tweets_20190411.csv_id2word\")\n",
    "id2word_2 = Dictionary.load(route_northamerica+\"english_northamerica_tweets_20190411.csv_id2word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-64ef41e7a392>:1: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\gonza\\\\Desktop\\\\Material tesis\\\\data\\\\cambridge_analytica\\\\regional_datasets\\\\files_europe\\\\english_europe_tweets_20190411.csv_corpus.pkl'>\n",
      "  corpus_1 = pickle.load(open(route_europe+\"english_europe_tweets_20190411.csv_corpus.pkl\", 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<ipython-input-23-64ef41e7a392>:2: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\gonza\\\\Desktop\\\\Material tesis\\\\data\\\\cambridge_analytica\\\\regional_datasets\\\\files_northamerica\\\\english_northamerica_tweets_20190411.csv_corpus.pkl'>\n",
      "  corpus_2 = pickle.load(open(route_northamerica+\"english_northamerica_tweets_20190411.csv_corpus.pkl\", 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "corpus_1 = pickle.load(open(route_europe+\"english_europe_tweets_20190411.csv_corpus.pkl\", 'rb'))\n",
    "corpus_2 = pickle.load(open(route_northamerica+\"english_northamerica_tweets_20190411.csv_corpus.pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most relevant documents - Europe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should I use text or text_completo column? in the first, username and link are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSee the discusion here:\\nhttps://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\\nhttps://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "See the discusion here:\n",
    "https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\n",
    "https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_1, _ = lda_model_1.inference(corpus_1)\n",
    "matrix_documents_topic_contribution_1 /= matrix_documents_topic_contribution_1.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067683</td>\n",
       "      <td>0.075224</td>\n",
       "      <td>0.111145</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.087522</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.133339</td>\n",
       "      <td>0.066889</td>\n",
       "      <td>0.092569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095597</td>\n",
       "      <td>0.098896</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.078630</td>\n",
       "      <td>0.078384</td>\n",
       "      <td>0.125669</td>\n",
       "      <td>0.079250</td>\n",
       "      <td>0.095611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.106643</td>\n",
       "      <td>0.106413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083831</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.136948</td>\n",
       "      <td>0.097352</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>0.077989</td>\n",
       "      <td>0.089814</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.084705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.148244</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.093189</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.093545</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.081169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.067683  0.075224  0.111145  0.083138  0.087522  0.095556  0.066845   \n",
       "1  0.095597  0.098896  0.078370  0.078370  0.078370  0.112853  0.078630   \n",
       "2  0.087413  0.087413  0.087413  0.087643  0.087413  0.087413  0.087413   \n",
       "3  0.083831  0.092424  0.136948  0.097352  0.075758  0.109086  0.077989   \n",
       "4  0.081169  0.148244  0.096213  0.093189  0.081169  0.081169  0.093545   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.120091  0.133339  0.066889  0.092569  \n",
       "1  0.078384  0.125669  0.079250  0.095611  \n",
       "2  0.106643  0.106413  0.087413  0.087413  \n",
       "3  0.089814  0.075758  0.076336  0.084705  \n",
       "4  0.081761  0.081204  0.081169  0.081169  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_1 = pd.DataFrame(matrix_documents_topic_contribution_1)\n",
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_1 = pd.Series(df_1['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_1 = pd.concat([matrix_documents_topic_contribution_1, contents_1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067683</td>\n",
       "      <td>0.075224</td>\n",
       "      <td>0.111145</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.087522</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.133339</td>\n",
       "      <td>0.066889</td>\n",
       "      <td>0.092569</td>\n",
       "      <td>Meet our newest product: the transparent CamHa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095597</td>\n",
       "      <td>0.098896</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.078630</td>\n",
       "      <td>0.078384</td>\n",
       "      <td>0.125669</td>\n",
       "      <td>0.079250</td>\n",
       "      <td>0.095611</td>\n",
       "      <td>@SupportOurLefty This week has been the most #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.106643</td>\n",
       "      <td>0.106413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>Pulitzer prize for this lady. Investigative jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083831</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.136948</td>\n",
       "      <td>0.097352</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>0.077989</td>\n",
       "      <td>0.089814</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.084705</td>\n",
       "      <td>Theres no more secrecy, confident and privacy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.148244</td>\n",
       "      <td>0.096213</td>\n",
       "      <td>0.093189</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.093545</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>AggregateIQ: the obscure Canadian tech firm an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.067683  0.075224  0.111145  0.083138  0.087522  0.095556  0.066845   \n",
       "1  0.095597  0.098896  0.078370  0.078370  0.078370  0.112853  0.078630   \n",
       "2  0.087413  0.087413  0.087413  0.087643  0.087413  0.087413  0.087413   \n",
       "3  0.083831  0.092424  0.136948  0.097352  0.075758  0.109086  0.077989   \n",
       "4  0.081169  0.148244  0.096213  0.093189  0.081169  0.081169  0.093545   \n",
       "\n",
       "          7         8         9        10  \\\n",
       "0  0.120091  0.133339  0.066889  0.092569   \n",
       "1  0.078384  0.125669  0.079250  0.095611   \n",
       "2  0.106643  0.106413  0.087413  0.087413   \n",
       "3  0.089814  0.075758  0.076336  0.084705   \n",
       "4  0.081761  0.081204  0.081169  0.081169   \n",
       "\n",
       "                                                text  \n",
       "0  Meet our newest product: the transparent CamHa...  \n",
       "1  @SupportOurLefty This week has been the most #...  \n",
       "2  Pulitzer prize for this lady. Investigative jo...  \n",
       "3  Theres no more secrecy, confident and privacy ...  \n",
       "4  AggregateIQ: the obscure Canadian tech firm an...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most relevant documents - North america dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_2, _ = lda_model_2.inference(corpus_2)\n",
    "matrix_documents_topic_contribution_2 /= matrix_documents_topic_contribution_2.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.132669</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.097289</td>\n",
       "      <td>0.114503</td>\n",
       "      <td>0.079745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.098908</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.095298</td>\n",
       "      <td>0.088173</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.102745</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.082645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110938</td>\n",
       "      <td>0.093992</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.110940</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.084232</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.093991</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.093993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.142317</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085813</td>\n",
       "      <td>0.085763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.132669</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.097289</td>\n",
       "      <td>0.114503</td>\n",
       "      <td>0.079745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.079757  0.079745  0.079745  0.132669  0.097288  0.079757  0.079757   \n",
       "1  0.082645  0.100826  0.082645  0.098908  0.082645  0.095298  0.088173   \n",
       "2  0.110938  0.093992  0.077042  0.087434  0.110940  0.077284  0.084232   \n",
       "3  0.085763  0.085763  0.142317  0.085763  0.085763  0.085763  0.085763   \n",
       "4  0.079757  0.079745  0.079745  0.132669  0.097288  0.079757  0.079757   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.079745  0.097289  0.114503  0.079745  \n",
       "1  0.082645  0.102745  0.100826  0.082645  \n",
       "2  0.093113  0.093991  0.077042  0.093993  \n",
       "3  0.085763  0.085763  0.085813  0.085763  \n",
       "4  0.079745  0.097289  0.114503  0.079745  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.DataFrame(matrix_documents_topic_contribution_2)\n",
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_2 = pd.Series(df_2['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.concat([matrix_documents_topic_contribution_2, contents_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.132669</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.097289</td>\n",
       "      <td>0.114503</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>@SenTedCruz Hold Facebook accountable for abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.098908</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.095298</td>\n",
       "      <td>0.088173</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.102745</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>Both sides should just drop it. Trump won...el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110938</td>\n",
       "      <td>0.093992</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.110940</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.084232</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.093991</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.093993</td>\n",
       "      <td>$AAPL lobbing threes while they can. Wasnt it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.142317</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085813</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.132669</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.097289</td>\n",
       "      <td>0.114503</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>@SenJohnThune Hold Facebook accountable for ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.079757  0.079745  0.079745  0.132669  0.097288  0.079757  0.079757   \n",
       "1  0.082645  0.100826  0.082645  0.098908  0.082645  0.095298  0.088173   \n",
       "2  0.110938  0.093992  0.077042  0.087434  0.110940  0.077284  0.084232   \n",
       "3  0.085763  0.085763  0.142317  0.085763  0.085763  0.085763  0.085763   \n",
       "4  0.079757  0.079745  0.079745  0.132669  0.097288  0.079757  0.079757   \n",
       "\n",
       "          7         8         9        10  \\\n",
       "0  0.079745  0.097289  0.114503  0.079745   \n",
       "1  0.082645  0.102745  0.100826  0.082645   \n",
       "2  0.093113  0.093991  0.077042  0.093993   \n",
       "3  0.085763  0.085763  0.085813  0.085763   \n",
       "4  0.079745  0.097289  0.114503  0.079745   \n",
       "\n",
       "                                                text  \n",
       "0  @SenTedCruz Hold Facebook accountable for abus...  \n",
       "1  Both sides should just drop it. Trump won...el...  \n",
       "2  $AAPL lobbing threes while they can. Wasnt it ...  \n",
       "3  15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...  \n",
       "4  @SenJohnThune Hold Facebook accountable for ab...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = 'data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n",
      "Calculating for omega =  0.0\n",
      "Calculating for omega =  0.01\n",
      "Calculating for omega =  0.02\n",
      "Calculating for omega =  0.03\n",
      "Calculating for omega =  0.04\n",
      "Calculating for omega =  0.05\n",
      "Calculating for omega =  0.06\n",
      "Calculating for omega =  0.07\n",
      "Calculating for omega =  0.08\n",
      "Calculating for omega =  0.09\n",
      "Calculating for omega =  0.1\n",
      "Calculating for omega =  0.11\n",
      "Calculating for omega =  0.12\n",
      "Calculating for omega =  0.13\n",
      "Calculating for omega =  0.14\n",
      "Calculating for omega =  0.15\n",
      "Calculating for omega =  0.16\n",
      "Calculating for omega =  0.17\n",
      "Calculating for omega =  0.18\n",
      "Calculating for omega =  0.19\n",
      "Calculating for omega =  0.2\n",
      "Calculating for omega =  0.21\n",
      "Calculating for omega =  0.22\n",
      "Calculating for omega =  0.23\n",
      "Calculating for omega =  0.24\n",
      "Calculating for omega =  0.25\n",
      "Calculating for omega =  0.26\n",
      "Calculating for omega =  0.27\n",
      "Calculating for omega =  0.28\n",
      "Calculating for omega =  0.29\n",
      "Calculating for omega =  0.3\n",
      "Calculating for omega =  0.31\n",
      "Calculating for omega =  0.32\n",
      "Calculating for omega =  0.33\n",
      "Calculating for omega =  0.34\n",
      "Calculating for omega =  0.35\n",
      "Calculating for omega =  0.36\n",
      "Calculating for omega =  0.37\n",
      "Calculating for omega =  0.38\n",
      "Calculating for omega =  0.39\n",
      "Calculating for omega =  0.4\n",
      "Calculating for omega =  0.41\n",
      "Calculating for omega =  0.42\n",
      "Calculating for omega =  0.43\n",
      "Calculating for omega =  0.44\n",
      "Calculating for omega =  0.45\n",
      "Calculating for omega =  0.46\n",
      "Calculating for omega =  0.47\n",
      "Calculating for omega =  0.48\n",
      "Calculating for omega =  0.49\n",
      "Calculating for omega =  0.5\n",
      "Calculating for omega =  0.51\n",
      "Calculating for omega =  0.52\n",
      "Calculating for omega =  0.53\n",
      "Calculating for omega =  0.54\n",
      "Calculating for omega =  0.55\n",
      "Calculating for omega =  0.56\n",
      "Calculating for omega =  0.57\n",
      "Calculating for omega =  0.58\n",
      "Calculating for omega =  0.59\n",
      "Calculating for omega =  0.6\n",
      "Calculating for omega =  0.61\n",
      "Calculating for omega =  0.62\n",
      "Calculating for omega =  0.63\n",
      "Calculating for omega =  0.64\n",
      "Calculating for omega =  0.65\n",
      "Calculating for omega =  0.66\n",
      "Calculating for omega =  0.67\n",
      "Calculating for omega =  0.68\n",
      "Calculating for omega =  0.69\n",
      "Calculating for omega =  0.7\n",
      "Calculating for omega =  0.71\n",
      "Calculating for omega =  0.72\n",
      "Calculating for omega =  0.73\n",
      "Calculating for omega =  0.74\n",
      "Calculating for omega =  0.75\n",
      "Calculating for omega =  0.76\n",
      "Calculating for omega =  0.77\n",
      "Calculating for omega =  0.78\n",
      "Calculating for omega =  0.79\n",
      "Calculating for omega =  0.8\n",
      "Calculating for omega =  0.81\n",
      "Calculating for omega =  0.82\n",
      "Calculating for omega =  0.83\n",
      "Calculating for omega =  0.84\n",
      "Calculating for omega =  0.85\n",
      "Calculating for omega =  0.86\n",
      "Calculating for omega =  0.87\n",
      "Calculating for omega =  0.88\n",
      "Calculating for omega =  0.89\n",
      "Calculating for omega =  0.9\n",
      "Calculating for omega =  0.91\n",
      "Calculating for omega =  0.92\n",
      "Calculating for omega =  0.93\n",
      "Calculating for omega =  0.94\n",
      "Calculating for omega =  0.95\n",
      "Calculating for omega =  0.96\n",
      "Calculating for omega =  0.97\n",
      "Calculating for omega =  0.98\n",
      "Calculating for omega =  0.99\n",
      "Calculating for omega =  1.0\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora = vis.calculate_topic_similarity_on_multi_corpora(word_embedding_model, lda_model_1,lda_model_2, corpus_1,corpus_2, id2word_1,id2word_2, matrix_documents_topic_contribution_1,matrix_documents_topic_contribution_2, topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show visualization - Multicorpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n"
     ]
    }
   ],
   "source": [
    "#import topicvisexplorer\n",
    "#import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_multi_corpora( lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2,  matrix_documents_topic_contribution_1, matrix_documents_topic_contribution_2, topic_similarity_matrix_multicorpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi corpora data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "#save data\n",
    "vis.save_multi_corpora_data(\"multi_corpora_data_europe_northamerica_ca_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.load_multi_corpora_data(\"multi_corpora_data_europe_northamerica_ca_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"borrar_nombre\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-04 14:34:59,271 : INFO :  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que le pase a jinja  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-04 14:35:22,858 : ERROR : Exception on /multicorpora [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask_classful.py\", line 268, in proxy\n",
      "    response = view(**request.view_args)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask_classful.py\", line 239, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\gonza\\Desktop\\TopicVisExplorer\\topicvisexplorer.py\", line 277, in multi_corpora\n",
      "    return render_template_string(html)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\flask\\templating.py\", line 155, in render_template_string\n",
      "    return _render(ctx.app.jinja_env.from_string(source), context, ctx.app)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\jinja2\\environment.py\", line 941, in from_string\n",
      "    return cls.from_code(self, self.compile(source), globals, None)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\jinja2\\environment.py\", line 638, in compile\n",
      "    self.handle_exception(source=source_hint)\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\jinja2\\environment.py\", line 832, in handle_exception\n",
      "    reraise(*rewrite_traceback_stack(source=source))\n",
      "  File \"c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\jinja2\\_compat.py\", line 28, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"<unknown>\", line 254, in template\n",
      "jinja2.exceptions.TemplateSyntaxError: expected token 'end of print statement', got 'How'\n",
      "2021-01-04 14:35:22,860 : INFO : 127.0.0.1 - - [04/Jan/2021 14:35:22] \"\u001b[35m\u001b[1mGET /multicorpora HTTP/1.1\u001b[0m\" 500 -\n",
      "2021-01-04 14:35:23,094 : INFO : 127.0.0.1 - - [04/Jan/2021 14:35:23] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "vis.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show visualization - Single corpora - Europe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate topic similarity metric for one single corpus\n",
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "import gensim\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = 'data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Calculating for omega =  0.0\n",
      "Calculating for omega =  0.01\n",
      "Calculating for omega =  0.02\n",
      "Calculating for omega =  0.03\n",
      "Calculating for omega =  0.04\n",
      "Calculating for omega =  0.05\n",
      "Calculating for omega =  0.06\n",
      "Calculating for omega =  0.07\n",
      "Calculating for omega =  0.08\n",
      "Calculating for omega =  0.09\n",
      "Calculating for omega =  0.1\n",
      "Calculating for omega =  0.11\n",
      "Calculating for omega =  0.12\n",
      "Calculating for omega =  0.13\n",
      "Calculating for omega =  0.14\n",
      "Calculating for omega =  0.15\n",
      "Calculating for omega =  0.16\n",
      "Calculating for omega =  0.17\n",
      "Calculating for omega =  0.18\n",
      "Calculating for omega =  0.19\n",
      "Calculating for omega =  0.2\n",
      "Calculating for omega =  0.21\n",
      "Calculating for omega =  0.22\n",
      "Calculating for omega =  0.23\n",
      "Calculating for omega =  0.24\n",
      "Calculating for omega =  0.25\n",
      "Calculating for omega =  0.26\n",
      "Calculating for omega =  0.27\n",
      "Calculating for omega =  0.28\n",
      "Calculating for omega =  0.29\n",
      "Calculating for omega =  0.3\n",
      "Calculating for omega =  0.31\n",
      "Calculating for omega =  0.32\n",
      "Calculating for omega =  0.33\n",
      "Calculating for omega =  0.34\n",
      "Calculating for omega =  0.35\n",
      "Calculating for omega =  0.36\n",
      "Calculating for omega =  0.37\n",
      "Calculating for omega =  0.38\n",
      "Calculating for omega =  0.39\n",
      "Calculating for omega =  0.4\n",
      "Calculating for omega =  0.41\n",
      "Calculating for omega =  0.42\n",
      "Calculating for omega =  0.43\n",
      "Calculating for omega =  0.44\n",
      "Calculating for omega =  0.45\n",
      "Calculating for omega =  0.46\n",
      "Calculating for omega =  0.47\n",
      "Calculating for omega =  0.48\n",
      "Calculating for omega =  0.49\n",
      "Calculating for omega =  0.5\n",
      "Calculating for omega =  0.51\n",
      "Calculating for omega =  0.52\n",
      "Calculating for omega =  0.53\n",
      "Calculating for omega =  0.54\n",
      "Calculating for omega =  0.55\n",
      "Calculating for omega =  0.56\n",
      "Calculating for omega =  0.57\n",
      "Calculating for omega =  0.58\n",
      "Calculating for omega =  0.59\n",
      "Calculating for omega =  0.6\n",
      "Calculating for omega =  0.61\n",
      "Calculating for omega =  0.62\n",
      "Calculating for omega =  0.63\n",
      "Calculating for omega =  0.64\n",
      "Calculating for omega =  0.65\n",
      "Calculating for omega =  0.66\n",
      "Calculating for omega =  0.67\n",
      "Calculating for omega =  0.68\n",
      "Calculating for omega =  0.69\n",
      "Calculating for omega =  0.7\n",
      "Calculating for omega =  0.71\n",
      "Calculating for omega =  0.72\n",
      "Calculating for omega =  0.73\n",
      "Calculating for omega =  0.74\n",
      "Calculating for omega =  0.75\n",
      "Calculating for omega =  0.76\n",
      "Calculating for omega =  0.77\n",
      "Calculating for omega =  0.78\n",
      "Calculating for omega =  0.79\n",
      "Calculating for omega =  0.8\n",
      "Calculating for omega =  0.81\n",
      "Calculating for omega =  0.82\n",
      "Calculating for omega =  0.83\n",
      "Calculating for omega =  0.84\n",
      "Calculating for omega =  0.85\n",
      "Calculating for omega =  0.86\n",
      "Calculating for omega =  0.87\n",
      "Calculating for omega =  0.88\n",
      "Calculating for omega =  0.89\n",
      "Calculating for omega =  0.9\n",
      "Calculating for omega =  0.91\n",
      "Calculating for omega =  0.92\n",
      "Calculating for omega =  0.93\n",
      "Calculating for omega =  0.94\n",
      "Calculating for omega =  0.95\n",
      "Calculating for omega =  0.96\n",
      "Calculating for omega =  0.97\n",
      "Calculating for omega =  0.98\n",
      "Calculating for omega =  0.99\n",
      "Calculating for omega =  1.0\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "\n",
    "topic_similarity_matrix_1 = vis.calculate_topic_similarity_on_single_corpus(word_embedding_model, lda_model_1, corpus_1, id2word_1, matrix_documents_topic_contribution_1,topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.prepare_single_corpus( lda_model_1, corpus_1, id2word_1, matrix_documents_topic_contribution_1, topic_similarity_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single corpus data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "vis.save_single_corpus_data(\"single_corpus_europe_cambridge_analytica_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show visualization - Single corpora - Northamerica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate topic similarity metric for one single corpus\n",
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "import gensim\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = 'data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Calculating for omega =  0.0\n",
      "Calculating for omega =  0.01\n",
      "Calculating for omega =  0.02\n",
      "Calculating for omega =  0.03\n",
      "Calculating for omega =  0.04\n",
      "Calculating for omega =  0.05\n",
      "Calculating for omega =  0.06\n",
      "Calculating for omega =  0.07\n",
      "Calculating for omega =  0.08\n",
      "Calculating for omega =  0.09\n",
      "Calculating for omega =  0.1\n",
      "Calculating for omega =  0.11\n",
      "Calculating for omega =  0.12\n",
      "Calculating for omega =  0.13\n",
      "Calculating for omega =  0.14\n",
      "Calculating for omega =  0.15\n",
      "Calculating for omega =  0.16\n",
      "Calculating for omega =  0.17\n",
      "Calculating for omega =  0.18\n",
      "Calculating for omega =  0.19\n",
      "Calculating for omega =  0.2\n",
      "Calculating for omega =  0.21\n",
      "Calculating for omega =  0.22\n",
      "Calculating for omega =  0.23\n",
      "Calculating for omega =  0.24\n",
      "Calculating for omega =  0.25\n",
      "Calculating for omega =  0.26\n",
      "Calculating for omega =  0.27\n",
      "Calculating for omega =  0.28\n",
      "Calculating for omega =  0.29\n",
      "Calculating for omega =  0.3\n",
      "Calculating for omega =  0.31\n",
      "Calculating for omega =  0.32\n",
      "Calculating for omega =  0.33\n",
      "Calculating for omega =  0.34\n",
      "Calculating for omega =  0.35\n",
      "Calculating for omega =  0.36\n",
      "Calculating for omega =  0.37\n",
      "Calculating for omega =  0.38\n",
      "Calculating for omega =  0.39\n",
      "Calculating for omega =  0.4\n",
      "Calculating for omega =  0.41\n",
      "Calculating for omega =  0.42\n",
      "Calculating for omega =  0.43\n",
      "Calculating for omega =  0.44\n",
      "Calculating for omega =  0.45\n",
      "Calculating for omega =  0.46\n",
      "Calculating for omega =  0.47\n",
      "Calculating for omega =  0.48\n",
      "Calculating for omega =  0.49\n",
      "Calculating for omega =  0.5\n",
      "Calculating for omega =  0.51\n",
      "Calculating for omega =  0.52\n",
      "Calculating for omega =  0.53\n",
      "Calculating for omega =  0.54\n",
      "Calculating for omega =  0.55\n",
      "Calculating for omega =  0.56\n",
      "Calculating for omega =  0.57\n",
      "Calculating for omega =  0.58\n",
      "Calculating for omega =  0.59\n",
      "Calculating for omega =  0.6\n",
      "Calculating for omega =  0.61\n",
      "Calculating for omega =  0.62\n",
      "Calculating for omega =  0.63\n",
      "Calculating for omega =  0.64\n",
      "Calculating for omega =  0.65\n",
      "Calculating for omega =  0.66\n",
      "Calculating for omega =  0.67\n",
      "Calculating for omega =  0.68\n",
      "Calculating for omega =  0.69\n",
      "Calculating for omega =  0.7\n",
      "Calculating for omega =  0.71\n",
      "Calculating for omega =  0.72\n",
      "Calculating for omega =  0.73\n",
      "Calculating for omega =  0.74\n",
      "Calculating for omega =  0.75\n",
      "Calculating for omega =  0.76\n",
      "Calculating for omega =  0.77\n",
      "Calculating for omega =  0.78\n",
      "Calculating for omega =  0.79\n",
      "Calculating for omega =  0.8\n",
      "Calculating for omega =  0.81\n",
      "Calculating for omega =  0.82\n",
      "Calculating for omega =  0.83\n",
      "Calculating for omega =  0.84\n",
      "Calculating for omega =  0.85\n",
      "Calculating for omega =  0.86\n",
      "Calculating for omega =  0.87\n",
      "Calculating for omega =  0.88\n",
      "Calculating for omega =  0.89\n",
      "Calculating for omega =  0.9\n",
      "Calculating for omega =  0.91\n",
      "Calculating for omega =  0.92\n",
      "Calculating for omega =  0.93\n",
      "Calculating for omega =  0.94\n",
      "Calculating for omega =  0.95\n",
      "Calculating for omega =  0.96\n",
      "Calculating for omega =  0.97\n",
      "Calculating for omega =  0.98\n",
      "Calculating for omega =  0.99\n",
      "Calculating for omega =  1.0\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "\n",
    "topic_similarity_matrix_2 = vis.calculate_topic_similarity_on_single_corpus(word_embedding_model, lda_model_2, corpus_2, id2word_2, matrix_documents_topic_contribution_2,topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.prepare_single_corpus( lda_model_2, corpus_2, id2word_2, matrix_documents_topic_contribution_2, topic_similarity_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single corpus data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "vis.save_single_corpus_data(\"single_corpus_northamerica_cambridge_analytica_lda_mallet_gensim_new_prepared_data_enero_11.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
