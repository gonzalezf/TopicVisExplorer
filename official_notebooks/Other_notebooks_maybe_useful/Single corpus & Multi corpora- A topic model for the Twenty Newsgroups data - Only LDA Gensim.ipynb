{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original idea from: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "\n",
    "\n",
    "In this post, we discuss techniques to visualize the output and results from topic model (LDA) based on the gensim package. I will be using a portion of the 20 Newsgroups dataset since the focus is more on approaches to visualizing the results.\n",
    "\n",
    "Let’s begin by importing the packages and the 20 News Groups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import NewsGroups Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', -1) This line is optional to see the full width of the column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the emails, new line characters, single quotes and finally split the sentence into a list of words using gensim’s simple_preprocess(). Setting the deacc=True option removes punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "# [['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution', 'usa', 'expires', 'sat', 'may', 'gmt', ...trucated...]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Bigram, Trigram Models and Lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n",
    "\n",
    "Next, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
    "\n",
    "We keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, I use spacy for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []    \n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the LDA topic model using LdaModel(), you need the corpus and the dictionary. Let’s create them first and then build the model. The trained topics (keywords and weights) are printed below as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.122*\"information\" + 0.109*\"far\" + 0.106*\"person\" + 0.100*\"address\" + '\n",
      "  '0.087*\"require\" + 0.079*\"sense\" + 0.064*\"pretty\" + 0.053*\"phone\" + '\n",
      "  '0.046*\"stuff\" + 0.031*\"division\"'),\n",
      " (1,\n",
      "  '0.274*\"experience\" + 0.041*\"brave\" + 0.000*\"evidence\" + 0.000*\"reason\" + '\n",
      "  '0.000*\"faith\" + 0.000*\"explain\" + 0.000*\"claim\" + 0.000*\"physical\" + '\n",
      "  '0.000*\"valid\" + 0.000*\"never\"'),\n",
      " (2,\n",
      "  '0.141*\"work\" + 0.116*\"problem\" + 0.075*\"sure\" + 0.062*\"however\" + '\n",
      "  '0.059*\"file\" + 0.056*\"buy\" + 0.043*\"wrong\" + 0.043*\"technology\" + '\n",
      "  '0.038*\"lose\" + 0.033*\"correct\"'),\n",
      " (3,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (4,\n",
      "  '0.187*\"thing\" + 0.124*\"call\" + 0.118*\"car\" + 0.075*\"name\" + 0.052*\"model\" + '\n",
      "  '0.051*\"small\" + 0.040*\"bring\" + 0.039*\"history\" + 0.034*\"body\" + '\n",
      "  '0.032*\"early\"'),\n",
      " (5,\n",
      "  '0.084*\"drive\" + 0.069*\"system\" + 0.050*\"tell\" + 0.038*\"post\" + 0.036*\"fact\" '\n",
      "  '+ 0.035*\"still\" + 0.034*\"part\" + 0.031*\"set\" + 0.030*\"true\" + 0.029*\"long\"'),\n",
      " (6,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (7,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (8,\n",
      "  '0.212*\"last\" + 0.183*\"man\" + 0.070*\"instead\" + 0.053*\"hit\" + '\n",
      "  '0.052*\"directly\" + 0.042*\"delete\" + 0.039*\"respond\" + 0.038*\"treatment\" + '\n",
      "  '0.033*\"knowledge\" + 0.011*\"fair\"'),\n",
      " (9,\n",
      "  '0.078*\"write\" + 0.045*\"people\" + 0.044*\"article\" + 0.031*\"time\" + '\n",
      "  '0.027*\"well\" + 0.027*\"organization\" + 0.026*\"year\" + 0.023*\"find\" + '\n",
      "  '0.023*\"way\" + 0.022*\"give\"'),\n",
      " (10,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (11,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (12,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (13,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (14,\n",
      "  '0.106*\"new\" + 0.070*\"bit\" + 0.057*\"computer\" + 0.051*\"price\" + '\n",
      "  '0.045*\"machine\" + 0.038*\"email\" + 0.035*\"disk\" + 0.031*\"display\" + '\n",
      "  '0.031*\"look\" + 0.030*\"advance\"'),\n",
      " (15,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (16,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"'),\n",
      " (17,\n",
      "  '0.219*\"help\" + 0.189*\"mail\" + 0.134*\"change\" + 0.102*\"win\" + '\n",
      "  '0.065*\"several\" + 0.053*\"appreciate\" + 0.000*\"team\" + 0.000*\"program\" + '\n",
      "  '0.000*\"window\" + 0.000*\"reply\"'),\n",
      " (18,\n",
      "  '0.118*\"report\" + 0.107*\"base\" + 0.103*\"send\" + 0.091*\"card\" + '\n",
      "  '0.091*\"message\" + 0.050*\"add\" + 0.042*\"request\" + 0.034*\"top\" + '\n",
      "  '0.030*\"procedure\" + 0.030*\"especially\"'),\n",
      " (19,\n",
      "  '0.000*\"wollt\" + 0.000*\"overjoyed\" + 0.000*\"palaestinens\" + 0.000*\"quaelt\" + '\n",
      "  '0.000*\"schneller\" + 0.000*\"sein\" + 0.000*\"vaeter\" + 0.000*\"wehrmacht\" + '\n",
      "  '0.000*\"ihr\" + 0.000*\"bradly\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=number_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get most relevant documents - LDA Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSee the discusion here:\\nhttps://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\\nhttps://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "See the discusion here:\n",
    "https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\n",
    "https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution, _ = lda_model.inference(corpus)\n",
    "matrix_documents_topic_contribution /= matrix_documents_topic_contribution.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution = pd.DataFrame(matrix_documents_topic_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.720319</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.120832</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.101036</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.048802</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.120212</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.096427</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.572654</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.505692</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.473881</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567398</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.045652</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.667864</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.235910</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.052178</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.001563  0.001563  0.001563  0.001563  0.720319  0.001563  0.001563   \n",
       "1  0.001191  0.072623  0.001191  0.001191  0.025000  0.048802  0.001191   \n",
       "2  0.000538  0.000538  0.000538  0.000538  0.000538  0.000538  0.000538   \n",
       "3  0.567398  0.002174  0.002174  0.002174  0.045652  0.045646  0.002174   \n",
       "4  0.001191  0.001191  0.001191  0.001191  0.025000  0.667864  0.001191   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.001563  0.001563  0.120832  0.001563  0.001563  0.001563  0.001563   \n",
       "1  0.001191  0.048804  0.120212  0.001191  0.001191  0.001191  0.001191   \n",
       "2  0.000538  0.011287  0.505692  0.000538  0.000538  0.000538  0.000538   \n",
       "3  0.002174  0.002174  0.306522  0.002174  0.002174  0.002174  0.002174   \n",
       "4  0.001191  0.001191  0.235910  0.001191  0.001191  0.001191  0.001191   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.101036  0.001563  0.001563  0.032812  0.001563  0.001563  \n",
       "1  0.096427  0.001191  0.001191  0.001191  0.572654  0.001191  \n",
       "2  0.473881  0.000538  0.000538  0.000538  0.000538  0.000538  \n",
       "3  0.002174  0.002174  0.002174  0.002174  0.002174  0.002174  \n",
       "4  0.052178  0.001191  0.001191  0.001191  0.001191  0.001191  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add document's text in last column\n",
    "contents = pd.Series(df['content']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution = pd.concat([matrix_documents_topic_contribution, contents], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.720319</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.120832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.101036</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.048802</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.120212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.096427</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.572654</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.505692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.473881</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567398</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.045652</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.667864</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.235910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.052178</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.001563  0.001563  0.001563  0.001563  0.720319  0.001563  0.001563   \n",
       "1  0.001191  0.072623  0.001191  0.001191  0.025000  0.048802  0.001191   \n",
       "2  0.000538  0.000538  0.000538  0.000538  0.000538  0.000538  0.000538   \n",
       "3  0.567398  0.002174  0.002174  0.002174  0.045652  0.045646  0.002174   \n",
       "4  0.001191  0.001191  0.001191  0.001191  0.025000  0.667864  0.001191   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.001563  0.001563  0.120832  ...  0.001563  0.001563  0.001563  0.101036   \n",
       "1  0.001191  0.048804  0.120212  ...  0.001191  0.001191  0.001191  0.096427   \n",
       "2  0.000538  0.011287  0.505692  ...  0.000538  0.000538  0.000538  0.473881   \n",
       "3  0.002174  0.002174  0.306522  ...  0.002174  0.002174  0.002174  0.002174   \n",
       "4  0.001191  0.001191  0.235910  ...  0.001191  0.001191  0.001191  0.052178   \n",
       "\n",
       "         15        16        17        18        19  \\\n",
       "0  0.001563  0.001563  0.032812  0.001563  0.001563   \n",
       "1  0.001191  0.001191  0.001191  0.572654  0.001191   \n",
       "2  0.000538  0.000538  0.000538  0.000538  0.000538   \n",
       "3  0.002174  0.002174  0.002174  0.002174  0.002174   \n",
       "4  0.001191  0.001191  0.001191  0.001191  0.001191   \n",
       "\n",
       "                                             content  \n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic similarity metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "\n",
    "ruta_word_embedding = 'data/wiki.multi.en.vec'\n",
    "word_embedding_model = KeyedVectors.load_word2vec_format(ruta_word_embedding)\n",
    "\n",
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\topicvisexplorerenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for omega =  0.0\n",
      "Calculating for omega =  0.01\n",
      "Calculating for omega =  0.02\n",
      "Calculating for omega =  0.03\n",
      "Calculating for omega =  0.04\n",
      "Calculating for omega =  0.05\n",
      "Calculating for omega =  0.06\n",
      "Calculating for omega =  0.07\n",
      "Calculating for omega =  0.08\n",
      "Calculating for omega =  0.09\n",
      "Calculating for omega =  0.1\n",
      "Calculating for omega =  0.11\n",
      "Calculating for omega =  0.12\n",
      "Calculating for omega =  0.13\n",
      "Calculating for omega =  0.14\n",
      "Calculating for omega =  0.15\n",
      "Calculating for omega =  0.16\n",
      "Calculating for omega =  0.17\n",
      "Calculating for omega =  0.18\n",
      "Calculating for omega =  0.19\n",
      "Calculating for omega =  0.2\n",
      "Calculating for omega =  0.21\n",
      "Calculating for omega =  0.22\n",
      "Calculating for omega =  0.23\n",
      "Calculating for omega =  0.24\n",
      "Calculating for omega =  0.25\n",
      "Calculating for omega =  0.26\n",
      "Calculating for omega =  0.27\n",
      "Calculating for omega =  0.28\n",
      "Calculating for omega =  0.29\n",
      "Calculating for omega =  0.3\n",
      "Calculating for omega =  0.31\n",
      "Calculating for omega =  0.32\n",
      "Calculating for omega =  0.33\n",
      "Calculating for omega =  0.34\n",
      "Calculating for omega =  0.35\n",
      "Calculating for omega =  0.36\n",
      "Calculating for omega =  0.37\n",
      "Calculating for omega =  0.38\n",
      "Calculating for omega =  0.39\n",
      "Calculating for omega =  0.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f95996b99e93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopicvisexplorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopicvisexplorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTopicVisExplorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"borrar_nombre\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtopic_similarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_topic_similarity_on_single_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevance_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\topicvisexplorer.py\u001b[0m in \u001b[0;36mcalculate_topic_similarity_on_single_corpus\u001b[1;34m(self, word_embedding_model, lda_model, corpus, id2word, matrix_documents_topic_contribution, topn_terms, topk_documents, relevance_lambda)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0msingle_corpus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'relevantDocumentsDict'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelevantDocumentsDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_dict_topic_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_corpus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PreparedDataObtained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_corpus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PreparedDataObtained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevance_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\_topic_similarity_matrix.py\u001b[0m in \u001b[0;36mget_dict_topic_similarity_matrix\u001b[1;34m(wordembedding, lda_model_1, relevantDocumentsDict_1, lda_model_2, relevantDocumentsDict_2, topn_terms, PreparedData_dict_with_more_info_1, PreparedData_dict_with_more_info_2, topkdocuments, relevance_lambda)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calculating for omega = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matrix_by_lambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevantDocumentsDict_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevantDocumentsDict_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtinfo_collection_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtinfo_collection_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopkdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevance_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;31m#print(\"matriiix\", matrix)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mmatrices_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\_topic_similarity_matrix.py\u001b[0m in \u001b[0;36mget_matrix_by_lambda\u001b[1;34m(wordembedding, lda_model_1, most_relevant_documents_1, lda_model_2, most_relevant_documents_2, n_terms, lambda_, PreparedData_dict_with_more_info_1, PreparedData_dict_with_more_info_2, topkdocuments, relevance_lambda)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m#final topic vector = (lambda)topic_keyword_vector + (lambda-1)topic_document_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mfinal_topic_vectors_dict_1\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_topic_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmost_relevant_documents_1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mPreparedData_dict_with_more_info_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopkdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mfinal_topic_vectors_dict_2\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_topic_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmost_relevant_documents_2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mPreparedData_dict_with_more_info_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopkdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mtopic_similarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\_topic_similarity_matrix.py\u001b[0m in \u001b[0;36mget_topic_vectors\u001b[1;34m(wordembedding, lda_model, most_relevant_documents, n_terms, lambda_, PreparedData_dict_with_more_info, topkdocuments)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_topic_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmost_relevant_documents\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mPreparedData_dict_with_more_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopkdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mtopkeywords_vectors_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevantdocuments_vectors_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_topkeywords_relevantdocuments_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmost_relevant_documents\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreparedData_dict_with_more_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopkdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[0mfinal_topic_vectors_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;31m#print(\"dic...t\",relevantdocuments_vectors_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\_topic_similarity_matrix.py\u001b[0m in \u001b[0;36mget_topkeywords_relevantdocuments_vectors\u001b[1;34m(wordembedding, lda_model, matrix_documents_topic_contribution, n_terms, PreparedData_dict_with_more_info, topkdocuments)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopkdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mj\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mrelevantDocumentsvector\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgetDocumentVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatrix_documents_topic_contribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordembedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreparedData_dict_with_more_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[1;31m#print(\"document contribution\", item[topic_id])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mrelevantdocuments_vectors_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelevantDocumentsvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\TopicVisExplorer\\_topic_similarity_matrix.py\u001b[0m in \u001b[0;36mgetDocumentVector\u001b[1;34m(text, wordembedding, topic_id, PreparedData_dict_with_more_info)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_terms_relevance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mraking_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_terms_relevance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordembedding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if word in wordembedding.wv:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;31m#print(\"WORD FOUND\", word, raking_word)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "importlib.reload(topicvisexplorer)\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix = vis.calculate_topic_similarity_on_single_corpus(word_embedding_model, lda_model, corpus, id2word, matrix_documents_topic_contribution,topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "\n",
    "ruta_word_embedding = 'data/wiki.multi.en.vec'\n",
    "word_embedding_model = KeyedVectors.load_word2vec_format(ruta_word_embedding)\n",
    "\n",
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora = vis.calculate_topic_similarity_on_multi_corpora(word_embedding_model, lda_model,lda_model, corpus,corpus, id2word,id2word, matrix_documents_topic_contribution,matrix_documents_topic_contribution, topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show visualization - Single corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_single_corpus( lda_model, corpus, id2word, matrix_documents_topic_contribution, topic_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "vis.save_single_corpus_data(\"single_corpus_data_newsgroup_lda_gensim_20_topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"borrar_nombre\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-12-31 18:24:59,005 : INFO :  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "2020-12-31 18:25:05,207 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:05] \"\u001b[37mGET /singlecorpus HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que le pase a jinja  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 18:25:07,267 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /singlecorpus HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,296 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/jquery.min.js HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que le pase a jinja  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 18:25:07,601 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/css/bootstrap-table.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,608 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/css/LDAvis.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,614 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,618 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/css/nouislider.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,620 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/popper.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,659 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,918 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/d3.v5.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,923 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/sankey.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,930 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/nouislider.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:07,935 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:07] \"\u001b[37mGET /static/js/bootstrap-table.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:08,376 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:08] \"\u001b[37mGET /static/js/LDAvis.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:25:08,626 : INFO : 127.0.0.1 - - [31/Dec/2020 18:25:08] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "vis.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show visualization - Multi corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topicvisexplorer\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_multi_corpora( lda_model,lda_model, corpus, corpus, id2word,id2word,  matrix_documents_topic_contribution, matrix_documents_topic_contribution, topic_similarity_matrix_multicorpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.save_multi_corpora_data(\"multi_corpora_data_newsgroup_lda_gensim_20_topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"borrar_nombre\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-12-31 18:18:01,664 : INFO :  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "2020-12-31 18:18:04,882 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:04] \"\u001b[37mGET /multicorpora HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:04,921 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:04] \"\u001b[37mGET /static/js/jquery.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,005 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/css/bootstrap-table.min.css HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que le pase a jinja  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 18:18:05,224 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,227 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/css/LDAvis.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,227 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/css/nouislider.css HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,237 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/popper.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,312 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,323 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/d3.v5.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,538 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/sankey.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,542 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/nouislider.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:05,547 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:05] \"\u001b[37mGET /static/js/bootstrap-table.min.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:06,067 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:06] \"\u001b[37mGET /static/js/LDAvis.js HTTP/1.1\u001b[0m\" 200 -\n",
      "2020-12-31 18:18:06,321 : INFO : 127.0.0.1 - - [31/Dec/2020 18:18:06] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "vis.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
