{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionar idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioma = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2021-04-03\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; \n",
    "#nltk.download('stopwords')\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from string import digits\n",
    "from string import punctuation\n",
    "import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(idioma)\n",
    "stop_words.extend(['linkremoved','amp', 'usernameremoved','link','removed', '<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "##stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Airline tweets/airlines_tweets_with_sentiment_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10560"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text_x</th>\n",
       "      <th>truncated</th>\n",
       "      <th>entities</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count_y</th>\n",
       "      <th>text_y</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Feb 24 17:04:10 +0000 2015</td>\n",
       "      <td>570267956648792064</td>\n",
       "      <td>570267956648792064</td>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JNLpierce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 09:04:10 -0800</td>\n",
       "      <td>Boston | Waltham</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tue Feb 24 16:49:01 +0000 2015</td>\n",
       "      <td>570264145116819457</td>\n",
       "      <td>570264145116819457</td>\n",
       "      <td>@VirginAmerica I love this graphic. http://t.c...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DT_Les</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I love this graphic. http://t.c...</td>\n",
       "      <td>[40.74804263, -73.99295302]</td>\n",
       "      <td>2015-02-24 08:49:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Tue Feb 24 00:08:07 +0000 2015</td>\n",
       "      <td>570012257549070337</td>\n",
       "      <td>570012257549070337</td>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [{'text': 'elevategold', 'indices...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>5.699362e+17</td>\n",
       "      <td>5.699362e+17</td>\n",
       "      <td>...</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arieldaie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 16:08:07 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tue Feb 24 19:13:57 +0000 2015</td>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>5.702999e+17</td>\n",
       "      <td>5.702999e+17</td>\n",
       "      <td>...</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Feb 23 21:30:10 +0000 2015</td>\n",
       "      <td>569972508499283968</td>\n",
       "      <td>569972508499283968</td>\n",
       "      <td>@VirginAmerica Congrats on winning the @Travel...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://www.hootsuite.com\" rel=\"nofoll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Travelzoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica Congrats on winning the @Travel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 13:30:10 -0800</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      created_at            tweet_id  \\\n",
       "0           0  Tue Feb 24 17:04:10 +0000 2015  570267956648792064   \n",
       "1           1  Tue Feb 24 16:49:01 +0000 2015  570264145116819457   \n",
       "2           2  Tue Feb 24 00:08:07 +0000 2015  570012257549070337   \n",
       "3           3  Tue Feb 24 19:13:57 +0000 2015  570300616901320704   \n",
       "4           4  Mon Feb 23 21:30:10 +0000 2015  569972508499283968   \n",
       "\n",
       "               id_str                                             text_x  \\\n",
       "0  570267956648792064  @VirginAmerica you know what would be amazingl...   \n",
       "1  570264145116819457  @VirginAmerica I love this graphic. http://t.c...   \n",
       "2  570012257549070337  @VirginAmerica I'm #elevategold for a good rea...   \n",
       "3  570300616901320704  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "4  569972508499283968  @VirginAmerica Congrats on winning the @Travel...   \n",
       "\n",
       "   truncated                                           entities  \\\n",
       "0      False  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1      False  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2      False  {'hashtags': [{'text': 'elevategold', 'indices...   \n",
       "3      False  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4      False  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                              source  in_reply_to_status_id  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                    NaN   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...           5.699362e+17   \n",
       "3  <a href=\"https://about.twitter.com/products/tw...           5.702999e+17   \n",
       "4  <a href=\"http://www.hootsuite.com\" rel=\"nofoll...                    NaN   \n",
       "\n",
       "   in_reply_to_status_id_str  ...         airline  airline_sentiment_gold  \\\n",
       "0                        NaN  ...  Virgin America                     NaN   \n",
       "1                        NaN  ...  Virgin America                     NaN   \n",
       "2               5.699362e+17  ...  Virgin America                     NaN   \n",
       "3               5.702999e+17  ...  Virgin America                     NaN   \n",
       "4                        NaN  ...  Virgin America                     NaN   \n",
       "\n",
       "         name negativereason_gold retweet_count_y  \\\n",
       "0   JNLpierce                 NaN               0   \n",
       "1      DT_Les                 NaN               0   \n",
       "2   arieldaie                 NaN               0   \n",
       "3  cjmcginnis                 NaN               0   \n",
       "4   Travelzoo                 NaN               0   \n",
       "\n",
       "                                              text_y  \\\n",
       "0  @VirginAmerica you know what would be amazingl...   \n",
       "1  @VirginAmerica I love this graphic. http://t.c...   \n",
       "2  @VirginAmerica I'm #elevategold for a good rea...   \n",
       "3  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "4  @VirginAmerica Congrats on winning the @Travel...   \n",
       "\n",
       "                   tweet_coord              tweet_created    tweet_location  \\\n",
       "0                          NaN  2015-02-24 09:04:10 -0800  Boston | Waltham   \n",
       "1  [40.74804263, -73.99295302]  2015-02-24 08:49:01 -0800               NaN   \n",
       "2                          NaN  2015-02-23 16:08:07 -0800       Los Angeles   \n",
       "3                          NaN  2015-02-24 11:13:57 -0800  San Francisco CA   \n",
       "4                          NaN  2015-02-23 13:30:10 -0800      New York, NY   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10259"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment', 'text_x', 'text_y']]\n",
    "df.drop_duplicates(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    6421\n",
       "neutral     2126\n",
       "positive    1712\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerar solo algunos tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.loc[(df['airline_sentiment'] == 'negative')]\n",
    "df_2 = df.loc[(df['airline_sentiment'] == 'positive') | (df['airline_sentiment'] == 'neutral')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6421\n",
      "3838\n"
     ]
    }
   ],
   "source": [
    "print(len(df_1))\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se usará la columna 'texto completo' para extraer los datos de los tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-2202e51a52d4>:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_1['texto_completo'] =  df_1['text_x'].str.lower()\n",
    "df_2['texto_completo'] =  df_2['text_x'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['texto_completo'].replace({\"http\\S+\": '<linkremoved>'}, inplace=True, regex=True)\n",
    "df_2['texto_completo'].replace({\"http\\S+\": '<linkremoved>'}, inplace=True, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['texto_completo'].replace({\"<link removed>\": '<linkremoved>'}, inplace=True, regex=True)\n",
    "df_2['texto_completo'].replace({\"<link removed>\": '<linkremoved>'}, inplace=True, regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['texto_completo'].replace({\"@[^\\s]+\": '<usernameremoved>'}, inplace=True, regex=True)\n",
    "df_2['texto_completo'].replace({\"@[^\\s]+\": '<usernameremoved>'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.drop_duplicates(['texto_completo'],keep='first', inplace=True)\n",
    "df_2.drop_duplicates(['texto_completo'],keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de tweets no duplicados es 6417\n",
      "El numero de tweets no duplicados es 3740\n"
     ]
    }
   ],
   "source": [
    "print(\"El numero de tweets no duplicados es\", len(df_1)) #numero de tweets no duplicados\n",
    "print(\"El numero de tweets no duplicados es\", len(df_2)) #numero de tweets no duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove digits, puntuactions, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation+='¡¿'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove digits and puntuaction\n",
    "remove_digits = str.maketrans(digits, ' '*len(digits))#remove_digits = str.maketrans('', '', digits)\n",
    "remove_punctuation = str.maketrans(punctuation, ' '*len(punctuation))#remove_punctuation = str.maketrans('', '', punctuation)\n",
    "remove_hashtags_caracter = str.maketrans('#', ' '*len('#'))\n",
    "#las palabras de los hashtag se mantiene, pero no el simbolo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ojo, en Topic modeling no remuevo tildes. Solo lo hago en word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "data_1 = []\n",
    "for tweet in df_1['texto_completo']:\n",
    "    tweet = tweet.translate(remove_digits)\n",
    "    #tweet = tweet.lower() it wasn't a good idea,, we lost a lot of\n",
    "    tweet = tweet.translate(remove_punctuation)\n",
    "    tweet = tweet.translate(remove_hashtags_caracter)\n",
    "    tweet = tweet.lower()\n",
    "    #tweet = unidecode.unidecode(tweet)  #esta linea se hacia en word embeddings\n",
    "    #tweet = tweet.strip().split()\n",
    "    #filtered_words = [word for word in tweet if word not in stopWords]\n",
    "    #corpus[id_tweet]= filtered_words\n",
    "    #id_tweet+=1\n",
    "    data_1.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_2 = []\n",
    "for tweet in df_2['texto_completo']:\n",
    "    tweet = tweet.translate(remove_digits)\n",
    "    #tweet = tweet.lower() it wasn't a good idea,, we lost a lot of\n",
    "    tweet = tweet.translate(remove_punctuation)\n",
    "    tweet = tweet.translate(remove_hashtags_caracter)\n",
    "    tweet = tweet.lower()\n",
    "    #tweet = unidecode.unidecode(tweet)  #esta linea se hacia en word embeddings\n",
    "    #tweet = tweet.strip().split()\n",
    "    #filtered_words = [word for word in tweet if word not in stopWords]\n",
    "    #corpus[id_tweet]= filtered_words\n",
    "    #id_tweet+=1\n",
    "    data_2.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6417\n",
      "3740\n"
     ]
    }
   ],
   "source": [
    "print(len(data_1))\n",
    "print(len(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Twitter tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(tknzr.tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_1 = list(sent_to_words(data_1))\n",
    "data_words_2 = list(sent_to_words(data_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create brigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "#quizas el min count es muy bajo\n",
    "#documentaicon de bigramas \n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages\n",
    "min_count = 5#int(len(df)*0.03)\n",
    "bigram_1 = gensim.models.Phrases(data_words_1, min_count=min_count) # higher threshold fewer phrases.\n",
    "trigram_1 = gensim.models.Phrases(bigram_1[data_words_1], min_count =min_count)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod_1 = gensim.models.phrases.Phraser(bigram_1)\n",
    "trigram_mod_1 = gensim.models.phrases.Phraser(trigram_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_count = 5#int(len(df)*0.03)\n",
    "bigram_2 = gensim.models.Phrases(data_words_2, min_count=min_count) # higher threshold fewer phrases.\n",
    "trigram_2 = gensim.models.Phrases(bigram_2[data_words_2], min_count =min_count)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod_2 = gensim.models.phrases.Phraser(bigram_2)\n",
    "trigram_mod_2 = gensim.models.phrases.Phraser(trigram_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod,trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops_1 = remove_stopwords(data_words_1)#0:33\n",
    "# Remove Stop Words\n",
    "data_words_nostops_2 = remove_stopwords(data_words_2)#0:33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams_1 = make_bigrams(data_words_nostops_1, bigram_mod_1)\n",
    "# Form Trigrams\n",
    "\n",
    "data_words_trigrams_1 = make_trigrams(data_words_bigrams_1, bigram_mod_1, trigram_mod_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams_2 = make_bigrams(data_words_nostops_2, bigram_mod_2)\n",
    "# Form Trigrams\n",
    "\n",
    "data_words_trigrams_2 = make_trigrams(data_words_bigrams_2, bigram_mod_2,trigram_mod_2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar en el idioma correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.54.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "IDIOMA ACTUAL : ENGLISH\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "if idioma == 'english':\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    print(\"IDIOMA ACTUAL : ENGLISH\")\n",
    "elif idioma == 'spanish':\n",
    "    !python -m spacy download es\n",
    "    print(\"IDIOMA ACTUAL : SPANISH\")\n",
    "else:\n",
    "    print(\"Error!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDIOMA ACTUAL : ENGLISH\n"
     ]
    }
   ],
   "source": [
    "if idioma == 'english':\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    print(\"IDIOMA ACTUAL : ENGLISH\")\n",
    "elif idioma == 'spanish':\n",
    "    nlp = spacy.load('es', disable=['parser', 'ner'])\n",
    "    print(\"IDIOMA ACTUAL : SPANISH\")\n",
    "else:\n",
    "    print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_1 = lemmatization(data_words_trigrams_1, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized_2 = lemmatization(data_words_trigrams_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary and corpus for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus\n",
    "# Create Dictionary\n",
    "id2word_1 = corpora.Dictionary(data_lemmatized_1)\n",
    "\n",
    "# Create Corpus\n",
    "texts_1 = data_lemmatized_1\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_1 = [id2word_1.doc2bow(text) for text in texts_1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_2 = corpora.Dictionary(data_lemmatized_2)\n",
    "\n",
    "# Create Corpus\n",
    "texts_2 = data_lemmatized_2\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_2 = [id2word_2.doc2bow(text) for text in texts_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6417\n",
      "3740\n"
     ]
    }
   ],
   "source": [
    "print(id2word_1.num_docs) #NUMERO DE TWEETS PROCESADOS\n",
    "print(id2word_2.num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulary size es:  4480\n",
      "El vocabulary size es:  3013\n"
     ]
    }
   ],
   "source": [
    "print(\"El vocabulary size es: \",str(len(id2word_1)))\n",
    "print(\"El vocabulary size es: \",str(len(id2word_2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and select topic model (ya fueron procesados antes) Seleccionar el mejor modelo de los ya realizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence_and_model_lists_airlines_data_scenario_1.pkl\n",
    "import pickle\n",
    "with open('coherence_and_model_lists_airlines_data_scenario_2_negative_tweets.pkl', 'rb') as handle:\n",
    "    dict_models_1 = pickle.load(handle)\n",
    "    \n",
    "with open('coherence_and_model_lists_airlines_data_scenario_2_positive_neutral_tweets.pkl', 'rb') as handle:\n",
    "    dict_models_2 = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_negative_tweets = dict_models_1['model_list'][5]\n",
    "lda_positive_tweets = dict_models_2['model_list'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_positive_tweets.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_1 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_negative_tweets)\n",
    "lda_model_2 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_positive_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ Optional ] Create a new topic modeling. It is not necessary anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions about how to install mallet are available here: http://mallet.cs.umass.edu/download.php\n",
    "\n",
    "'''\n",
    "Windows installation: After unzipping MALLET, set the environment variable %MALLET_HOME% to point to the MALLET directory.\n",
    "In all command line examples, substitute bin\\mallet for bin/mallet.\n",
    "'''\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "\n",
    "path_to_mallet_binary = \"C:\\\\mallet-2.0.8\\\\bin\\\\mallet\"\n",
    "os.environ.update({'MALLET_HOME':r'C:\\mallet-2.0.8'}) #OJO!, por alguna razon mallet solo puede estar disponible en esa carpeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_2 = LdaMallet(path_to_mallet_binary, corpus=corpus_2, num_topics=5, id2word=id2word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lda mallet model to lda gensim model\n",
    "lda_model_2 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results using TopicVisExplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.201657</td>\n",
       "      <td>0.175404</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.168394</td>\n",
       "      <td>0.151515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205509</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.168145</td>\n",
       "      <td>0.153067</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>0.151515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.148810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.148810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.159296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.201657  0.175404  0.151515  0.151515  0.168394  0.151515\n",
       "1  0.205509  0.151515  0.168145  0.153067  0.170249  0.151515\n",
       "2  0.148810  0.184524  0.166667  0.184524  0.166667  0.148810\n",
       "3  0.148810  0.166667  0.202381  0.184524  0.148810  0.148810\n",
       "4  0.184619  0.166667  0.148810  0.191800  0.148810  0.159296"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_1, _ = lda_model_1.inference(corpus_1)\n",
    "matrix_documents_topic_contribution_1 /= matrix_documents_topic_contribution_1.sum(axis=1)[:, None]\n",
    "matrix_documents_topic_contribution_1 = pd.DataFrame(matrix_documents_topic_contribution_1)\n",
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_1 = pd.Series(df_1['texto_completo']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_1 = pd.concat([matrix_documents_topic_contribution_1, contents_1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>texto_completo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.201657</td>\n",
       "      <td>0.175404</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.168394</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>&lt;usernameremoved&gt; i called a 3-4 weeks ago about adding 3 flights from 2014 to my elevate...they still haven't shown up...help!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205509</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.168145</td>\n",
       "      <td>0.153067</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>&lt;usernameremoved&gt; is anyone doing anything there today?  website is useless and no one is answering the phone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>&lt;usernameremoved&gt; amazing to me that we can't get any cold air from the vents. #vx358 #noair #worstflightever #roasted #sfotobos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>&lt;usernameremoved&gt; it was a disappointing experience which will be shared with every business traveler i meet. #neverflyvirgin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.159296</td>\n",
       "      <td>&lt;usernameremoved&gt; status match program.  i applied and it's been three weeks.  called and emailed with no response.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5  \\\n",
       "0  0.201657  0.175404  0.151515  0.151515  0.168394  0.151515   \n",
       "1  0.205509  0.151515  0.168145  0.153067  0.170249  0.151515   \n",
       "2  0.148810  0.184524  0.166667  0.184524  0.166667  0.148810   \n",
       "3  0.148810  0.166667  0.202381  0.184524  0.148810  0.148810   \n",
       "4  0.184619  0.166667  0.148810  0.191800  0.148810  0.159296   \n",
       "\n",
       "                                                                                                                     texto_completo  \n",
       "0  <usernameremoved> i called a 3-4 weeks ago about adding 3 flights from 2014 to my elevate...they still haven't shown up...help!   \n",
       "1  <usernameremoved> is anyone doing anything there today?  website is useless and no one is answering the phone.                    \n",
       "2  <usernameremoved> amazing to me that we can't get any cold air from the vents. #vx358 #noair #worstflightever #roasted #sfotobos  \n",
       "3  <usernameremoved> it was a disappointing experience which will be shared with every business traveler i meet. #neverflyvirgin     \n",
       "4  <usernameremoved> status match program.  i applied and it's been three weeks.  called and emailed with no response.               "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_2, _ = lda_model_2.inference(corpus_2)\n",
    "matrix_documents_topic_contribution_2 /= matrix_documents_topic_contribution_2.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.203704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.188679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.180713</td>\n",
       "      <td>0.230935</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.231209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.240741  0.185185  0.185185  0.185185  0.203704\n",
       "1  0.200000  0.200000  0.200000  0.200000  0.200000\n",
       "2  0.203704  0.185185  0.185185  0.203704  0.222222\n",
       "3  0.207547  0.188679  0.207547  0.207547  0.188679\n",
       "4  0.178571  0.180713  0.230935  0.178571  0.231209"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.DataFrame(matrix_documents_topic_contribution_2)\n",
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_2 = pd.Series(df_2['texto_completo']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.concat([matrix_documents_topic_contribution_2, contents_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic similarity emtric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = '../data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora = vis.calculate_topic_similarity_on_multi_corpora(word_embedding_model, lda_model_1,lda_model_2, corpus_1,corpus_2, id2word_1,id2word_2, matrix_documents_topic_contribution_1,matrix_documents_topic_contribution_2, topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import topicvisexplorer\n",
    "#import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_multi_corpora( lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2,  matrix_documents_topic_contribution_1, matrix_documents_topic_contribution_2, topic_similarity_matrix_multicorpora)\n",
    "#save data\n",
    "vis.save_multi_corpora_data(\"../models_output/multi_corpora_data_airlines_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check topic similarity metric baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim_helpers \n",
    "#from _prepare import prepare, js_PCoA, PreparedData, _pcoa\n",
    "topn_terms = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = '../data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n",
      "Not found {'jetblue'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'jetblue'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'jetblue'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'jetblue'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'rebooke', 'jetblue'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'rebooke'}\n",
      "Not found {'jetblue'}\n",
      "Not found {'rebooke'}\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora_metric_baseline = vis.calculate_topic_similarity_on_multi_corpora_metric_baseline(word_embedding_model,  lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2, relevance_lambda, topn_terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n"
     ]
    }
   ],
   "source": [
    "vis.prepare_multi_corpora( lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2,  matrix_documents_topic_contribution_1, matrix_documents_topic_contribution_2, topic_similarity_matrix_multicorpora_metric_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi corpora data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "vis.save_multi_corpora_data(\"../models_output/multi_corpora_data_airlines_dataset_baseline_metric.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
