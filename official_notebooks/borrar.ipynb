{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the data lemmatized before calculating coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/coherencemodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccionar idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioma = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2021-04-17\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; \n",
    "#nltk.download('stopwords')\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from string import digits\n",
    "from string import punctuation\n",
    "import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(idioma)\n",
    "stop_words.extend(['linkremoved','amp', 'usernameremoved','link','removed', '<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "##stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"G:\\\\My Drive\\\\2019-1\\\\Tesis\\\\Topic Modeling\\\\LDA2VEC\\\\Data\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'europe_english'\n",
    "\n",
    "#full_english\n",
    "#full_spanish\n",
    "#northamerica_english\n",
    "#asia_english\n",
    "#europe_english\n",
    "#latinamerica_spanish\n",
    "#europe_spanish\n",
    "name_file = ''\n",
    "if region == 'full_english':\n",
    "    name_file = 'English_tweets_geolocated_without_preprocess_text_20190411.csv'\n",
    "elif region == 'full_spanish':\n",
    "    name_file = 'Spanish_tweets_geolocated_without_preprocess_text_20190411.csv'\n",
    "elif region == 'northamerica_english':\n",
    "    name_file = 'english_northamerica_tweets_20190411.csv'\n",
    "elif region == 'asia_english':\n",
    "    name_file = 'english_asia_tweets_20190411.csv'\n",
    "elif region == 'europe_english':\n",
    "    name_file = 'english_europe_tweets_20190411.csv'\n",
    "elif region == 'latinamerica_spanish':\n",
    "    name_file = 'spanish_latinamerica_tweets_20190411.csv'\n",
    "elif region == 'europe_spanish':\n",
    "    name_file = 'spanish_europe_tweets_20190411.csv'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_data+name_file,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111745"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111745"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se usará la columna 'texto completo' para extraer los datos de los tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-3662fb1d5b04>:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at2</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_created_at</th>\n",
       "      <th>retweet_full_text</th>\n",
       "      <th>texto_completo</th>\n",
       "      <th>ubicacion_encontrada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480940567924736</td>\n",
       "      <td>Sun Apr 01 16:24:07 +0000 2018</td>\n",
       "      <td>CamHatch</td>\n",
       "      <td>Rotterdam, The Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meet our newest product: the transparent CamHatch webcam cover! We've combined everything you loved about the Matte https://t.co/ZHCUxIFlds</td>\n",
       "      <td>Meet our newest product: the transparent CamHatch webcam cover! We've combined everything you loved about the Matte Black version with the transparency of Mark Zuckerberg. Protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  https://t.co/s9GbLf8tRG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meet our newest product: the transparent camhatch webcam cover! we've combined everything you loved about the matte black version with the transparency of mark zuckerberg. protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  &lt;link removed&gt;</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980481203110281216</td>\n",
       "      <td>Sun Apr 01 16:25:09 +0000 2018</td>\n",
       "      <td>iky86</td>\n",
       "      <td>stoke on trent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SupportOurLefty This week has been the most #Triggered I've seen Lefties since Trump won (Sadly, we have #Maybot &amp;amp; https://t.co/GPajHgkGur</td>\n",
       "      <td>@SupportOurLefty This week has been the most #Triggered I've seen Lefties since Trump won (Sadly, we have #Maybot &amp;amp; Amber Dudd  on other side) Friends! https://t.co/B9IOQD3FFB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;usernameremoved&gt; this week has been the most #triggered i've seen lefties since trump won (sadly, we have #maybot &amp;amp; amber dudd  on other side) friends! &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480454846566401</td>\n",
       "      <td>Sun Apr 01 16:22:11 +0000 2018</td>\n",
       "      <td>MichaelJF80</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pulitzer prize for this lady. Investigative journalism at it's finest. https://t.co/ojcRGpCQf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pulitzer prize for this lady. investigative journalism at it's finest. &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480424429400071</td>\n",
       "      <td>Sun Apr 01 16:22:04 +0000 2018</td>\n",
       "      <td>ruechids1</td>\n",
       "      <td>Enfield, London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theres no more secrecy, confident and privacy and no protection of personal data. Our lives are traded on the open https://t.co/jSIOYGP0mu</td>\n",
       "      <td>Theres no more secrecy, confident and privacy and no protection of personal data. Our lives are traded on the open market! https://t.co/gVYQCH3qvJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theres no more secrecy, confident and privacy and no protection of personal data. our lives are traded on the open market! &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980480325083164673</td>\n",
       "      <td>Sun Apr 01 16:21:40 +0000 2018</td>\n",
       "      <td>Hugh_Macfarlane</td>\n",
       "      <td>Glenrothes, Fife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AggregateIQ: the obscure Canadian tech firm and the Brexit data riddle https://t.co/nzEsS4mVbi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aggregateiq: the obscure canadian tech firm and the brexit data riddle &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980482011507908608</td>\n",
       "      <td>Sun Apr 01 16:28:22 +0000 2018</td>\n",
       "      <td>amasoliverdilme</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80 Best Data Science Books That Are Worthy Reading https://t.co/ggd5ebEBLq #bigdata #datascience https://t.co/Pz20Aqe6HK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80 best data science books that are worthy reading &lt;link removed&gt; #bigdata #datascience &lt;link removed&gt;</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980482066977640449</td>\n",
       "      <td>Sun Apr 01 16:28:35 +0000 2018</td>\n",
       "      <td>tinchissy</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You cant just go swanning around being *Hacked off* and expect us plebs to keep you up to date. Awake. That is wha https://t.co/7lJt54Tovb</td>\n",
       "      <td>You cant just go swanning around being *Hacked off* and expect us plebs to keep you up to date. Awake. That is what you need to be young man. AWAKE! We need you now. Put the champagne down. https://t.co/EmGYJ5qtmb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you cant just go swanning around being *hacked off* and expect us plebs to keep you up to date. awake. that is what you need to be young man. awake! we need you now. put the champagne down. &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980482108228501506</td>\n",
       "      <td>Sun Apr 01 16:28:45 +0000 2018</td>\n",
       "      <td>Tonybirte</td>\n",
       "      <td>Antwerp, Belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recommend everyone does this in the #FBPE community. https://t.co/qkIdUVcJgY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recommend everyone does this in the #fbpe community. &lt;link removed&gt;</td>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980482280358666240</td>\n",
       "      <td>Sun Apr 01 16:29:26 +0000 2018</td>\n",
       "      <td>thetommoriarty</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@omminc I'm pretty sure many who voted Leave had no idea what they were voting for and had been conditioned by https://t.co/ivBv7KmbAs</td>\n",
       "      <td>@omminc I'm pretty sure many who voted Leave had no idea what they were voting for and had been conditioned by #CambridgeAnalytica to vote based on a lie they had been programmed with. They voted that way because they were told to.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;usernameremoved&gt; i'm pretty sure many who voted leave had no idea what they were voting for and had been conditioned by #cambridgeanalytica to vote based on a lie they had been programmed with. they voted that way because they were told to.</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>id-980482244564471809</td>\n",
       "      <td>Sun Apr 01 16:29:18 +0000 2018</td>\n",
       "      <td>WillBlackWriter</td>\n",
       "      <td>Cambridge, European Union</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The longer @CamAnalytica ignores questions about this, the more suspicious people will be that their data was someh https://t.co/xIF0fDgbrm</td>\n",
       "      <td>The longer @CamAnalytica ignores questions about this, the more suspicious people will be that their data was somehow given to Russia to aid its hybrid war on democracy. https://t.co/BeHQAgi90D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the longer &lt;usernameremoved&gt; ignores questions about this, the more suspicious people will be that their data was somehow given to russia to aid its hybrid war on democracy. &lt;link removed&gt;</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 created_at2                     id  \\\n",
       "0  2           2018-04-01  id-980480940567924736   \n",
       "1  17          2018-04-01  id-980481203110281216   \n",
       "2  43          2018-04-01  id-980480454846566401   \n",
       "3  45          2018-04-01  id-980480424429400071   \n",
       "4  51          2018-04-01  id-980480325083164673   \n",
       "5  73          2018-04-01  id-980482011507908608   \n",
       "6  77          2018-04-01  id-980482066977640449   \n",
       "7  90          2018-04-01  id-980482108228501506   \n",
       "8  91          2018-04-01  id-980482280358666240   \n",
       "9  94          2018-04-01  id-980482244564471809   \n",
       "\n",
       "                       created_at user.screen_name  \\\n",
       "0  Sun Apr 01 16:24:07 +0000 2018  CamHatch          \n",
       "1  Sun Apr 01 16:25:09 +0000 2018  iky86             \n",
       "2  Sun Apr 01 16:22:11 +0000 2018  MichaelJF80       \n",
       "3  Sun Apr 01 16:22:04 +0000 2018  ruechids1         \n",
       "4  Sun Apr 01 16:21:40 +0000 2018  Hugh_Macfarlane   \n",
       "5  Sun Apr 01 16:28:22 +0000 2018  amasoliverdilme   \n",
       "6  Sun Apr 01 16:28:35 +0000 2018  tinchissy         \n",
       "7  Sun Apr 01 16:28:45 +0000 2018  Tonybirte         \n",
       "8  Sun Apr 01 16:29:26 +0000 2018  thetommoriarty    \n",
       "9  Sun Apr 01 16:29:18 +0000 2018  WillBlackWriter   \n",
       "\n",
       "                user_location coordinates  \\\n",
       "0  Rotterdam, The Netherlands  NaN          \n",
       "1  stoke on trent              NaN          \n",
       "2  Liverpool, England          NaN          \n",
       "3  Enfield, London             NaN          \n",
       "4  Glenrothes, Fife            NaN          \n",
       "5  Barcelona                   NaN          \n",
       "6  united kingdom              NaN          \n",
       "7  Antwerp, Belgium            NaN          \n",
       "8  London                      NaN          \n",
       "9  Cambridge, European Union   NaN          \n",
       "\n",
       "                                                                                                                                              text  \\\n",
       "0  Meet our newest product: the transparent CamHatch webcam cover! We've combined everything you loved about the Matte https://t.co/ZHCUxIFlds       \n",
       "1  @SupportOurLefty This week has been the most #Triggered I've seen Lefties since Trump won (Sadly, we have #Maybot &amp; https://t.co/GPajHgkGur   \n",
       "2  Pulitzer prize for this lady. Investigative journalism at it's finest. https://t.co/ojcRGpCQf4                                                    \n",
       "3  Theres no more secrecy, confident and privacy and no protection of personal data. Our lives are traded on the open https://t.co/jSIOYGP0mu        \n",
       "4  AggregateIQ: the obscure Canadian tech firm and the Brexit data riddle https://t.co/nzEsS4mVbi                                                    \n",
       "5  80 Best Data Science Books That Are Worthy Reading https://t.co/ggd5ebEBLq #bigdata #datascience https://t.co/Pz20Aqe6HK                          \n",
       "6  You cant just go swanning around being *Hacked off* and expect us plebs to keep you up to date. Awake. That is wha https://t.co/7lJt54Tovb        \n",
       "7  Recommend everyone does this in the #FBPE community. https://t.co/qkIdUVcJgY                                                                      \n",
       "8  @omminc I'm pretty sure many who voted Leave had no idea what they were voting for and had been conditioned by https://t.co/ivBv7KmbAs            \n",
       "9  The longer @CamAnalytica ignores questions about this, the more suspicious people will be that their data was someh https://t.co/xIF0fDgbrm       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                 full_text  \\\n",
       "0  Meet our newest product: the transparent CamHatch webcam cover! We've combined everything you loved about the Matte Black version with the transparency of Mark Zuckerberg. Protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  https://t.co/s9GbLf8tRG   \n",
       "1  @SupportOurLefty This week has been the most #Triggered I've seen Lefties since Trump won (Sadly, we have #Maybot &amp; Amber Dudd  on other side) Friends! https://t.co/B9IOQD3FFB                                                                                                                       \n",
       "2  NaN                                                                                                                                                                                                                                                                                                       \n",
       "3  Theres no more secrecy, confident and privacy and no protection of personal data. Our lives are traded on the open market! https://t.co/gVYQCH3qvJ                                                                                                                                                        \n",
       "4  NaN                                                                                                                                                                                                                                                                                                       \n",
       "5  NaN                                                                                                                                                                                                                                                                                                       \n",
       "6  You cant just go swanning around being *Hacked off* and expect us plebs to keep you up to date. Awake. That is what you need to be young man. AWAKE! We need you now. Put the champagne down. https://t.co/EmGYJ5qtmb                                                                                     \n",
       "7  NaN                                                                                                                                                                                                                                                                                                       \n",
       "8  @omminc I'm pretty sure many who voted Leave had no idea what they were voting for and had been conditioned by #CambridgeAnalytica to vote based on a lie they had been programmed with. They voted that way because they were told to.                                                                   \n",
       "9  The longer @CamAnalytica ignores questions about this, the more suspicious people will be that their data was somehow given to Russia to aid its hybrid war on democracy. https://t.co/BeHQAgi90D                                                                                                         \n",
       "\n",
       "   retweet_created_at  retweet_full_text  \\\n",
       "0 NaN                 NaN                  \n",
       "1 NaN                 NaN                  \n",
       "2 NaN                 NaN                  \n",
       "3 NaN                 NaN                  \n",
       "4 NaN                 NaN                  \n",
       "5 NaN                 NaN                  \n",
       "6 NaN                 NaN                  \n",
       "7 NaN                 NaN                  \n",
       "8 NaN                 NaN                  \n",
       "9 NaN                 NaN                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   texto_completo  \\\n",
       "0  meet our newest product: the transparent camhatch webcam cover! we've combined everything you loved about the matte black version with the transparency of mark zuckerberg. protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  <link removed>   \n",
       "1  <usernameremoved> this week has been the most #triggered i've seen lefties since trump won (sadly, we have #maybot &amp; amber dudd  on other side) friends! <link removed>                                                                                                                      \n",
       "2  pulitzer prize for this lady. investigative journalism at it's finest. <link removed>                                                                                                                                                                                                            \n",
       "3  theres no more secrecy, confident and privacy and no protection of personal data. our lives are traded on the open market! <link removed>                                                                                                                                                        \n",
       "4  aggregateiq: the obscure canadian tech firm and the brexit data riddle <link removed>                                                                                                                                                                                                            \n",
       "5  80 best data science books that are worthy reading <link removed> #bigdata #datascience <link removed>                                                                                                                                                                                           \n",
       "6  you cant just go swanning around being *hacked off* and expect us plebs to keep you up to date. awake. that is what you need to be young man. awake! we need you now. put the champagne down. <link removed>                                                                                     \n",
       "7  recommend everyone does this in the #fbpe community. <link removed>                                                                                                                                                                                                                              \n",
       "8  <usernameremoved> i'm pretty sure many who voted leave had no idea what they were voting for and had been conditioned by #cambridgeanalytica to vote based on a lie they had been programmed with. they voted that way because they were told to.                                                \n",
       "9  the longer <usernameremoved> ignores questions about this, the more suspicious people will be that their data was somehow given to russia to aid its hybrid war on democracy. <link removed>                                                                                                     \n",
       "\n",
       "  ubicacion_encontrada  \n",
       "0  Netherlands          \n",
       "1  United Kingdom       \n",
       "2  United Kingdom       \n",
       "3  United Kingdom       \n",
       "4  United Kingdom       \n",
       "5  Spain                \n",
       "6  United Kingdom       \n",
       "7  Belgium              \n",
       "8  United Kingdom       \n",
       "9  United Kingdom       "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df['texto_completo'] =  df['texto_completo'].str.lower()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_completo'].replace({\"http\\S+\": '<linkremoved>'}, inplace=True, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_completo'].replace({\"<link removed>\": '<linkremoved>'}, inplace=True, regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    meet our newest product: the transparent camhatch webcam cover! we've combined everything you loved about the matte black version with the transparency of mark zuckerberg. protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  <linkremoved>\n",
       "1    <usernameremoved> this week has been the most #triggered i've seen lefties since trump won (sadly, we have #maybot &amp; amber dudd  on other side) friends! <linkremoved>                                                                                                                   \n",
       "2    pulitzer prize for this lady. investigative journalism at it's finest. <linkremoved>                                                                                                                                                                                                         \n",
       "3    theres no more secrecy, confident and privacy and no protection of personal data. our lives are traded on the open market! <linkremoved>                                                                                                                                                     \n",
       "4    aggregateiq: the obscure canadian tech firm and the brexit data riddle <linkremoved>                                                                                                                                                                                                         \n",
       "Name: texto_completo, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['texto_completo'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_completo'].replace({\"@[^\\s]+\": '<usernameremoved>'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111745"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['texto_completo'],keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de tweets no duplicados es 111695\n"
     ]
    }
   ],
   "source": [
    "print(\"El numero de tweets no duplicados es\", len(df)) #numero de tweets no duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de usuarios es 46918\n"
     ]
    }
   ],
   "source": [
    "# numero de usuarios\n",
    "tweets_by_user_data = df.groupby('user.screen_name').agg('size').sort_values(ascending=False).reset_index()\n",
    "print(\"El numero de usuarios es\",len(tweets_by_user_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove digits, puntuactions, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation+='¡¿'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove digits and puntuaction\n",
    "remove_digits = str.maketrans(digits, ' '*len(digits))#remove_digits = str.maketrans('', '', digits)\n",
    "remove_punctuation = str.maketrans(punctuation, ' '*len(punctuation))#remove_punctuation = str.maketrans('', '', punctuation)\n",
    "remove_hashtags_caracter = str.maketrans('#', ' '*len('#'))\n",
    "#las palabras de los hashtag se mantiene, pero no el simbolo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    meet our newest product: the transparent camhatch webcam cover! we've combined everything you loved about the matte black version with the transparency of mark zuckerberg. protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  <linkremoved>\n",
       "1    <usernameremoved> this week has been the most #triggered i've seen lefties since trump won (sadly, we have #maybot &amp; amber dudd  on other side) friends! <linkremoved>                                                                                                                   \n",
       "2    pulitzer prize for this lady. investigative journalism at it's finest. <linkremoved>                                                                                                                                                                                                         \n",
       "3    theres no more secrecy, confident and privacy and no protection of personal data. our lives are traded on the open market! <linkremoved>                                                                                                                                                     \n",
       "4    aggregateiq: the obscure canadian tech firm and the brexit data riddle <linkremoved>                                                                                                                                                                                                         \n",
       "Name: texto_completo, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['texto_completo'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['texto_completo'].replace({\"ee.uu\": 'eeuu'}, inplace=True, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df[df['texto_completo'].str.contains('ee.uu', regex = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(a)+51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ojo, en Topic modeling no remuevo tildes. Solo lo hago en word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "data = []\n",
    "for tweet in df['texto_completo']:\n",
    "    tweet = tweet.translate(remove_digits)\n",
    "    #tweet = tweet.lower() it wasn't a good idea,, we lost a lot of\n",
    "    tweet = tweet.translate(remove_punctuation)\n",
    "    tweet = tweet.translate(remove_hashtags_caracter)\n",
    "    tweet = tweet.lower()\n",
    "    #tweet = unidecode.unidecode(tweet)  #esta linea se hacia en word embeddings\n",
    "    #tweet = tweet.strip().split()\n",
    "    #filtered_words = [word for word in tweet if word not in stopWords]\n",
    "    #corpus[id_tweet]= filtered_words\n",
    "    #id_tweet+=1\n",
    "    data.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meet our newest product  the transparent camhatch webcam cover  we ve combined everything you loved about the matte black version with the transparency of mark zuckerberg  protect your privacy without having to change the appearance of your laptop  tablet or smartphone    linkremoved ',\n",
       " ' usernameremoved  this week has been the most  triggered i ve seen lefties since trump won  sadly  we have  maybot  amp  amber dudd  on other side  friends   linkremoved ',\n",
       " 'pulitzer prize for this lady  investigative journalism at it s finest   linkremoved ',\n",
       " 'theres no more secrecy  confident and privacy and no protection of personal data  our lives are traded on the open market   linkremoved ',\n",
       " 'aggregateiq  the obscure canadian tech firm and the brexit data riddle  linkremoved ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111695"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111695"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' usernameremoved  this week has been the most  triggered i ve seen lefties '\n",
      " 'since trump won  sadly  we have  maybot  amp  amber dudd  on other side  '\n",
      " 'friends   linkremoved ')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Twitter tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(tknzr.tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "#0:27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usernameremoved', 'this', 'week', 'has', 'been', 'the', 'most', 'triggered', 'i', 've', 'seen', 'lefties', 'since', 'trump', 'won', 'sadly', 'we', 'have', 'maybot', 'amp', 'amber', 'dudd', 'on', 'other', 'side', 'friends', 'linkremoved']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggregateiq', 'the', 'obscure', 'canadian', 'tech', 'firm', 'and', 'the', 'brexit', 'data', 'riddle', 'linkremoved']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'data', 'science', 'books', 'that', 'are', 'worthy', 'reading', 'linkremoved', 'bigdata', 'datascience', 'linkremoved']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create brigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usernameremoved', 'this_week', 'has_been', 'the', 'most', 'triggered', 'i_ve_seen', 'lefties', 'since', 'trump_won', 'sadly', 'we', 'have', 'maybot', 'amp', 'amber', 'dudd', 'on', 'other_side', 'friends', 'linkremoved']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "#quizas el min count es muy bajo\n",
    "#documentaicon de bigramas \n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages\n",
    "min_count = 5#int(len(df)*0.03)\n",
    "bigram = gensim.models.Phrases(data_words, min_count=min_count) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], min_count =min_count)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meet', 'our_newest', 'product', 'the', 'transparent', 'camhatch', 'webcam', 'cover', 'we_ve', 'combined', 'everything', 'you', 'loved', 'about', 'the', 'matte', 'black', 'version', 'with', 'the', 'transparency', 'of', 'mark_zuckerberg', 'protect_your_privacy', 'without_having', 'to', 'change', 'the', 'appearance', 'of', 'your', 'laptop', 'tablet', 'or', 'smartphone', 'linkremoved']\n",
      "['usernameremoved', 'this_week', 'has_been', 'the', 'most', 'triggered', 'i_ve_seen', 'lefties', 'since', 'trump_won', 'sadly', 'we', 'have', 'maybot', 'amp', 'amber', 'dudd', 'on', 'other_side', 'friends', 'linkremoved']\n",
      "['pulitzer', 'prize', 'for', 'this', 'lady', 'investigative_journalism', 'at', 'it', 's', 'finest', 'linkremoved']\n",
      "['theres_no', 'more', 'secrecy', 'confident', 'and', 'privacy', 'and', 'no', 'protection', 'of', 'personal_data', 'our_lives', 'are', 'traded', 'on', 'the', 'open', 'market', 'linkremoved']\n",
      "['aggregateiq', 'the', 'obscure_canadian_tech', 'firm', 'and', 'the', 'brexit_data_riddle', 'linkremoved']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "#print(trigram_mod[bigram_mod[data_words[1]]])\n",
    "for i in range(0,5):\n",
    "    print(trigram_mod[bigram_mod[data_words[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)#0:33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meet',\n",
       " 'newest',\n",
       " 'product',\n",
       " 'transparent',\n",
       " 'camhatch',\n",
       " 'webcam',\n",
       " 'cover',\n",
       " 'combined',\n",
       " 'everything',\n",
       " 'loved',\n",
       " 'matte',\n",
       " 'black',\n",
       " 'version',\n",
       " 'transparency',\n",
       " 'mark',\n",
       " 'zuckerberg',\n",
       " 'protect',\n",
       " 'privacy',\n",
       " 'without',\n",
       " 'change',\n",
       " 'appearance',\n",
       " 'laptop',\n",
       " 'tablet',\n",
       " 'smartphone']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['meet',\n",
       "  'newest',\n",
       "  'product',\n",
       "  'transparent',\n",
       "  'camhatch',\n",
       "  'webcam',\n",
       "  'cover',\n",
       "  'combined',\n",
       "  'everything',\n",
       "  'loved',\n",
       "  'matte',\n",
       "  'black',\n",
       "  'version',\n",
       "  'transparency',\n",
       "  'mark',\n",
       "  'zuckerberg',\n",
       "  'protect',\n",
       "  'privacy',\n",
       "  'without',\n",
       "  'change',\n",
       "  'appearance',\n",
       "  'laptop',\n",
       "  'tablet',\n",
       "  'smartphone'],\n",
       " ['week',\n",
       "  'triggered',\n",
       "  'seen',\n",
       "  'lefties',\n",
       "  'since',\n",
       "  'trump',\n",
       "  'sadly',\n",
       "  'maybot',\n",
       "  'amber',\n",
       "  'dudd',\n",
       "  'side',\n",
       "  'friends'],\n",
       " ['pulitzer', 'prize', 'lady', 'investigative', 'journalism', 'finest'],\n",
       " ['theres',\n",
       "  'secrecy',\n",
       "  'confident',\n",
       "  'privacy',\n",
       "  'protection',\n",
       "  'personal',\n",
       "  'data',\n",
       "  'lives',\n",
       "  'traded',\n",
       "  'open',\n",
       "  'market'],\n",
       " ['aggregateiq',\n",
       "  'obscure',\n",
       "  'canadian',\n",
       "  'tech',\n",
       "  'firm',\n",
       "  'brexit',\n",
       "  'data',\n",
       "  'riddle'],\n",
       " ['best',\n",
       "  'data',\n",
       "  'science',\n",
       "  'books',\n",
       "  'worthy',\n",
       "  'reading',\n",
       "  'bigdata',\n",
       "  'datascience'],\n",
       " ['cant',\n",
       "  'go',\n",
       "  'swanning',\n",
       "  'around',\n",
       "  'hacked',\n",
       "  'expect',\n",
       "  'us',\n",
       "  'plebs',\n",
       "  'keep',\n",
       "  'date',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'young',\n",
       "  'man',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'put',\n",
       "  'champagne'],\n",
       " ['recommend', 'everyone', 'fbpe', 'community'],\n",
       " ['pretty',\n",
       "  'sure',\n",
       "  'many',\n",
       "  'voted',\n",
       "  'leave',\n",
       "  'idea',\n",
       "  'voting',\n",
       "  'conditioned',\n",
       "  'vote',\n",
       "  'based',\n",
       "  'lie',\n",
       "  'programmed',\n",
       "  'voted',\n",
       "  'way',\n",
       "  'told'],\n",
       " ['longer',\n",
       "  'ignores',\n",
       "  'questions',\n",
       "  'suspicious',\n",
       "  'people',\n",
       "  'data',\n",
       "  'somehow',\n",
       "  'given',\n",
       "  'russia',\n",
       "  'aid',\n",
       "  'hybrid',\n",
       "  'war',\n",
       "  'democracy']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pulitzer', 'prize', 'lady', 'investigative', 'journalism', 'finest']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Form Trigrams\n",
    "\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint\n",
    "#data_words_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar en el idioma correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.54.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gonza\\tesisenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "IDIOMA ACTUAL : ENGLISH\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "if idioma == 'english':\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    print(\"IDIOMA ACTUAL : ENGLISH\")\n",
    "elif idioma == 'spanish':\n",
    "    !python -m spacy download es\n",
    "    print(\"IDIOMA ACTUAL : SPANISH\")\n",
    "else:\n",
    "    print(\"Error!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDIOMA ACTUAL : ENGLISH\n"
     ]
    }
   ],
   "source": [
    "if idioma == 'english':\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    print(\"IDIOMA ACTUAL : ENGLISH\")\n",
    "elif idioma == 'spanish':\n",
    "    nlp = spacy.load('es', disable=['parser', 'ner'])\n",
    "    print(\"IDIOMA ACTUAL : SPANISH\")\n",
    "else:\n",
    "    print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['meet', 'new', 'product', 'transparent', 'camhatch', 'webcam', 'cover', 'combine', 'love', 'black', 'version', 'transparency', 'protect', 'privacy', 'change', 'appearance', 'laptop', 'tablet', 'smartphone']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#Aqui aparecen la lista de postags disponibles, https://spacy.io/api/annotation solo lematizaremos en una seleccion\n",
    "\n",
    "print(data_lemmatized[:1])#4:13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['meet',\n",
       "  'new',\n",
       "  'product',\n",
       "  'transparent',\n",
       "  'camhatch',\n",
       "  'webcam',\n",
       "  'cover',\n",
       "  'combine',\n",
       "  'love',\n",
       "  'black',\n",
       "  'version',\n",
       "  'transparency',\n",
       "  'protect',\n",
       "  'privacy',\n",
       "  'change',\n",
       "  'appearance',\n",
       "  'laptop',\n",
       "  'tablet',\n",
       "  'smartphone'],\n",
       " ['week',\n",
       "  'trigger',\n",
       "  'see',\n",
       "  'lefty',\n",
       "  'trump',\n",
       "  'sadly',\n",
       "  'maybot',\n",
       "  'dudd',\n",
       "  'side',\n",
       "  'friend'],\n",
       " ['fine'],\n",
       " ['s', 'life', 'trade', 'open', 'market'],\n",
       " ['aggregateiq', 'firm', 'brexit_data_riddle'],\n",
       " ['good', 'book', 'worthy', 'reading', 'bigdata_datascience'],\n",
       " ['can',\n",
       "  'go',\n",
       "  'swanne',\n",
       "  'around',\n",
       "  'hack',\n",
       "  'expect',\n",
       "  'pleb',\n",
       "  'keep',\n",
       "  'date',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'put',\n",
       "  'champagne'],\n",
       " ['recommend', 'fbpe', 'community'],\n",
       " ['pretty_sure',\n",
       "  'many',\n",
       "  'idea',\n",
       "  'voting',\n",
       "  'condition',\n",
       "  'vote',\n",
       "  'base',\n",
       "  'vote',\n",
       "  'way',\n",
       "  'tell'],\n",
       " ['long',\n",
       "  'ignore',\n",
       "  'question',\n",
       "  'suspicious',\n",
       "  'people',\n",
       "  'datum',\n",
       "  'somehow',\n",
       "  'give',\n",
       "  'aid',\n",
       "  'hybrid',\n",
       "  'war',\n",
       "  'democracy'],\n",
       " ['decision', 'delete', 'old', 'account', 'take'],\n",
       " ['business', 'bigdata_datascience_machinelearning_deeplearning', 'iot'],\n",
       " [],\n",
       " ['post'],\n",
       " ['founder', 'leave', 'company'],\n",
       " ['quite', 'thread'],\n",
       " ['black',\n",
       "  'third',\n",
       "  'time',\n",
       "  'turn',\n",
       "  'parliament',\n",
       "  'request',\n",
       "  'answer_question',\n",
       "  'issue'],\n",
       " ['look',\n",
       "  'halve',\n",
       "  'next',\n",
       "  'couple',\n",
       "  'year',\n",
       "  'basically',\n",
       "  'techlash',\n",
       "  'deletefacebook',\n",
       "  'digitalfix'],\n",
       " ['else', 'paranoid', 'thought', 'psyop', 'culture', 'war'],\n",
       " ['suspicious',\n",
       "  'speed',\n",
       "  'get',\n",
       "  'warrant',\n",
       "  'execute',\n",
       "  'seem',\n",
       "  'want',\n",
       "  'evidence',\n",
       "  'grubby',\n",
       "  'hand'],\n",
       " ['facebook',\n",
       "  'cheat',\n",
       "  'advertiser',\n",
       "  'video',\n",
       "  'really',\n",
       "  'watch',\n",
       "  'fake',\n",
       "  'targeting',\n",
       "  'spread',\n",
       "  'steal',\n",
       "  'content',\n",
       "  'maybe',\n",
       "  'get',\n",
       "  'care'],\n",
       " ['information', 'legal', 'connection', 'beleave'],\n",
       " ['never_mind', 'seeing', 'know', 'rt'],\n",
       " ['pay', 'whiff'],\n",
       " ['want', 'skilled', 'migrant', 'create', 'new', 'hurdle', 'bigdata_ividata'],\n",
       " ['tory',\n",
       "  'news',\n",
       "  'network',\n",
       "  'try',\n",
       "  'hide',\n",
       "  'story',\n",
       "  'involve',\n",
       "  'tory',\n",
       "  'company',\n",
       "  'sense',\n",
       "  'believe',\n",
       "  'endorse',\n",
       "  'antisemitism',\n",
       "  'vote',\n",
       "  'labour'],\n",
       " ['go'],\n",
       " ['give',\n",
       "  'ago',\n",
       "  'little',\n",
       "  'wonder',\n",
       "  'cambridge_university',\n",
       "  'get',\n",
       "  'tie',\n",
       "  'mention',\n",
       "  'pesky',\n",
       "  'latent',\n",
       "  'trait',\n",
       "  'problem',\n",
       "  'embarrass',\n",
       "  'fore'],\n",
       " ['scottish_government_believe', 'could', 'trigger', 'second'],\n",
       " []]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week',\n",
       " 'trigger',\n",
       " 'see',\n",
       " 'lefty',\n",
       " 'trump',\n",
       " 'sadly',\n",
       " 'maybot',\n",
       " 'dudd',\n",
       " 'side',\n",
       " 'friend']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long',\n",
       " 'ignore',\n",
       " 'question',\n",
       " 'suspicious',\n",
       " 'people',\n",
       " 'datum',\n",
       " 'somehow',\n",
       " 'give',\n",
       " 'aid',\n",
       " 'hybrid',\n",
       " 'war',\n",
       " 'democracy']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['meet',\n",
       "  'new',\n",
       "  'product',\n",
       "  'transparent',\n",
       "  'camhatch',\n",
       "  'webcam',\n",
       "  'cover',\n",
       "  'combine',\n",
       "  'love',\n",
       "  'black',\n",
       "  'version',\n",
       "  'transparency',\n",
       "  'protect',\n",
       "  'privacy',\n",
       "  'change',\n",
       "  'appearance',\n",
       "  'laptop',\n",
       "  'tablet',\n",
       "  'smartphone'],\n",
       " ['week',\n",
       "  'trigger',\n",
       "  'see',\n",
       "  'lefty',\n",
       "  'trump',\n",
       "  'sadly',\n",
       "  'maybot',\n",
       "  'dudd',\n",
       "  'side',\n",
       "  'friend'],\n",
       " ['fine'],\n",
       " ['s', 'life', 'trade', 'open', 'market'],\n",
       " ['aggregateiq', 'firm', 'brexit_data_riddle'],\n",
       " ['good', 'book', 'worthy', 'reading', 'bigdata_datascience'],\n",
       " ['can',\n",
       "  'go',\n",
       "  'swanne',\n",
       "  'around',\n",
       "  'hack',\n",
       "  'expect',\n",
       "  'pleb',\n",
       "  'keep',\n",
       "  'date',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'awake',\n",
       "  'need',\n",
       "  'put',\n",
       "  'champagne'],\n",
       " ['recommend', 'fbpe', 'community'],\n",
       " ['pretty_sure',\n",
       "  'many',\n",
       "  'idea',\n",
       "  'voting',\n",
       "  'condition',\n",
       "  'vote',\n",
       "  'base',\n",
       "  'vote',\n",
       "  'way',\n",
       "  'tell'],\n",
       " ['long',\n",
       "  'ignore',\n",
       "  'question',\n",
       "  'suspicious',\n",
       "  'people',\n",
       "  'datum',\n",
       "  'somehow',\n",
       "  'give',\n",
       "  'aid',\n",
       "  'hybrid',\n",
       "  'war',\n",
       "  'democracy']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    meet our newest product: the transparent camhatch webcam cover! we've combined everything you loved about the matte black version with the transparency of mark zuckerberg. protect your privacy without having to change the appearance of your laptop, tablet or smartphone.  <linkremoved>\n",
       "1    <usernameremoved> this week has been the most #triggered i've seen lefties since trump won (sadly, we have #maybot &amp; amber dudd  on other side) friends! <linkremoved>                                                                                                                   \n",
       "2    pulitzer prize for this lady. investigative journalism at it's finest. <linkremoved>                                                                                                                                                                                                         \n",
       "3    theres no more secrecy, confident and privacy and no protection of personal data. our lives are traded on the open market! <linkremoved>                                                                                                                                                     \n",
       "4    aggregateiq: the obscure canadian tech firm and the brexit data riddle <linkremoved>                                                                                                                                                                                                         \n",
       "5    80 best data science books that are worthy reading <linkremoved> #bigdata #datascience <linkremoved>                                                                                                                                                                                         \n",
       "6    you cant just go swanning around being *hacked off* and expect us plebs to keep you up to date. awake. that is what you need to be young man. awake! we need you now. put the champagne down. <linkremoved>                                                                                  \n",
       "7    recommend everyone does this in the #fbpe community. <linkremoved>                                                                                                                                                                                                                           \n",
       "8    <usernameremoved> i'm pretty sure many who voted leave had no idea what they were voting for and had been conditioned by #cambridgeanalytica to vote based on a lie they had been programmed with. they voted that way because they were told to.                                            \n",
       "9    the longer <usernameremoved> ignores questions about this, the more suspicious people will be that their data was somehow given to russia to aid its hybrid war on democracy. <linkremoved>                                                                                                  \n",
       "Name: texto_completo, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['texto_completo'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('wordnet') #esto no funciona en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lemmatizer = WordNetLemmatizer() \n",
    "#def lemmatization_tweets(sentence):\n",
    " #   for word in sentence:\n",
    "  #      yield(lemmatizer.lemmatize(word, pos=\"v\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba = list(lemmatization_tweets(data_words_nostops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary and corpus for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111695"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word.num_docs #NUMERO DE TWEETS PROCESADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulary size es:  37758\n"
     ]
    }
   ],
   "source": [
    "print(\"El vocabulary size es: \",str(len(id2word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0, 1) above implies, word id 0 occurs once in the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'appearance'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('appearance', 1),\n",
       "  ('black', 1),\n",
       "  ('camhatch', 1),\n",
       "  ('change', 1),\n",
       "  ('combine', 1),\n",
       "  ('cover', 1),\n",
       "  ('laptop', 1),\n",
       "  ('love', 1),\n",
       "  ('meet', 1),\n",
       "  ('new', 1),\n",
       "  ('privacy', 1),\n",
       "  ('product', 1),\n",
       "  ('protect', 1),\n",
       "  ('smartphone', 1),\n",
       "  ('tablet', 1),\n",
       "  ('transparency', 1),\n",
       "  ('transparent', 1),\n",
       "  ('version', 1),\n",
       "  ('webcam', 1)]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim - Build the topic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim - View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(lda_model.print_topics())\n",
    "#doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim -  Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "#coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular mejor numero de topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions about how to install mallet are available here: http://mallet.cs.umass.edu/download.php\n",
    "\n",
    "'''\n",
    "Windows installation: After unzipping MALLET, set the environment variable %MALLET_HOME% to point to the MALLET directory.\n",
    "In all command line examples, substitute bin\\mallet for bin/mallet.\n",
    "'''\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "\n",
    "path_to_mallet_binary = \"C:\\\\mallet-2.0.8\\\\bin\\\\mallet\"\n",
    "os.environ.update({'MALLET_HOME':r'C:\\mallet-2.0.8'}) #OJO!, por alguna razon mallet solo puede estar disponible en esa carpeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def compute_coherence_values(dictionary, corpus, texts, list_num_topics,window_size,topn):\n",
    "    print(\"NUMERO DE TOPICOS A CALCULAR\",list_num_topics)\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    #modificar aqui la lista de valores en las que se desea probar\n",
    "    \n",
    "    #for num_topics in range(start, limit, step):\n",
    "    for num_topics in list_num_topics:\n",
    "        model = gensim.models.wrappers.LdaMallet(path_to_mallet_binary, corpus=corpus, num_topics=num_topics, id2word=id2word, random_seed=1)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v', window_size=window_size, topn=topn)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERO DE TOPICOS A CALCULAR [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "00:15:48.03\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#list_num_topics= [5,8,11,15,20,50, 100]\n",
    "list_num_topics = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "#list_num_topics = [4]\n",
    "topn=20\n",
    "window_size=110\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, list_num_topics= list_num_topics,window_size = window_size, topn=topn)\n",
    "#ojo, el numero de start, limit y step no tiene sentido en la linea anterior, se modifican directamente en el algoritmo\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "#04:46\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "\n",
    "#x = range(start, limit, step)\n",
    "\n",
    "x = list_num_topics\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title(\"Coherence Score on \"+region)\n",
    "plt.plot(x, coherence_values)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Num topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.xticks(x, x)\n",
    "\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 1  has Coherence Value of 0.2505\n",
      "Num Topics = 2  has Coherence Value of 0.1247\n",
      "Num Topics = 3  has Coherence Value of 0.209\n",
      "Num Topics = 4  has Coherence Value of 0.2787\n",
      "Num Topics = 5  has Coherence Value of 0.3352\n",
      "Num Topics = 6  has Coherence Value of 0.3521\n",
      "Num Topics = 7  has Coherence Value of 0.3821\n",
      "Num Topics = 8  has Coherence Value of 0.3877\n",
      "Num Topics = 9  has Coherence Value of 0.3908\n",
      "Num Topics = 10  has Coherence Value of 0.4139\n",
      "Num Topics = 11  has Coherence Value of 0.3849\n",
      "Num Topics = 12  has Coherence Value of 0.3932\n",
      "Num Topics = 13  has Coherence Value of 0.3682\n",
      "Num Topics = 14  has Coherence Value of 0.4068\n",
      "Num Topics = 15  has Coherence Value of 0.385\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save information for later\n",
    "dict_models = dict()\n",
    "dict_models['model_list'] = model_list\n",
    "dict_models['coherence_values'] = coherence_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('coherence_and_model_lists_europe_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_models, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Parece que hasta aqui llegue] Seleccionar el mejor modelo de los ya realizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ojo, revisar bien que pertenezca el numerod e topicos\n",
    "ldamallet = model_list_2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Saltar] Building LDA Mallet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo detras de Mallet debiese dar mejores resultados que Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colocar el mejor numero de topicos segun lo obtenido en el punto anterior\n",
    "num_topics = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "ldamallet = LdaMallet(path_to_mallet_binary, corpus=corpus, num_topics=num_topics, id2word=id2word,random_seed=1)\n",
    "\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "#docu : https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/models/wrappers/ldamallet/index.html\n",
    "pprint(ldamallet.show_topics(formatted=False, num_words=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords by topic - colocar numero de topicos a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topicos_actual = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicos = ldamallet.show_topics(formatted=False, num_words=100, num_topics=num_topicos_actual)\n",
    "list_of_df = list()\n",
    "for topic in topicos:\n",
    "    number, words_weight = topic\n",
    "    words_weight_df=pd.DataFrame(words_weight, columns=['term', 'weight'])\n",
    "    list_of_df.append(words_weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(ldamallet.show_topics(formatted=False, num_words=30,num_topics=num_topicos_actual))\n",
    "len(topicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_by_topic= pd.concat(list_of_df,axis=1)\n",
    "keywords_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_by_topic.to_csv(region+'_'+str(today)+'_'+'keywords_weights.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest topic per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].astype(int)\n",
    "\n",
    "#df_dominant_topic.head(20)\n",
    "#10:38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dominant_topic.to_csv(region+'_'+str(today)+'_'+'highest_topic_per_tweet.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Find the most representative tweets for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "numero_de_tweets_por_topico = 50\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic') # esta matriz contiene todos los tweets con el porcentaje de contribucion y max topico. se agrupa y se obtiene lo q queremos\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(numero_de_tweets_por_topico)], \n",
    "                                            axis=0)\n",
    "\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "sent_topics_sorteddf_mallet['Topic_Num'] = sent_topics_sorteddf_mallet['Topic_Num'].astype(int)\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_topics_sorteddf_mallet.to_csv(region+'_'+str(today)+'_'+'most_representative_tweets_per_topic.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (RANDOM) Find the most representative tweets for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#El objetivo es obtener los tweets en orden aleatorio. ¿entregan la misma info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "numero_de_tweets_por_topico = 30\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic') # esta matriz contiene todos los tweets con el porcentaje de contribucion y max topico. se agrupa y se obtiene lo q queremos\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(numero_de_tweets_por_topico)], \n",
    "                                            axis=0)\n",
    "\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "sent_topics_sorteddf_mallet['Topic_Num'] = sent_topics_sorteddf_mallet['Topic_Num'].astype(int)\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle #poneraleatorio30tweets\n",
    "sent_topics_sorteddf_mallet = shuffle(sent_topics_sorteddf_mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.sort_values(by='Topic_Num',inplace=True)\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.groupby('Topic_Num').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_topics_sorteddf_mallet.to_csv(region+'_'+str(today)+'_'+'RANDOM_most_representative_tweets_per_topic.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic distribution across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra el numero de documentos y el porcentaje de documentos en donde ese topico es el principal componente. \n",
    "El código siguiente fue arreglado. El del tutorial tenía errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords.drop_duplicates(subset='Dominant_Topic', keep='first').sort_values(by=['Dominant_Topic']).reset_index(drop=True), topic_counts.sort_index(), topic_contribution.sort_index()], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "df_dominant_topics['Dominant_Topic'] = df_dominant_topics['Dominant_Topic'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#funciona perfecto\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics.to_csv(region+'_'+str(today)+'_'+'topic_distribution_across_documents.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize topics with Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(prueba, corpus,id2word, mds='pcoa', sort_topics=False, R=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis,region+\"_\"+str(today)+\"_visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_json(vis,region+\"_\"+str(today)+\"_visualization.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para cargar json model\n",
    "#esto no funciona, hay que arreglasrlo.\n",
    "\n",
    "import json\n",
    "def load_R_model(filename):\n",
    "    with open(filename, 'r') as j:\n",
    "        data_input = json.load(j)\n",
    "    data = {'topic_term_dists': data_input['phi'], \n",
    "            'doc_topic_dists': data_input['theta'],\n",
    "            'doc_lengths': data_input['doc.length'],\n",
    "            'vocab': data_input['vocab'],\n",
    "            'term_frequency': data_input['term.frequency']}\n",
    "    return data\n",
    "\n",
    "def vis_R_model(filename):\n",
    "    return pyLDAvis.display(pyLDAvis.prepare(**load_R_model(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[df['texto_completo'].str.contains(\"y por primera vez en dos\")]['texto_completo']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.drop_duplicates(inplace=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a['texto_completo'].str.contains(\"Stop peddling lies\")]['texto_completo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the topics-keywords - Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis\n",
    "''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
