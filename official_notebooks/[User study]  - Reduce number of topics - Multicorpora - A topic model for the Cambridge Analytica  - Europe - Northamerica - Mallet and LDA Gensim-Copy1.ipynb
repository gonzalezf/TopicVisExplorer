{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(_prepare)\n",
    "\n",
    "REVISAR CARPETA COSAS NO EN GITHUB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original idea from: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "\n",
    "\n",
    "In this post, we discuss techniques to visualize the output and results from topic model (LDA) based on the gensim package. I will be using a portion of the 20 Newsgroups dataset since the focus is more on approaches to visualizing the results.\n",
    "\n",
    "Letâ€™s begin by importing the packages and the 20 News Groups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='c:\\\\users\\\\gonza\\\\tesisenv\\\\lib\\\\site-packages\\\\thinc\\\\neural\\\\_custom_kernels.cu' mode='r' encoding='utf8'>\n",
      "  SRC = (PWD / \"_custom_kernels.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='c:\\\\users\\\\gonza\\\\tesisenv\\\\lib\\\\site-packages\\\\thinc\\\\neural\\\\_murmur3.cu' mode='r' encoding='utf8'>\n",
      "  MMH_SRC = (PWD / \"_murmur3.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['linkremoved',' <link removed>','usernameremoved','<usernameremoved>','<linkremoved>','usernameremoved_usernameremoved','linkremoved_linkremoved'])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions about how to install mallet are available here: http://mallet.cs.umass.edu/download.php\n",
    "\n",
    "'''\n",
    "Windows installation: After unzipping MALLET, set the environment variable %MALLET_HOME% to point to the MALLET directory.\n",
    "In all command line examples, substitute bin\\mallet for bin/mallet.\n",
    "'''\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "\n",
    "path_to_mallet_binary = \"C:\\\\mallet-2.0.8\\\\bin\\\\mallet\"\n",
    "os.environ.update({'MALLET_HOME':r'C:\\mallet-2.0.8'}) #OJO!, por alguna razon mallet solo puede estar disponible en esa carpeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Cambridge Analytica datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this libraries are for this dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_1 = 'english_europe_tweets_20190411.csv'\n",
    "file_name_2 = 'english_northamerica_tweets_20190411.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cambridge_analytica/regional_datasets/english_europe_tweets_20190411.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ddcef1133457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_1\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/cambridge_analytica/regional_datasets/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_2\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/cambridge_analytica/regional_datasets/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gonza\\tesisenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cambridge_analytica/regional_datasets/english_europe_tweets_20190411.csv'"
     ]
    }
   ],
   "source": [
    "df_1  = pd.read_csv('data/cambridge_analytica/regional_datasets/'+file_name_1)\n",
    "df_2  = pd.read_csv('data/cambridge_analytica/regional_datasets/'+file_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1 = df_1.sample(1000)  #remove this line\n",
    "#df_2 = df_2.sample(1000)  #remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the emails, new line characters, single quotes and finally split the sentence into a list of words using gensimâ€™s simple_preprocess(). Setting the deacc=True option removes punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['meet', 'our', 'newest', 'product', 'the', 'transparent', 'camhatch', 'webcam', 'cover', 'weve', 'combined', 'everything', 'you', 'loved', 'about', 'the', 'matte', 'black', 'version', 'with', 'the', 'transparency', 'of', 'mark', 'zuckerberg', 'protect', 'your', 'privacy', 'without', 'having', 'to', 'change', 'the', 'appearance', 'of', 'your', 'laptop', 'tablet', 'or', 'smartphone', 'link', 'removed']]\n",
      "[['usernameremoved', 'hold', 'facebook', 'accountable', 'for', 'abusing', 'our', 'data', 'and', 'censoring', 'our', 'voice', 'vote', 'no', 'on', 'the', 'cra', 'link', 'removed'], ['both', 'sides', 'should', 'just', 'drop', 'it', 'trump', 'won', 'elections', 'over', 'move', 'on', 'link', 'removed']]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data_1 = df_1.texto_completo.values.tolist()\n",
    "data_words_1 = list(sent_to_words(data_1))\n",
    "print(data_words_1[:1])\n",
    "\n",
    "data_2 = df_2.texto_completo.values.tolist()\n",
    "data_words_2 = list(sent_to_words(data_2))\n",
    "print(data_words_2[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Bigram, Trigram Models and Lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n",
    "\n",
    "Next, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
    "\n",
    "We keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, I use spacy for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you haven't installed yet\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram_1 = gensim.models.Phrases(data_words_1, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram_1 = gensim.models.Phrases(bigram_1[data_words_1], threshold=100)  \n",
    "bigram_mod_1 = gensim.models.phrases.Phraser(bigram_1)\n",
    "trigram_mod_1 = gensim.models.phrases.Phraser(trigram_1)\n",
    "\n",
    "bigram_2 = gensim.models.Phrases(data_words_2, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram_2 = gensim.models.Phrases(bigram_2[data_words_2], threshold=100)  \n",
    "bigram_mod_2 = gensim.models.phrases.Phraser(bigram_2)\n",
    "trigram_mod_2 = gensim.models.phrases.Phraser(trigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "\"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "def process_words(texts, bigram_mod, trigram_mod, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):    \n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []    \n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready_1 = process_words(data_words_1, bigram_mod_1, trigram_mod_1)  # processed Text Data!\n",
    "data_ready_2 = process_words(data_words_2, bigram_mod_2, trigram_mod_2)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the LDA topic model using LdaModel(), you need the corpus and the dictionary. Letâ€™s create them first and then build the model. The trained topics (keywords and weights) are printed below as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic modeling europe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with genim and mallet\n",
    "number_topics_1 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_1 = corpora.Dictionary(data_ready_1)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus_1 = [id2word_1.doc2bow(text) for text in data_ready_1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py:285: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\gonza\\\\AppData\\\\Local\\\\Temp\\\\a4df11_state.mallet.gz'>\n",
      "  self.word_topics = self.load_word_topics()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "lda_model_1 = LdaMallet(path_to_mallet_binary, corpus=corpus_1, num_topics=number_topics_1, id2word=id2word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lda mallet model to lda gensim model\n",
    "lda_model_1 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.318*\"remove\" + 0.288*\"link\" + 0.033*\"late\" + 0.023*\"business\" + '\n",
      "  '0.020*\"marketing\" + 0.011*\"learn\" + 0.009*\"daily\" + 0.009*\"advertising\" + '\n",
      "  '0.008*\"customer\" + 0.007*\"tool\"'),\n",
      " (1,\n",
      "  '0.143*\"remove\" + 0.127*\"link\" + 0.093*\"facebook\" + 0.038*\"time\" + '\n",
      "  '0.024*\"news\" + 0.024*\"ad\" + 0.016*\"platform\" + 0.015*\"story\" + 0.013*\"real\" '\n",
      "  '+ 0.012*\"show\"'),\n",
      " (2,\n",
      "  '0.094*\"privacy\" + 0.049*\"facebook\" + 0.024*\"change\" + 0.023*\"scandal\" + '\n",
      "  '0.020*\"law\" + 0.019*\"data\" + 0.014*\"issue\" + 0.012*\"problem\" + '\n",
      "  '0.011*\"policy\" + 0.011*\"user\"'),\n",
      " (3,\n",
      "  '0.190*\"datum\" + 0.051*\"user\" + 0.030*\"share\" + 0.028*\"company\" + '\n",
      "  '0.027*\"give\" + 0.022*\"facebook\" + 0.020*\"personal\" + 0.018*\"information\" + '\n",
      "  '0.017*\"data\" + 0.016*\"app\"'),\n",
      " (4,\n",
      "  '0.371*\"remove\" + 0.356*\"link\" + 0.029*\"read\" + 0.017*\"interesting\" + '\n",
      "  '0.012*\"article\" + 0.011*\"important\" + 0.009*\"research\" + 0.006*\"thread\" + '\n",
      "  '0.005*\"piece\" + 0.005*\"testify\"'),\n",
      " (5,\n",
      "  '0.140*\"bigdata\" + 0.045*\"ai\" + 0.038*\"removed\" + 0.026*\"tech\" + '\n",
      "  '0.025*\"technology\" + 0.025*\"big\" + 0.024*\"datascience\" + 0.022*\"analytic\" + '\n",
      "  '0.019*\"machinelearne\" + 0.015*\"cybersecurity\"'),\n",
      " (6,\n",
      "  '0.039*\"amp\" + 0.032*\"question\" + 0.018*\"leave\" + 0.016*\"campaign\" + '\n",
      "  '0.015*\"election\" + 0.014*\"answer\" + 0.013*\"vote\" + 0.013*\"lie\" + '\n",
      "  '0.012*\"tory\" + 0.012*\"government\"'),\n",
      " (7,\n",
      "  '0.130*\"remove\" + 0.127*\"link\" + 0.038*\"good\" + 0.034*\"make\" + 0.025*\"work\" '\n",
      "  '+ 0.024*\"great\" + 0.021*\"year\" + 0.021*\"world\" + 0.015*\"follow\" + '\n",
      "  '0.012*\"create\"'),\n",
      " (8,\n",
      "  '0.315*\"remove\" + 0.292*\"link\" + 0.015*\"face\" + 0.014*\"scandal\" + '\n",
      "  '0.013*\"open\" + 0.010*\"zuckerberg\" + 0.010*\"report\" + 0.010*\"plan\" + '\n",
      "  '0.008*\"close\" + 0.007*\"mark\"'),\n",
      " (9,\n",
      "  '0.029*\"page\" + 0.029*\"facebook\" + 0.022*\"day\" + 0.021*\"today\" + '\n",
      "  '0.020*\"live\" + 0.020*\"watch\" + 0.019*\"post\" + 0.018*\"twitter\" + '\n",
      "  '0.018*\"start\" + 0.017*\"friend\"'),\n",
      " (10,\n",
      "  '0.064*\"people\" + 0.032*\"trump\" + 0.028*\"delete\" + 0.023*\"thing\" + '\n",
      "  '0.021*\"account\" + 0.019*\"make\" + 0.014*\"call\" + 0.014*\"stop\" + 0.013*\"bad\" '\n",
      "  '+ 0.011*\"money\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_1.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic modeling north america dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with genim and mallet\n",
    "number_topics_2 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_2 = corpora.Dictionary(data_ready_2)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus_2 = [id2word_2.doc2bow(text) for text in data_ready_2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gonza\\tesisenv\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py:285: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\gonza\\\\AppData\\\\Local\\\\Temp\\\\806975_state.mallet.gz'>\n",
      "  self.word_topics = self.load_word_topics()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "lda_model_2 = LdaMallet(path_to_mallet_binary, corpus=corpus_2, num_topics=number_topics_2, id2word=id2word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lda mallet model to lda gensim model\n",
    "lda_model_2 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.154*\"datum\" + 0.062*\"user\" + 0.051*\"facebook\" + 0.035*\"company\" + '\n",
      "  '0.029*\"share\" + 0.025*\"data\" + 0.025*\"give\" + 0.023*\"information\" + '\n",
      "  '0.023*\"sell\" + 0.021*\"personal\"'),\n",
      " (1,\n",
      "  '0.110*\"trump\" + 0.022*\"campaign\" + 0.022*\"election\" + 0.015*\"vote\" + '\n",
      "  '0.013*\"work\" + 0.012*\"retweete\" + 0.011*\"country\" + 0.008*\"russian\" + '\n",
      "  '0.008*\"win\" + 0.008*\"child\"'),\n",
      " (2,\n",
      "  '0.217*\"remove\" + 0.193*\"link\" + 0.032*\"today\" + 0.029*\"watch\" + '\n",
      "  '0.026*\"live\" + 0.017*\"check\" + 0.016*\"video\" + 0.015*\"week\" + 0.014*\"story\" '\n",
      "  '+ 0.011*\"testify\"'),\n",
      " (3,\n",
      "  '0.435*\"remove\" + 0.401*\"link\" + 0.007*\"socialmedia\" + 0.006*\"mining\" + '\n",
      "  '0.004*\"network\" + 0.003*\"rise\" + 0.003*\"trading\" + 0.003*\"alt\" + '\n",
      "  '0.003*\"signal\" + 0.003*\"decentralize\"'),\n",
      " (4,\n",
      "  '0.038*\"good\" + 0.036*\"question\" + 0.031*\"thing\" + 0.022*\"time\" + '\n",
      "  '0.020*\"hear\" + 0.019*\"work\" + 0.018*\"bad\" + 0.017*\"zuckerberg\" + '\n",
      "  '0.013*\"make\" + 0.013*\"hearing\"'),\n",
      " (5,\n",
      "  '0.401*\"remove\" + 0.376*\"link\" + 0.023*\"read\" + 0.012*\"job\" + 0.010*\"great\" '\n",
      "  '+ 0.009*\"article\" + 0.008*\"interesting\" + 0.008*\"hire\" + 0.008*\"report\" + '\n",
      "  '0.007*\"step\"'),\n",
      " (6,\n",
      "  '0.125*\"remove\" + 0.110*\"link\" + 0.078*\"privacy\" + 0.021*\"change\" + '\n",
      "  '0.018*\"scandal\" + 0.016*\"law\" + 0.014*\"issue\" + 0.014*\"policy\" + '\n",
      "  '0.012*\"problem\" + 0.012*\"government\"'),\n",
      " (7,\n",
      "  '0.058*\"amp\" + 0.027*\"news\" + 0.024*\"call\" + 0.024*\"people\" + 0.019*\"lie\" + '\n",
      "  '0.019*\"trump\" + 0.017*\"stop\" + 0.014*\"fake\" + 0.013*\"conservative\" + '\n",
      "  '0.013*\"leave\"'),\n",
      " (8,\n",
      "  '0.080*\"bigdata\" + 0.028*\"removed\" + 0.028*\"big\" + 0.024*\"ai\" + '\n",
      "  '0.022*\"business\" + 0.018*\"tech\" + 0.017*\"late\" + 0.016*\"datascience\" + '\n",
      "  '0.015*\"analytic\" + 0.014*\"learn\"'),\n",
      " (9,\n",
      "  '0.099*\"facebook\" + 0.035*\"delete\" + 0.026*\"account\" + 0.026*\"post\" + '\n",
      "  '0.025*\"friend\" + 0.025*\"day\" + 0.023*\"time\" + 0.022*\"page\" + '\n",
      "  '0.021*\"twitter\" + 0.021*\"find\"'),\n",
      " (10,\n",
      "  '0.053*\"make\" + 0.050*\"people\" + 0.033*\"facebook\" + 0.030*\"ad\" + 0.022*\"pay\" '\n",
      "  '+ 0.018*\"money\" + 0.018*\"platform\" + 0.014*\"free\" + 0.013*\"run\" + '\n",
      "  '0.012*\"give\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_2.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most relevant documents - Europe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should I use text or text_completo column? in the first, username and link are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSee the discusion here:\\nhttps://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\\nhttps://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "See the discusion here:\n",
    "https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396\n",
    "https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_1, _ = lda_model_1.inference(corpus_1)\n",
    "matrix_documents_topic_contribution_1 /= matrix_documents_topic_contribution_1.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.087647</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.099345</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.087511</td>\n",
       "      <td>0.091275</td>\n",
       "      <td>0.080369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080736</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>0.073328</td>\n",
       "      <td>0.100784</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>0.170088</td>\n",
       "      <td>0.076284</td>\n",
       "      <td>0.080745</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>0.089435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095084</td>\n",
       "      <td>0.089647</td>\n",
       "      <td>0.104423</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.089472</td>\n",
       "      <td>0.095112</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.077576</td>\n",
       "      <td>0.107303</td>\n",
       "      <td>0.107298</td>\n",
       "      <td>0.083477</td>\n",
       "      <td>0.084574</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.094552</td>\n",
       "      <td>0.100313</td>\n",
       "      <td>0.074516</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093338</td>\n",
       "      <td>0.087986</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.087815</td>\n",
       "      <td>0.093346</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.103457  0.068377  0.153100  0.080369  0.087647  0.080369  0.099345   \n",
       "1  0.080736  0.076415  0.073314  0.073328  0.100784  0.073314  0.170088   \n",
       "2  0.095084  0.089647  0.104423  0.085971  0.097237  0.085763  0.085763   \n",
       "3  0.088574  0.077576  0.107303  0.107298  0.083477  0.084574  0.090909   \n",
       "4  0.093338  0.087986  0.084175  0.084175  0.095428  0.084175  0.121212   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.068182  0.087511  0.091275  0.080369  \n",
       "1  0.076284  0.080745  0.105558  0.089435  \n",
       "2  0.089472  0.095112  0.085763  0.085763  \n",
       "3  0.094552  0.100313  0.074516  0.090909  \n",
       "4  0.087815  0.093346  0.084175  0.084175  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_1 = pd.DataFrame(matrix_documents_topic_contribution_1)\n",
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_1 = pd.Series(df_1['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_1 = pd.concat([matrix_documents_topic_contribution_1, contents_1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.087647</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.099345</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.087511</td>\n",
       "      <td>0.091275</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>Meet our newest product: the transparent CamHa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080736</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>0.073328</td>\n",
       "      <td>0.100784</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>0.170088</td>\n",
       "      <td>0.076284</td>\n",
       "      <td>0.080745</td>\n",
       "      <td>0.105558</td>\n",
       "      <td>0.089435</td>\n",
       "      <td>@SupportOurLefty This week has been the most #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095084</td>\n",
       "      <td>0.089647</td>\n",
       "      <td>0.104423</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.089472</td>\n",
       "      <td>0.095112</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>Pulitzer prize for this lady. Investigative jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.077576</td>\n",
       "      <td>0.107303</td>\n",
       "      <td>0.107298</td>\n",
       "      <td>0.083477</td>\n",
       "      <td>0.084574</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.094552</td>\n",
       "      <td>0.100313</td>\n",
       "      <td>0.074516</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>Theres no more secrecy, confident and privacy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093338</td>\n",
       "      <td>0.087986</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.087815</td>\n",
       "      <td>0.093346</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>AggregateIQ: the obscure Canadian tech firm an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.103457  0.068377  0.153100  0.080369  0.087647  0.080369  0.099345   \n",
       "1  0.080736  0.076415  0.073314  0.073328  0.100784  0.073314  0.170088   \n",
       "2  0.095084  0.089647  0.104423  0.085971  0.097237  0.085763  0.085763   \n",
       "3  0.088574  0.077576  0.107303  0.107298  0.083477  0.084574  0.090909   \n",
       "4  0.093338  0.087986  0.084175  0.084175  0.095428  0.084175  0.121212   \n",
       "\n",
       "          7         8         9        10  \\\n",
       "0  0.068182  0.087511  0.091275  0.080369   \n",
       "1  0.076284  0.080745  0.105558  0.089435   \n",
       "2  0.089472  0.095112  0.085763  0.085763   \n",
       "3  0.094552  0.100313  0.074516  0.090909   \n",
       "4  0.087815  0.093346  0.084175  0.084175   \n",
       "\n",
       "                                                text  \n",
       "0  Meet our newest product: the transparent CamHa...  \n",
       "1  @SupportOurLefty This week has been the most #...  \n",
       "2  Pulitzer prize for this lady. Investigative jo...  \n",
       "3  Theres no more secrecy, confident and privacy ...  \n",
       "4  AggregateIQ: the obscure Canadian tech firm an...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most relevant documents - North america dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code we get the full matrix of topic-documents contribution\n",
    "matrix_documents_topic_contribution_2, _ = lda_model_2.inference(corpus_2)\n",
    "matrix_documents_topic_contribution_2 /= matrix_documents_topic_contribution_2.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.142397</td>\n",
       "      <td>0.081470</td>\n",
       "      <td>0.088499</td>\n",
       "      <td>0.075770</td>\n",
       "      <td>0.087492</td>\n",
       "      <td>0.078918</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.117736</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.092003</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.101053</td>\n",
       "      <td>0.094386</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.086211</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.088458</td>\n",
       "      <td>0.076688</td>\n",
       "      <td>0.087431</td>\n",
       "      <td>0.081405</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>0.103596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084184</td>\n",
       "      <td>0.139712</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.142397</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.088497</td>\n",
       "      <td>0.075770</td>\n",
       "      <td>0.087494</td>\n",
       "      <td>0.078917</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.109091  0.142397  0.081470  0.088499  0.075770  0.087492  0.078918   \n",
       "1  0.079748  0.117736  0.085194  0.092003  0.079745  0.110900  0.101053   \n",
       "2  0.109091  0.086211  0.081469  0.088458  0.076688  0.087431  0.081405   \n",
       "3  0.084175  0.084175  0.084184  0.139712  0.084175  0.084175  0.084175   \n",
       "4  0.109091  0.142397  0.081469  0.088497  0.075770  0.087494  0.078917   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.109070  0.075758  0.075758  0.075779  \n",
       "1  0.094386  0.079745  0.079745  0.079745  \n",
       "2  0.079692  0.075758  0.130202  0.103596  \n",
       "3  0.084175  0.102703  0.084175  0.084175  \n",
       "4  0.109070  0.075758  0.075758  0.075779  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.DataFrame(matrix_documents_topic_contribution_2)\n",
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_2 = pd.Series(df_2['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_documents_topic_contribution_2 = pd.concat([matrix_documents_topic_contribution_2, contents_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.142397</td>\n",
       "      <td>0.081470</td>\n",
       "      <td>0.088499</td>\n",
       "      <td>0.075770</td>\n",
       "      <td>0.087492</td>\n",
       "      <td>0.078918</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>@SenTedCruz Hold Facebook accountable for abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.117736</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.092003</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.101053</td>\n",
       "      <td>0.094386</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>Both sides should just drop it. Trump won...el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.086211</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.088458</td>\n",
       "      <td>0.076688</td>\n",
       "      <td>0.087431</td>\n",
       "      <td>0.081405</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>0.103596</td>\n",
       "      <td>$AAPL lobbing threes while they can. Wasnt it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084184</td>\n",
       "      <td>0.139712</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.084175</td>\n",
       "      <td>15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.142397</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.088497</td>\n",
       "      <td>0.075770</td>\n",
       "      <td>0.087494</td>\n",
       "      <td>0.078917</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>@SenJohnThune Hold Facebook accountable for ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.109091  0.142397  0.081470  0.088499  0.075770  0.087492  0.078918   \n",
       "1  0.079748  0.117736  0.085194  0.092003  0.079745  0.110900  0.101053   \n",
       "2  0.109091  0.086211  0.081469  0.088458  0.076688  0.087431  0.081405   \n",
       "3  0.084175  0.084175  0.084184  0.139712  0.084175  0.084175  0.084175   \n",
       "4  0.109091  0.142397  0.081469  0.088497  0.075770  0.087494  0.078917   \n",
       "\n",
       "          7         8         9        10  \\\n",
       "0  0.109070  0.075758  0.075758  0.075779   \n",
       "1  0.094386  0.079745  0.079745  0.079745   \n",
       "2  0.079692  0.075758  0.130202  0.103596   \n",
       "3  0.084175  0.102703  0.084175  0.084175   \n",
       "4  0.109070  0.075758  0.075758  0.075779   \n",
       "\n",
       "                                                text  \n",
       "0  @SenTedCruz Hold Facebook accountable for abus...  \n",
       "1  Both sides should just drop it. Trump won...el...  \n",
       "2  $AAPL lobbing threes while they can. Wasnt it ...  \n",
       "3  15 min #RSI Signals:\\n\\n$BTC - $SLR: 6.84\\n$BT...  \n",
       "4  @SenJohnThune Hold Facebook accountable for ab...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_documents_topic_contribution_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = 'data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n",
      "Calculating for different omegas\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora = vis.calculate_topic_similarity_on_multi_corpora(word_embedding_model, lda_model_1,lda_model_2, corpus_1,corpus_2, id2word_1,id2word_2, matrix_documents_topic_contribution_1,matrix_documents_topic_contribution_2, topn_terms, topk_documents, relevance_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show visualization - Multi corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n"
     ]
    }
   ],
   "source": [
    "#import topicvisexplorer\n",
    "#import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_multi_corpora( lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2,  matrix_documents_topic_contribution_1, matrix_documents_topic_contribution_2, topic_similarity_matrix_multicorpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi corpora data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "#save data\n",
    "vis.save_multi_corpora_data(\"models_output/multi_corpora_data_europe_northamerica_ca_lda_mallet_gensim.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.load_multi_corpora_data(\"multi_corpora_data_europe_northamerica_ca_lda_mallet_gensim.pkl\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vis.run()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic similarity metric baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the # top keywords and # top documents a considerar en la metrica\n",
    "\n",
    "topn_terms = 20\n",
    "topk_documents = 20\n",
    "relevance_lambda = 0.6 \n",
    "\n",
    "ruta_word_embedding = 'data/embedding_english_europe_northamerica_word2vec_300dimensions_cbow_trim3_epoch50.model'\n",
    "word_embedding_model = gensim.models.Word2Vec.load(ruta_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n"
     ]
    }
   ],
   "source": [
    "import topicvisexplorer\n",
    "import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "warnings.filterwarnings('ignore')\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "topic_similarity_matrix_multicorpora_metric_baseline = vis.calculate_topic_similarity_on_multi_corpora_metric_baseline(word_embedding_model,  lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2, relevance_lambda, topn_terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Prepared Data script - sort_topics False\n",
      "Using Prepared Data script - sort_topics False\n"
     ]
    }
   ],
   "source": [
    "#import topicvisexplorer\n",
    "#import importlib\n",
    "importlib.reload(topicvisexplorer)\n",
    "\n",
    "vis = topicvisexplorer.TopicVisExplorer(\"borrar_nombre\")\n",
    "vis.prepare_multi_corpora( lda_model_1,lda_model_2, corpus_1, corpus_2, id2word_1,id2word_2,  matrix_documents_topic_contribution_1, matrix_documents_topic_contribution_2, topic_similarity_matrix_multicorpora_metric_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi corpora data saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "#save data\n",
    "vis.save_multi_corpora_data(\"models_output/multi_corpora_data_europe_northamerica_ca_lda_mallet_gensim_topic_similarity_baseline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
