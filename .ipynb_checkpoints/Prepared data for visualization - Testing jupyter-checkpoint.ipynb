{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.keyedvectors import KeyedVector\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from past.builtins import basestring\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pyLDAvis Gensim\n",
    "===============\n",
    "Helper functions to visualize LDA models trained by Gensim\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "import funcy as fp\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "\n",
    "def _extract_data(topic_model, corpus, dictionary, doc_topic_dists=None):\n",
    "    import gensim\n",
    "\n",
    "    if not gensim.matutils.ismatrix(corpus):\n",
    "        corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))\n",
    "    else:\n",
    "        corpus_csc = corpus\n",
    "        # Need corpus to be a streaming gensim list corpus for len and inference functions below:\n",
    "        corpus = gensim.matutils.Sparse2Corpus(corpus_csc)\n",
    "\n",
    "    vocab = list(dictionary.token2id.keys())\n",
    "    # TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\n",
    "    # for now, I'll just make sure we don't ever get zeros...\n",
    "    beta = 0.01\n",
    "    fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)\n",
    "    term_freqs = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]\n",
    "    term_freqs[term_freqs == 0] = beta\n",
    "    doc_lengths = corpus_csc.sum(axis=0).A.ravel()\n",
    "\n",
    "    assert term_freqs.shape[0] == len(dictionary),\\\n",
    "        'Term frequencies and dictionary have different shape {} != {}'.format(\n",
    "        term_freqs.shape[0], len(dictionary))\n",
    "    assert doc_lengths.shape[0] == len(corpus),\\\n",
    "        'Document lengths and corpus have different sizes {} != {}'.format(\n",
    "        doc_lengths.shape[0], len(corpus))\n",
    "\n",
    "    if hasattr(topic_model, 'lda_alpha'):\n",
    "        num_topics = len(topic_model.lda_alpha)\n",
    "    else:\n",
    "        num_topics = topic_model.num_topics\n",
    "\n",
    "    if doc_topic_dists is None:\n",
    "        # If its an HDP model.\n",
    "        if hasattr(topic_model, 'lda_beta'):\n",
    "            gamma = topic_model.inference(corpus)\n",
    "        else:\n",
    "            gamma, _ = topic_model.inference(corpus)\n",
    "        doc_topic_dists = gamma / gamma.sum(axis=1)[:, None]\n",
    "    else:\n",
    "        if isinstance(doc_topic_dists, list):\n",
    "            doc_topic_dists = gensim.matutils.corpus2dense(doc_topic_dists, num_topics).T\n",
    "        elif issparse(doc_topic_dists):\n",
    "            doc_topic_dists = doc_topic_dists.T.todense()\n",
    "        doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)\n",
    "\n",
    "    assert doc_topic_dists.shape[1] == num_topics,\\\n",
    "        'Document topics and number of topics do not match {} != {}'.format(\n",
    "        doc_topic_dists.shape[1], num_topics)\n",
    "\n",
    "    # get the topic-term distribution straight from gensim without\n",
    "    # iterating over tuples\n",
    "    if hasattr(topic_model, 'lda_beta'):\n",
    "        topic = topic_model.lda_beta\n",
    "    else:\n",
    "        topic = topic_model.state.get_lambda()\n",
    "    topic = topic / topic.sum(axis=1)[:, None]\n",
    "    topic_term_dists = topic[:, fnames_argsort]\n",
    "\n",
    "    assert topic_term_dists.shape[0] == doc_topic_dists.shape[1]\n",
    "\n",
    "    return {'topic_term_dists': topic_term_dists, 'doc_topic_dists': doc_topic_dists,\n",
    "            'doc_lengths': doc_lengths, 'vocab': vocab, 'term_frequency': term_freqs}\n",
    "\n",
    "\n",
    "def prepare(topic_model, corpus, dictionary, doc_topic_dist=None, **kwargs):\n",
    "    \"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\n",
    "    the data structures needed for the visualization.\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_model : gensim.models.ldamodel.LdaModel\n",
    "        An already trained Gensim LdaModel. The other gensim model types are\n",
    "        not supported (PRs welcome).\n",
    "    corpus : array-like list of bag of word docs in tuple form or scipy CSC matrix\n",
    "        The corpus in bag of word form, the same docs used to train the model.\n",
    "        The corpus is transformed into a csc matrix internally, if you intend to\n",
    "        call prepare multiple times it is a good idea to first call\n",
    "        `gensim.matutils.corpus2csc(corpus)` and pass in the csc matrix instead.\n",
    "    For example: [(50, 3), (63, 5), ....]\n",
    "    dictionary: gensim.corpora.Dictionary\n",
    "        The dictionary object used to create the corpus. Needed to extract the\n",
    "        actual terms (not ids).\n",
    "    doc_topic_dist (optional): Document topic distribution from LDA (default=None)\n",
    "        The document topic distribution that is eventually visualised, if you will\n",
    "        be calling `prepare` multiple times it's a good idea to explicitly pass in\n",
    "        `doc_topic_dist` as inferring this for large corpora can be quite\n",
    "        expensive.\n",
    "    **kwargs :\n",
    "        additional keyword arguments are passed through to :func:`pyldavis.prepare`.\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "        the data structures used in the visualization\n",
    "    Example\n",
    "    --------\n",
    "    For example usage please see this notebook:\n",
    "    http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
    "    See\n",
    "    ------\n",
    "    See `pyLDAvis.prepare` for **kwargs.\n",
    "    \"\"\"\n",
    "    opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n",
    "    return vis_prepare(**opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from past.builtins import basestring\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import logging\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "#from .utils import NumPyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreparedData(namedtuple('PreparedData', ['topic_coordinates', 'topic_info', 'token_table',\n",
    "                                               'R', 'lambda_step', 'plot_opts', 'topic_order'])):\n",
    "\n",
    "    def sorted_terms(self, topic=1, _lambda=1):\n",
    "        \"\"\"Retuns a dataframe using _lambda to calculate term relevance of a given topic.\"\"\"\n",
    "        tdf = pd.DataFrame(self.topic_info[self.topic_info.Category == 'Topic' + str(topic)])\n",
    "        if _lambda < 0 or _lambda > 1:\n",
    "            _lambda = 1\n",
    "        stdf = tdf.assign(relevance=_lambda * tdf['logprob'] + (1 - _lambda) * tdf['loglift'])\n",
    "        return stdf.sort_values('relevance', ascending=False)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {'mdsDat': self.topic_coordinates.to_dict(orient='list'),\n",
    "                'tinfo': self.topic_info.to_dict(orient='list'),\n",
    "                'token.table': self.token_table.to_dict(orient='list'),\n",
    "                'R': self.R,\n",
    "                'lambda.step': self.lambda_step,\n",
    "                'plot.opts': self.plot_opts,\n",
    "                'topic.order': self.topic_order}\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), cls=NumPyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pyLDAvis Prepare\n",
    "===============\n",
    "Main transformation functions for preparing LDAdata to the visualization's data structures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import MDS, TSNE\n",
    "    sklearn_present = True\n",
    "except ImportError:\n",
    "    sklearn_present = False\n",
    "\n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "\n",
    "    def err(msg):\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err_msg = ('Number of rows of topic_term_dists does not match number of columns of '\n",
    "                   'doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "        err(err_msg)\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err_msg = ('Length of doc_lengths not equal to the number of rows in doc_topic_dists;'\n",
    "                   'both should be equal to the number of documents in the data.')\n",
    "        err(err_msg)\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err_msg = ('Number of terms in vocabulary does not match the number of columns of '\n",
    "                   'topic_term_dists (where each row of topic_term_dists is a probability '\n",
    "                   'distribution of terms for a given topic)')\n",
    "        err(err_msg)\n",
    "    if len(term_frequency) != W:\n",
    "        err_msg = ('Length of term_frequency not equal to the number of terms in the '\n",
    "                   'number of terms in the vocabulary (len of vocab)')\n",
    "        err(err_msg)\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "\n",
    "\n",
    "def _jensen_shannon(_P, _Q):\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "\n",
    "def _pcoa(pair_dists, n_components=2):\n",
    "    \"\"\"Principal Coordinate Analysis,\n",
    "    aka Classical Multidimensional Scaling\n",
    "    \"\"\"\n",
    "    # code referenced from skbio.stats.ordination.pcoa\n",
    "    # https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py\n",
    "\n",
    "    # pairwise distance matrix is assumed symmetric\n",
    "    pair_dists = np.asarray(pair_dists, np.float64)\n",
    "\n",
    "    # perform SVD on double centred distance matrix\n",
    "    n = pair_dists.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = - H.dot(pair_dists ** 2).dot(H) / 2\n",
    "    eigvals, eigvecs = np.linalg.eig(B)\n",
    "\n",
    "    # Take first n_components of eigenvalues and eigenvectors\n",
    "    # sorted in decreasing order\n",
    "    ix = eigvals.argsort()[::-1][:n_components]\n",
    "    eigvals = eigvals[ix]\n",
    "    eigvecs = eigvecs[:, ix]\n",
    "\n",
    "    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes\n",
    "    # at least 1 eigenvalue must be zero\n",
    "    eigvals[np.isclose(eigvals, 0)] = 0\n",
    "    if np.any(eigvals < 0):\n",
    "        ix_neg = eigvals < 0\n",
    "        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)\n",
    "        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)\n",
    "\n",
    "    return np.sqrt(eigvals) * eigvecs\n",
    "\n",
    "\n",
    "def js_PCoA(distributions):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Principal Coordinate Analysis\n",
    "    (aka Classical Multidimensional Scaling)\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "    Returns\n",
    "    -------\n",
    "    pcoa : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    return _pcoa(dist_matrix)\n",
    "\n",
    "\n",
    "def js_MMDS(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & Metric Multidimensional Scaling\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`\n",
    "    Returns\n",
    "    -------\n",
    "    mmds : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def js_TSNE(distributions, **kwargs):\n",
    "    \"\"\"Dimension reduction via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`\n",
    "    Returns\n",
    "    -------\n",
    "    tsne : array, shape (`n_dists`, 2)\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "\n",
    "\n",
    "def _topic_coordinates(mds, topic_term_dists, topic_proportion):\n",
    "    K = topic_term_dists.shape[0]\n",
    "    mds_res = mds(topic_term_dists)\n",
    "    assert mds_res.shape == (K, 2)\n",
    "    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1),\n",
    "                          'cluster': 1, 'Freq': topic_proportion * 100})\n",
    "    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26\n",
    "    return mds_df\n",
    "\n",
    "\n",
    "def _chunks(l, n):\n",
    "    \"\"\" Yield successive n-sized chunks from l.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def _job_chunks(l, n_jobs):\n",
    "    n_chunks = n_jobs\n",
    "    if n_jobs < 0:\n",
    "        # so, have n chunks if we are using all n cores/cpus\n",
    "        n_chunks = cpu_count() + 1 - n_jobs\n",
    "\n",
    "    return _chunks(l, n_chunks)\n",
    "\n",
    "\n",
    "def _find_relevance(log_ttd, log_lift, R, lambda_):\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)\n",
    "\n",
    "\n",
    "def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):\n",
    "    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])\n",
    "\n",
    "\n",
    "def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n",
    "                vocab, lambda_step, R, n_jobs):\n",
    "    # marginal distribution over terms (width of blue bars)\n",
    "    term_proportion = term_frequency / term_frequency.sum()\n",
    "\n",
    "    # compute the distinctiveness and saliency of the terms:\n",
    "    # this determines the R terms that are displayed when no topic is selected\n",
    "    topic_given_term = topic_term_dists / topic_term_dists.sum()\n",
    "    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
    "    distinctiveness = kernel.sum()\n",
    "    saliency = term_proportion * distinctiveness\n",
    "    # Order the terms for the \"default\" view by decreasing saliency:\n",
    "    default_term_info = pd.DataFrame({\n",
    "        'saliency': saliency,\n",
    "        'Term': vocab,\n",
    "        'Freq': term_frequency,\n",
    "        'Total': term_frequency,\n",
    "        'Category': 'Default'})\n",
    "    default_term_info = default_term_info.sort_values(\n",
    "        by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
    "    # Rounding Freq and Total to integer values to match LDAvis code:\n",
    "    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n",
    "    default_term_info['Total'] = np.floor(default_term_info['Total'])\n",
    "    ranks = np.arange(R, 0, -1)\n",
    "    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n",
    "\n",
    "    # compute relevance and top terms for each topic\n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n",
    "\n",
    "    def topic_top_term_df(tup):\n",
    "        new_topic_id, (original_topic_id, topic_terms) = tup\n",
    "        term_ix = topic_terms.unique()\n",
    "        return pd.DataFrame({'Term': vocab[term_ix],\n",
    "                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n",
    "                             'Total': term_frequency[term_ix],\n",
    "                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n",
    "                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n",
    "                             'Category': 'Topic%d' % new_topic_id})\n",
    "\n",
    "    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n",
    "                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n",
    "                          for ls in _job_chunks(lambda_seq, n_jobs)))\n",
    "    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n",
    "    return pd.concat([default_term_info] + list(topic_dfs), sort=True)\n",
    "\n",
    "\n",
    "def _token_table(topic_info, term_topic_freq, vocab, term_frequency):\n",
    "    # last, to compute the areas of the circles when a term is highlighted\n",
    "    # we must gather all unique terms that could show up (for every combination\n",
    "    # of topic and value of lambda) and compute its distribution over topics.\n",
    "\n",
    "    # term-topic frequency table of unique terms across all topics and all values of lambda\n",
    "    term_ix = topic_info.index.unique()\n",
    "    term_ix = np.sort(term_ix)\n",
    "\n",
    "    top_topic_terms_freq = term_topic_freq[term_ix]\n",
    "    # use the new ordering for the topics\n",
    "    K = len(term_topic_freq)\n",
    "    top_topic_terms_freq.index = range(1, K + 1)\n",
    "    top_topic_terms_freq.index.name = 'Topic'\n",
    "\n",
    "    # we filter to Freq >= 0.5 to avoid sending too much data to the browser\n",
    "    token_table = pd.DataFrame({'Freq': top_topic_terms_freq.unstack()})\\\n",
    "        .reset_index().set_index('term').query('Freq >= 0.5')\n",
    "\n",
    "    token_table['Freq'] = token_table['Freq'].round()\n",
    "    token_table['Term'] = vocab[token_table.index.values].values\n",
    "    # Normalize token frequencies:\n",
    "    token_table['Freq'] = token_table.Freq / term_frequency[token_table.index]\n",
    "    return token_table.sort_values(by=['Term', 'Topic'])\n",
    "\n",
    "\n",
    "def prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,\n",
    "            R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1,\n",
    "            plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=True):\n",
    "    \"\"\"Transforms the topic model distributions and related corpus data into\n",
    "    the data structures needed for the visualization.\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "        Matrix of topic-term probabilities. Where `n_terms` is `len(vocab)`.\n",
    "    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape `n_docs`\n",
    "        The length of each document, i.e. the number of words in each document.\n",
    "        The order of the numbers should be consistent with the ordering of the\n",
    "        docs in `doc_topic_dists`.\n",
    "    vocab : array-like, shape `n_terms`\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape `n_terms`\n",
    "        The count of each particular term over the entire corpus. The ordering\n",
    "        of these counts should correspond with `vocab` and `topic_term_dists`.\n",
    "    R : int\n",
    "        The number of terms to display in the barcharts of the visualization.\n",
    "        Default is 30. Recommended to be roughly between 10 and 50.\n",
    "    lambda_step : float, between 0 and 1\n",
    "        Determines the interstep distance in the grid of lambda values over\n",
    "        which to iterate when computing relevance.\n",
    "        Default is 0.01. Recommended to be between 0.01 and 0.1.\n",
    "    mds : function or a string representation of function\n",
    "        A function that takes `topic_term_dists` as an input and outputs a\n",
    "        `n_topics` by `2`  distance matrix. The output approximates the distance\n",
    "        between topics. See :func:`js_PCoA` for details on the default function.\n",
    "        A string representation currently accepts `pcoa` (or upper case variant),\n",
    "        `mmds` (or upper case variant) and `tsne` (or upper case variant),\n",
    "        if `sklearn` package is installed for the latter two.\n",
    "    n_jobs : int\n",
    "        The number of cores to be used to do the computations. The regular\n",
    "        joblib conventions are followed so `-1`, which is the default, will\n",
    "        use all cores.\n",
    "    plot_opts : dict, with keys 'xlab' and `ylab`\n",
    "        Dictionary of plotting options, right now only used for the axis labels.\n",
    "    sort_topics : sort topics by topic proportion (percentage of tokens covered). Set to false to\n",
    "        to keep original topic order.\n",
    "    Returns\n",
    "    -------\n",
    "    prepared_data : PreparedData\n",
    "        A named tuple containing all the data structures required to create\n",
    "        the visualization. To be passed on to functions like :func:`display`.\n",
    "        This named tuple can be represented as json or a python dictionary.\n",
    "        There is a helper function 'sorted_terms' that can be used to get\n",
    "        the terms of a topic using lambda to rank their relevance.\n",
    "    Notes\n",
    "    -----\n",
    "    This implements the method of `Sievert, C. and Shirley, K. (2014):\n",
    "    LDAvis: A Method for Visualizing and Interpreting Topics, ACL Workshop on\n",
    "    Interactive Language Learning, Visualization, and Interfaces.`\n",
    "    http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`save_json`: save json representation of a figure to file\n",
    "    :func:`save_html` : save html representation of a figure to file\n",
    "    :func:`show` : launch a local server and show a figure in a browser\n",
    "    :func:`display` : embed figure within the IPython notebook\n",
    "    :func:`enable_notebook` : automatically embed visualizations in IPython notebook\n",
    "   \"\"\"\n",
    "    # parse mds\n",
    "    if isinstance(mds, basestring):\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "\n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "    R = min(R, len(vocab))\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)\n",
    "    if (sort_topics):\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "\n",
    "    topic_order = topic_proportion.index\n",
    "    # reorder all data based on new ordering of topics\n",
    "    topic_freq = topic_freq[topic_order]\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "    doc_topic_dists = doc_topic_dists[topic_order]\n",
    "\n",
    "    # token counts for each term-topic combination (widths of red bars)\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    # Quick fix for red bar width bug.  We calculate the\n",
    "    # term frequencies internally, using the topic term distributions and the\n",
    "    # topic frequencies, rather than using the user-supplied term frequencies.\n",
    "    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "\n",
    "    topic_info = _topic_info(topic_term_dists, topic_proportion,\n",
    "                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n",
    "    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)\n",
    "    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "    client_topic_order = [x + 1 for x in topic_order]\n",
    "\n",
    "    return PreparedData(topic_coordinates, topic_info,\n",
    "                        token_table, R, lambda_step, plot_opts, client_topic_order)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NumPyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int64) or isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.float64) or isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for each topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, threading, webbrowser\n",
    "import gensim, pickle, random\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from flask import Flask, render_template, request, json\n",
    "from prepare_utils import *\n",
    "from gensim_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:root:failed to load state from data/sample/chiledesperto_sample_ldamodel.state: [Errno 2] No such file or directory: 'data/sample/chiledesperto_sample_ldamodel.state'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Category', 'Freq', 'Term', 'Total', 'loglift', 'logprob', 'relevance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################Load data#########################\n",
    "##Load Model\n",
    "LdaModel = gensim.models.ldamodel.LdaModel\n",
    "lda_model = LdaModel.load(\"data/sample/chiledesperto_sample_ldamodel\")\n",
    "\n",
    "##Load corpus\n",
    "with open('data/sample/chiledesperto_corpus_ldamodel.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "##Load id2word\n",
    "id2word = Dictionary.load(\"data/sample/chiledesperto_id2word_ldamodel\")\n",
    "\n",
    "\n",
    "\n",
    "##Load relevant documents\n",
    "import pickle\n",
    "\n",
    "#relevant documents were already calculated\n",
    "with open('data/sample/sent_topics_sorteddf_mallet_ldamodel', 'rb') as f:\n",
    "    sent_topics_sorteddf_mallet = pickle.load(f)\n",
    "\n",
    "sent_topics_sorteddf_mallet = sent_topics_sorteddf_mallet[['Topic_Num','Topic_Perc_Contrib','text']]\n",
    "\n",
    "test_text = 'Lorem Ipsum es simplemente el texto de relleno de las imprentas y archivos de texto. Lorem Ipsum ha sido el texto de relleno estándar de las industrias desde el año 1500, cuando un impresor (N. del T. persona que se dedica a la imprenta) desconocido usó una galería de textos y los mezcló de tal manera que logró hacer un libro de textos especimen. No sólo sobrevivió 500 años, sino que tambien ingresó como texto de relleno en documentos electrónicos, quedando esencialmente igual al original. Fue popularizado en los 60s con la creación de las hojas \"Letraset\", las cuales contenian pasajes de Lorem Ipsum, y más recientemente con software de autoedición, como por ejemplo Aldus PageMaker, el cual incluye versiones de Lorem Ipsum.'\n",
    "\n",
    "\n",
    "#prepared data for visualization\n",
    "'''#Uncomment these lineas in production\n",
    "\n",
    "#convert LDAMALLET to LDAModel gensim\n",
    "lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model)\n",
    "model = extract_data(lda_model, corpus, id2word)\n",
    "PreparedDataObtained= prepare(model['topic_term_dists'],model['doc_topic_dists'],model['doc_lengths'],model['vocab'],model['term_frequency'])\n",
    "\n",
    "\n",
    "\n",
    "with open('data/sample/chiledesperto_prepared_data', 'wb') as f:\n",
    "    pickle.dump(PreparedDataObtained, f)\n",
    "'''\n",
    "\n",
    "with open('data/sample/chiledesperto_prepared_data', 'rb') as f:\n",
    "    PreparedDataObtained = pickle.load(f)\n",
    "\n",
    "\n",
    "print(PreparedDataObtained.sorted_terms(topic=4,_lambda=0.6).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_lda_models = 'G:\\\\Mi unidad\\\\2019-2\\\\EstoPasaEnChile\\\\Instagram\\\\Detecting Violence\\\\Parte II\\\\results\\\\CSCW\\\\text_for_topicmodeling\\\\ldamallet_models\\\\'\n",
    "LdaModel = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n",
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n",
      "WARNING:root:failed to load state from G:\\Mi unidad\\2019-2\\EstoPasaEnChile\\Instagram\\Detecting Violence\\Parte II\\results\\CSCW\\text_for_topicmodeling\\ldamallet_models\\instagram_violento_cscw.state: [Errno 2] No such file or directory: 'G:\\\\Mi unidad\\\\2019-2\\\\EstoPasaEnChile\\\\Instagram\\\\Detecting Violence\\\\Parte II\\\\results\\\\CSCW\\\\text_for_topicmodeling\\\\ldamallet_models\\\\instagram_violento_cscw.state'\n"
     ]
    }
   ],
   "source": [
    "name_model ='instagram_violento_cscw'\n",
    "corpus_name = 'instagram_violento_cscw_corpus.pkl'\n",
    "lda_model = LdaModel.load(ruta_lda_models+name_model)\n",
    "\n",
    "#Twitter_pacifico\n",
    "\n",
    "## Convert  Mallet model into a LDAModel\n",
    "lda_model= gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model)\n",
    "dictionary = lda_model.id2word\n",
    "##load corpus\n",
    "\n",
    "with open(ruta_lda_models+corpus_name, 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "model = _extract_data(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:216: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\gonza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\gonza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:236: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "PreparedDataObtained= prepare(model['topic_term_dists'],model['doc_topic_dists'],model['doc_lengths'],model['vocab'],model['term_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I cant save the class into a pickle\n",
    "with open('instagram_violento_cscw_prepared_data.pkl', 'wb') as f:\n",
    "    pickle.dump(PreparedDataObtained, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(ruta_lda_models+'instagram_pacifico_cscw_prepared_data.pkl', 'rb') as f:\n",
    "    prueba = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreparedData.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreparedData.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreparedData.sorted_terms(topic=4,_lambda=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreparedDataDictionary['topic.order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster      Freq\n",
       "topic                                               \n",
       "11    -0.008866 -0.161807  1       1        8.639098\n",
       "1     -0.169495  0.001830  2       1        8.607634\n",
       "0     -0.180452  0.192896  3       1        8.184394\n",
       "10    -0.176313  0.175726  4       1        8.063626\n",
       "9     -0.207132  0.015597  5       1        7.760817\n",
       "5      0.039782 -0.158633  6       1        7.737531\n",
       "8      0.032712 -0.154913  7       1        7.720558\n",
       "6      0.020073 -0.093163  8       1        7.692291\n",
       "7     -0.040804 -0.003993  9       1        7.627436\n",
       "4      0.224402  0.217575  10      1        7.287469\n",
       "2      0.278410  0.220156  11      1        7.090797\n",
       "3      0.012625 -0.091434  12      1        6.901496\n",
       "12     0.175058 -0.159837  13      1        6.686852, topic_info=     Category         Freq                  Term        Total  loglift  \\\n",
       "15    Default  3580.000000  chile                 3580.000000  30.0000   \n",
       "250   Default  2032.000000  marchar               2032.000000  29.0000   \n",
       "63    Default  2040.000000  luchar                2040.000000  28.0000   \n",
       "64    Default  1561.000000  poblar                1561.000000  27.0000   \n",
       "320   Default  1832.000000  día                   1832.000000  26.0000   \n",
       "91    Default  1000.000000  octubre               1000.000000  25.0000   \n",
       "161   Default  952.000000   ver                   952.000000   24.0000   \n",
       "144   Default  931.000000   país                  931.000000   23.0000   \n",
       "784   Default  870.000000   seguir                870.000000   22.0000   \n",
       "58    Default  837.000000   foto                  837.000000   21.0000   \n",
       "23    Default  1063.000000  gente                 1063.000000  20.0000   \n",
       "27    Default  1668.000000  hoy                   1668.000000  19.0000   \n",
       "483   Default  837.000000   unir                  837.000000   18.0000   \n",
       "52    Default  1156.000000  hacer                 1156.000000  17.0000   \n",
       "12    Default  811.000000   callar                811.000000   16.0000   \n",
       "403   Default  768.000000   manifestación         768.000000   15.0000   \n",
       "48    Default  863.000000   vivir                 863.000000   14.0000   \n",
       "4617  Default  727.000000   noviembre             727.000000   13.0000   \n",
       "762   Default  693.000000   dignidad              693.000000   12.0000   \n",
       "188   Default  680.000000   decir                 680.000000   11.0000   \n",
       "56    Default  1142.000000  querer                1142.000000  10.0000   \n",
       "153   Default  594.000000   protestar             594.000000   9.0000    \n",
       "126   Default  736.000000   calle                 736.000000   8.0000    \n",
       "97    Default  562.000000   cambiar               562.000000   7.0000    \n",
       "316   Default  642.000000   derecho               642.000000   6.0000    \n",
       "327   Default  539.000000   gracia                539.000000   5.0000    \n",
       "295   Default  708.000000   año                   708.000000   4.0000    \n",
       "36    Default  729.000000   nunca                 729.000000   3.0000    \n",
       "121   Default  523.000000   aguantar              523.000000   2.0000    \n",
       "33    Default  502.000000   miedo                 502.000000   1.0000    \n",
       "...       ...         ...     ...                        ...      ...    \n",
       "687   Topic13  121.937751   marcha_pacífica       121.937751   2.7050    \n",
       "4843  Topic13  102.087419   domingo               102.087419   2.7050    \n",
       "1205  Topic13  90.744373    viña                  90.744373    2.7050    \n",
       "1660  Topic13  87.908611    paro_nacional         87.908611    2.7050    \n",
       "1834  Topic13  86.018103    jueves                86.018103    2.7050    \n",
       "2154  Topic13  83.182342    miércoles             83.182342    2.7050    \n",
       "2550  Topic13  79.401326    antofagasta           79.401326    2.7050    \n",
       "652   Topic13  59.550995    nueva_jornada         59.550995    2.7050    \n",
       "5007  Topic13  58.605741    archivar              58.605741    2.7050    \n",
       "2015  Topic13  52.934217    chillán               52.934217    2.7050    \n",
       "2212  Topic13  51.043710    fotógrafo             51.043710    2.7050    \n",
       "3730  Topic13  45.372186    gran_marcha           45.372186    2.7050    \n",
       "2048  Topic13  38.755409    marcha_masiva         38.755409    2.7050    \n",
       "356   Topic13  34.974394    registro_fotográfico  34.974394    2.7050    \n",
       "4687  Topic13  34.029140    modelo_económico      34.029140    2.7050    \n",
       "916   Topic13  33.083886    conce                 33.083886    2.7050    \n",
       "2206  Topic13  84.127596    registro              85.130661    2.6932    \n",
       "1309  Topic13  70.894041    postal                72.943763    2.6765    \n",
       "1467  Topic13  202.284331   registrar             222.835564   2.6083    \n",
       "1203  Topic13  113.430466   mar                   121.421162   2.6370    \n",
       "240   Topic13  147.459606   centrar               160.815940   2.6183    \n",
       "94    Topic13  403.623408   santiago              478.162476   2.5356    \n",
       "1534  Topic13  92.634880    convocatorio          98.771235    2.6409    \n",
       "732   Topic13  199.448569   realizar              252.741327   2.4682    \n",
       "320   Topic13  764.710391   día                   1832.436494  1.8311    \n",
       "72    Topic13  356.360714   partir                763.565593   1.9430    \n",
       "656   Topic13  108.704196   fotografiar           212.438993   2.0350    \n",
       "31    Topic13  109.649450   manifestar            226.829440   1.9781    \n",
       "6087  Topic13  120.047243   plaza_dignidad        486.943878   1.3048    \n",
       "214   Topic13  92.634880    valparaíso            226.414822   1.8113    \n",
       "\n",
       "      logprob  \n",
       "15    30.0000  \n",
       "250   29.0000  \n",
       "63    28.0000  \n",
       "64    27.0000  \n",
       "320   26.0000  \n",
       "91    25.0000  \n",
       "161   24.0000  \n",
       "144   23.0000  \n",
       "784   22.0000  \n",
       "58    21.0000  \n",
       "23    20.0000  \n",
       "27    19.0000  \n",
       "483   18.0000  \n",
       "52    17.0000  \n",
       "12    16.0000  \n",
       "403   15.0000  \n",
       "48    14.0000  \n",
       "4617  13.0000  \n",
       "762   12.0000  \n",
       "188   11.0000  \n",
       "56    10.0000  \n",
       "153   9.0000   \n",
       "126   8.0000   \n",
       "97    7.0000   \n",
       "316   6.0000   \n",
       "327   5.0000   \n",
       "295   4.0000   \n",
       "36    3.0000   \n",
       "121   2.0000   \n",
       "33    1.0000   \n",
       "...      ...   \n",
       "687  -4.7231   \n",
       "4843 -4.9007   \n",
       "1205 -5.0185   \n",
       "1660 -5.0503   \n",
       "1834 -5.0720   \n",
       "2154 -5.1055   \n",
       "2550 -5.1521   \n",
       "652  -5.4397   \n",
       "5007 -5.4557   \n",
       "2015 -5.5575   \n",
       "2212 -5.5939   \n",
       "3730 -5.7117   \n",
       "2048 -5.8693   \n",
       "356  -5.9720   \n",
       "4687 -5.9994   \n",
       "916  -6.0275   \n",
       "2206 -5.0942   \n",
       "1309 -5.2654   \n",
       "1467 -4.2169   \n",
       "1203 -4.7954   \n",
       "240  -4.5330   \n",
       "94   -3.5261   \n",
       "1534 -4.9979   \n",
       "732  -4.2310   \n",
       "320  -2.8871   \n",
       "72   -3.6506   \n",
       "656  -4.8379   \n",
       "31   -4.8293   \n",
       "6087 -4.7387   \n",
       "214  -4.9979   \n",
       "\n",
       "[1073 rows x 6 columns], token_table=       Topic      Freq                Term\n",
       "term                                      \n",
       "998    6      0.987751  abajar            \n",
       "998    10     0.011759  abajar            \n",
       "1022   6      0.017880  abandonar         \n",
       "1022   11     0.983418  abandonar         \n",
       "9808   5      0.999066  ablehnung         \n",
       "10131  5      0.999066  aborrecer         \n",
       "181    3      1.005067  abrazar           \n",
       "1342   3      0.005260  abrir             \n",
       "1342   4      0.904697  abrir             \n",
       "1342   6      0.005260  abrir             \n",
       "1342   12     0.084158  abrir             \n",
       "617    2      0.997327  abuelo            \n",
       "284    5      0.999066  aburrir           \n",
       "486    1      0.312461  abusar            \n",
       "486    7      0.491011  abusar            \n",
       "486    10     0.198839  abusar            \n",
       "285    10     1.000636  abuso             \n",
       "1128   10     0.996943  acabar            \n",
       "7116   2      0.988093  acceder           \n",
       "9      1      1.006918  accionar          \n",
       "8475   8      0.993583  accurate          \n",
       "7213   1      0.964071  ace               \n",
       "1320   9      0.972508  acompañar         \n",
       "1320   12     0.028325  acompañar         \n",
       "1480   1      1.001632  acordar           \n",
       "2788   7      0.011581  actividad         \n",
       "2788   12     0.984411  actividad         \n",
       "1748   1      0.994676  actuar            \n",
       "1482   3      0.986689  acá               \n",
       "1482   8      0.009136  acá               \n",
       "...   ..           ...  ...               \n",
       "7430   6      0.975742  violencia_machista\n",
       "162    1      0.965685  violento          \n",
       "162    3      0.010730  violento          \n",
       "162    11     0.021460  violento          \n",
       "9078   4      0.980045  violents          \n",
       "6393   1      0.964071  virtud            \n",
       "48     3      0.062537  vivir             \n",
       "48     5      0.938060  vivir             \n",
       "252    5      0.128364  vivo              \n",
       "252    6      0.834369  vivo              \n",
       "252    10     0.009169  vivo              \n",
       "252    13     0.027507  vivo              \n",
       "1205   13     1.002817  viña              \n",
       "470    3      0.462921  volver            \n",
       "470    10     0.011222  volver            \n",
       "470    11     0.524644  volver            \n",
       "9378   7      0.829721  vorgehen          \n",
       "442    5      0.999066  vosotros          \n",
       "369    4      0.002964  voz               \n",
       "369    5      0.996044  voz               \n",
       "7541   6      0.975742  vulgo             \n",
       "9379   7      0.829721  warum             \n",
       "9380   7      0.829721  weil              \n",
       "9381   7      0.976142  wir               \n",
       "9725   11     1.019990  yapa              \n",
       "4131   8      0.993583  yuta              \n",
       "8201   13     1.057917  álvaro            \n",
       "9384   7      0.829721  über              \n",
       "9492   11     1.019990  ᴇɴ                \n",
       "9496   11     1.019990  ᴘᴇϙᴜᴇɴ            \n",
       "\n",
       "[1343 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[12, 2, 1, 11, 10, 6, 9, 7, 8, 5, 3, 4, 13])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
